
# Inteligencia artificial y ciencia de datos


En la ciencia de datos, hay muchos términos que se usan indistintamente, por lo que es importante explorar los más comunes. El término "macrodatos" se refiere a conjuntos de datos tan masivos, generados tan rápidamente y tan variados que desafían los métodos de análisis tradicionales, como aquellos utilizados con bases de datos relacionales. El desarrollo simultáneo de una enorme potencia de cómputo en las redes distribuidas y de nuevas herramientas y técnicas para el análisis de datos ha permitido que las organizaciones puedan analizar estos vastos conjuntos de datos.

Cada vez hay más conocimientos e ideas al alcance de todos. Los macrodatos suelen describirse en términos de cinco V: velocidad, volumen, variedad, veracidad y valor. La minería de datos es el proceso de buscar y analizar datos automáticamente, descubriendo patrones no revelados previamente. Este proceso implica preprocesar los datos para prepararlos y transformarlos en un formato apropiado. Una vez hecho esto, se extraen conocimientos y patrones utilizando diversas herramientas y técnicas, que van desde simples herramientas de visualización de datos hasta modelos estadísticos y de aprendizaje automático.

El aprendizaje automático es un subconjunto de la inteligencia artificial (IA) que utiliza algoritmos informáticos para analizar datos y tomar decisiones inteligentes en función de lo aprendido, sin necesidad de ser programados explícitamente. Los algoritmos de aprendizaje automático se entrenan con grandes conjuntos de datos y aprenden a partir de ejemplos, sin seguir reglas predefinidas. El aprendizaje automático permite a las máquinas resolver problemas por sí mismas y hacer predicciones precisas utilizando los datos proporcionados.

El aprendizaje profundo es un subconjunto especializado del aprendizaje automático que utiliza redes neuronales en capas para simular la toma de decisiones humana. Los algoritmos de aprendizaje profundo pueden etiquetar y categorizar la información e identificar patrones. Esto permite a los sistemas de IA aprender continuamente durante el trabajo y mejorar la calidad y precisión de los resultados, determinando si las decisiones fueron correctas.

Las redes neuronales artificiales, a menudo denominadas simplemente redes neuronales, se inspiran en las redes neuronales biológicas, aunque funcionan de manera diferente. En la IA, una red neuronal es un conjunto de pequeñas unidades informáticas llamadas neuronas que toman los datos entrantes y aprenden a tomar decisiones con el tiempo. Las redes neuronales suelen tener capas profundas, lo que hace que los algoritmos de aprendizaje profundo sean más eficientes a medida que los conjuntos de datos aumentan en volumen, a diferencia de otros algoritmos de aprendizaje automático que pueden estabilizarse al aumentar los datos.

Ahora que se tiene un amplio conocimiento de las diferencias entre algunos conceptos clave de la IA, es importante entender otra distinción: la que existe entre la inteligencia artificial y la ciencia de datos. La ciencia de datos es el proceso y método para extraer conocimiento e información a partir de grandes volúmenes de datos dispares. Es un campo interdisciplinario que incluye las matemáticas, el análisis estadístico, la visualización de datos, el aprendizaje automático, entre otros. Este campo nos permite apropiarnos de la información, ver patrones, encontrar significado a partir de grandes volúmenes de datos y utilizarlos para tomar decisiones que impulsen el negocio.

La ciencia de datos puede utilizar muchas de las técnicas de la IA para obtener información a partir de los datos. Por ejemplo, podría emplear algoritmos de aprendizaje automático e incluso modelos de aprendizaje profundo para extraer significado y hacer inferencias a partir de los datos. Aunque existe cierta interacción entre la IA y la ciencia de datos, una no es un subconjunto de la otra. La ciencia de datos es un término amplio que abarca toda la metodología de procesamiento de datos, mientras que la IA incluye todo lo que permite a las computadoras aprender a resolver problemas y tomar decisiones inteligentes. Ambas disciplinas pueden implicar el uso de macrodatos, es decir, volúmenes de datos significativamente grandes.





# Redes neurales y aprendizaje profundo



Supongo que las Ciencias de la Computación intentan imitar las neuronas reales y cómo funciona nuestro cerebro. Hace unos 20-23 años, una red neuronal recibiría algunos insumos que serían procesados por diferentes nodos. Estos nodos transformarían los datos, los agregarían y posiblemente los pasarían a otro nivel de nodos. Finalmente, se generaría una salida. Recuerdo haber entrenado una red neuronal para reconocer dígitos manuscritos y otras tareas similares.

Una red neuronal intenta imitar cómo las neuronas y las sinapsis de nuestro cerebro procesan información. El objetivo es construir redes complejas que puedan ser entrenadas para hacer transformaciones a partir de ciertas entradas hasta que produzcan una salida esperada. Este proceso se repite una y otra vez, con el fin de que la red neuronal converja hacia resultados correctos. A pesar de que la teoría de las redes neuronales estaba bien establecida y podían resolver problemas pequeños como el reconocimiento de dígitos, la computación necesaria era muy intensiva. Debido a esto, dejé de enseñar sobre ellas hace unos 15 años.

De repente, hace unos 4 o 5 años, comenzó a hablarse del "aprendizaje profundo". Al principio, no entendía bien qué era, pero al investigar, descubrí que es esencialmente redes neuronales más avanzadas, potenciado con un mayor poder computacional. El aprendizaje profundo utiliza múltiples capas de redes neuronales, lo que lo hace más poderoso. Para estos modelos, es necesario contar con grandes capacidades de procesamiento, como las que ofrecen las Unidades de Procesamiento de Gráficos (GPU), que permiten hacer cálculos de matrices y álgebra lineal rápidamente, fundamentales para el funcionamiento de las redes neuronales.

Hoy en día, las redes neuronales y el aprendizaje profundo son capaces de realizar tareas impresionantes. Pueden reconocer el habla y las caras, por ejemplo, y esto está sucediendo gracias a la gran potencia computacional disponible. En mi universidad, por ejemplo, en el centro de datos se encuentran pilas de servidores Dell equipados con GPU, utilizados específicamente para el aprendizaje profundo. Cada unidad de procesamiento de gráficos tiene alrededor de 600 núcleos de procesamiento, lo que permite realizar millones de cálculos simultáneamente, necesarios para el aprendizaje profundo.

Algunos de los primeros avances notables del aprendizaje profundo fueron en el reconocimiento del habla. Un joven profesor de marketing, que investiga este campo, utiliza una computadora con una GPU para entrenar redes neuronales a reconocer el habla mientras enseña en clase. El aprendizaje profundo ha permitido que las redes neuronales realicen tareas que antes eran limitadas a pequeños problemas, como el reconocimiento de dígitos manuscritos. Ahora, pueden identificar objetos, como distinguir entre un gato y un perro, sin necesidad de ser específicamente programadas para hacerlo. Es por eso que se le llama "aprendizaje profundo", ya que la red neuronal aprende por sí misma a medida que recibe más datos.

Lo fascinante es cómo las redes neuronales ahora pueden generar habla, y el sonido que producen cuando aprenden a hablar suena como el de un bebé aprendiendo a comunicarse. Es impresionante ver cómo la máquina pasa de no hablar nada a generar un discurso claro. Sin embargo, para poder trabajar en aprendizaje profundo, es necesario contar con conocimientos en álgebra lineal y matemáticas avanzadas, ya que muchas de las operaciones fundamentales se basan en transformaciones matriciales. A pesar de que existen paquetes de software que facilitan el trabajo, tener una comprensión básica de lo que sucede "bajo el capó" es esencial.

Finalmente, el aprendizaje profundo requiere una gran cantidad de recursos computacionales. No es algo que se pueda hacer fácilmente con un ordenador personal; se necesitan equipos especializados con un alto poder de procesamiento. Sin embargo, si se dispone de los recursos adecuados, el potencial de estas tecnologías es asombroso y ha revolucionado campos como el reconocimiento de voz, la visión por computadora y muchas otras aplicaciones de la inteligencia artificial.





# Aplicaciones del aprendizaje automático


Ahora todo el mundo se ocupa del aprendizaje automático, y los sistemas de recomendación son, sin duda, una de las principales aplicaciones. Los sistemas de clasificación, análisis de conglomerados y la búsqueda de soluciones para problemas de marketing, como el análisis de la cesta de compra, también han sido revolucionados por el aprendizaje automático. Un ejemplo clásico es la pregunta de qué productos se suelen comprar juntos. Este era un problema muy difícil desde el punto de vista computacional, pero hoy en día lo resolvemos con facilidad gracias al aprendizaje automático. De este modo, el análisis predictivo se ha convertido en otra área importante del aprendizaje automático, utilizando nuevas técnicas para hacer predicciones sobre diversas situaciones, algo que a menudo no es del agrado de los estadísticos tradicionales.

El aprendizaje automático emplea una variedad de técnicas como árboles de decisiones, análisis bayesiano, y modelos bayesianos ingenuos. Lo interesante de estas técnicas es que, si bien es importante comprender cómo utilizarlas, no es necesario entender completamente los detalles de su funcionamiento. Es esencial entender sus significados y las compensaciones que conllevan, como la precisión frente al recuerdo, o los problemas de sobremuestreo y sobreajuste. Así, una persona que tenga algunos conocimientos de ciencia de datos puede aplicar estas técnicas, pero debe estar consciente de las limitaciones y las elecciones que debe hacer al usarlas.

En el sector de la tecnología financiera, el aprendizaje automático tiene múltiples aplicaciones. Una de ellas es el uso de sistemas de recomendación, que ya son familiares para quienes utilizan plataformas como Netflix o Facebook. En estos servicios, las recomendaciones están basadas en los intereses previos del usuario. Por ejemplo, si has visto un programa, te sugerirán otros similares que podrían interesarte. De manera similar, en el ámbito de las finanzas, si eres un profesional de la inversión que ha analizado una idea de inversión, el sistema podría recomendarte estudiar otra idea similar, ya sea por tipo de activo, sector o técnica de inversión.

Otra área clave de aplicación del aprendizaje automático en la tecnología financiera es la detección de fraude. Esto es especialmente relevante en el sector minorista y en la banca, donde el desafío es determinar si una transacción con tarjeta de crédito es fraudulenta o no, y hacerlo en tiempo real. El sistema debe aprender de todas las transacciones previas para crear un modelo que permita evaluar cada nueva transacción. Al recibir un cargo, el modelo realiza los cálculos necesarios y decide si es legítimo o si debe enviarse a los encargados de la detección de fraudes para su verificación.



## Capítulo 7. Por qué los padres altos no tienen hijos aún más altos

Es posible que hayas notado que los padres más altos a menudo tienen hijos altos que no son necesariamente más altos que ellos, y eso es algo bueno. Esto no sugiere que los niños nacidos de padres altos no sean necesariamente más altos que los demás. Puede que ese sea el caso, pero no son necesariamente más altos que sus propios padres “altos”. La razón por la que creo que esto es algo bueno requiere una simple simulación mental. Imagina que cada generación sucesiva nacida de padres altos fuera más alta que sus padres; en cuestión de un par de milenios, los seres humanos se volverían incómodamente altos para su propio bien, requiriendo muebles, coches y aviones aún más grandes.

Sir Frances Galton estudió la misma cuestión en 1886 y llegó a una técnica estadística que hoy conocemos como modelos de regresión. Este capítulo explora el funcionamiento de los modelos de regresión, que se han convertido en el caballo de batalla del análisis estadístico. En casi todas las búsquedas empíricas de investigación, ya sea en el ámbito académico o profesional, el uso de modelos de regresión, o sus variantes, es ubicuo. En la ciencia médica, se están utilizando modelos de regresión para desarrollar medicamentos más efectivos, mejorar los métodos de operaciones y optimizar recursos para hospitales grandes y pequeños. En el mundo empresarial, los modelos de regresión están a la vanguardia del análisis del comportamiento del consumidor, la productividad de las empresas y la competitividad de las entidades del sector público y privado.

Me gustaría introducir los modelos de regresión narrando una historia sobre mi tesis de maestría. Creo que esta historia puede ayudar a explicar la utilidad de los modelos de regresión.

## El Departamento de Conclusiones Obvias

En 1999, terminé mi investigación de Maestría sobre el desarrollo de modelos de precios hedónicos para propiedades residenciales. Me tomó tres años completar el proyecto que involucró 500,000 transacciones inmobiliarias. Mientras me preparaba para la defensa, mi esposa se ofreció generosamente a llevarme a la universidad. Mientras íbamos de camino, me preguntó: “Dime, ¿qué has encontrado en tu investigación?”. Me alegró que finalmente me pidiera explicar lo que había estado haciendo durante los últimos tres años. “Bueno, he estado estudiando los determinantes de los precios de la vivienda. He encontrado que las casas más grandes se venden por más que las casas más pequeñas”, le dije a mi esposa con una expresión triunfante en mi rostro mientras sostenía el borrador de la tesis en mis manos.

Nos acercábamos a la rampa de entrada de una autopista. Tan pronto como terminé la frase, mi esposa de repente giró el auto hacia el arcén y aplicó los frenos. Al detenerse el auto, se volvió hacia mí y dijo: “No puedo creer que te estén dando un título de Maestría por encontrar solo eso. Yo podría haberte dicho que las casas más grandes se venden por más que las casas más pequeñas”.

En ese mismo momento, me sentí como un profesor que enseñaba en el departamento de conclusiones obvias. ¿Cómo puedo culparla por estar sorprendida de que lo que se conoce comúnmente sobre los precios de la vivienda me valga un título de Maestría de una universidad de alta reputación?

Le pedí a mi esposa que reanudara la conducción para que pudiera tomar los próximos diez minutos para explicarle las complejidades de mi investigación. Ella me dio cinco minutos en su lugar, pensando que esto podría no requerir ni siquiera eso. Acepté los cinco y pasé el siguiente minuto organizando mis pensamientos. Le expliqué que mi investigación no solo había encontrado la correlación entre los precios de la vivienda y el tamaño de las unidades habitacionales, sino que también había descubierto la magnitud de esas relaciones. Por ejemplo, descubrí que, manteniendo todo lo demás constante, un término que explico más adelante en este capítulo, un baño adicional agrega más al precio de la vivienda que un dormitorio adicional. Dicho de otra manera, el aumento marginal en el precio de una casa es mayor por un baño adicional que por un dormitorio adicional. Más tarde descubrí que los agentes inmobiliarios en Toronto, de hecho, apreciaron este hallazgo.

También le expliqué a mi esposa que la proximidad a infraestructuras de transporte, como el metro, resultaba en precios de vivienda más altos. Por ejemplo, las casas situadas más cerca del metro se vendían por más que aquellas situadas más lejos. Sin embargo, las casas cerca de autopistas o carreteras se vendían por menos que otras. De manera similar, también descubrí que la proximidad a grandes centros comerciales tenía un impacto no lineal en los precios de la vivienda. Las casas ubicadas muy cerca (menos de 2.5 km) de los centros comerciales se vendían por menos que el resto. Sin embargo, las casas ubicadas más cerca (menos de 5 km, pero más de 2.5 km) del centro comercial se vendían por más que aquellas situadas más lejos. También encontré que los valores de las viviendas en Toronto disminuían con la distancia al centro de la ciudad.

Mientras explicaba mis contribuciones al estudio de los mercados de vivienda, noté que mi esposa estaba levemente impresionada. La razón probable de su recepción tibia era que mis hallazgos confirmaban lo que ya sabíamos por nuestra experiencia cotidiana. Sin embargo, el verdadero valor añadido por la investigación residía en cuantificar la magnitud de esas relaciones.

## ¿Por qué Regressar?

Se podrían plantear una gran cantidad de preguntas a la análisis de regresión. Algunos ejemplos de preguntas que los modelos de regresión (hedónicos) podrían abordar incluyen:

- ¿Cuánto más puede venderse una casa por un dormitorio adicional?
    
- ¿Cuál es el impacto del tamaño del lote en el precio de la vivienda?
    
- ¿Las casas con exteriores de ladrillo se venden por menos que las casas con exteriores de piedra?
    
- ¿Cuánto contribuye un sótano terminado al precio de una unidad de vivienda?
    
- ¿Las casas ubicadas cerca de líneas de alta tensión se venden por más o por menos que el resto?


![[Pasted image 20241218164610.png]]


![[Pasted image 20241218164618.png]]

## Galería de IBM Cloud

Tiempo estimado (45 min)

El hub de recursos de IBM Cloud es una colección en crecimiento de conjuntos de datos, cuadernos y plantillas de proyectos. En este laboratorio, utilizarás el _hub de recursos de IBM Cloud_ para explorar diferentes conjuntos de datos. Como aprendiste en el curso, los datos pueden ser más que solo números. Los datos pueden ser numéricos, texto, imágenes, videos, audios y más. Examinarás tres muestras.

**Muestra 1** contiene datos con solo atributos numéricos.

**Muestra 2** contiene datos con atributos numéricos y de texto.

**Muestra 3** contiene un cuaderno de Jupyter, una herramienta que los científicos de datos utilizan para crear modelos.

Veamos cómo los científicos de datos utilizan diferentes conjuntos de datos.

#### Objetivos :

Aprenderás a:

- Explorar el centro de recursos de IBM Cloud
- Examinar un conjunto de datos numéricos
- Examinar un conjunto de datos con atributos no numéricos
- Examinar un cuaderno Jupyter

#### Ejercicio 1: Examina un conjunto de datos numéricos

1. Haz clic en el enlace: [https://dataplatform.cloud.ibm.com/gallery](https://dataplatform.cloud.ibm.com/gallery)
    
2. Haz clic en el botón de filtro en la parte superior derecha de la ventana:
    
    ![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/i3wHdL5V2zxB_s1iS847_w/filterbutton.jpg)
3. En el menú desplegable que aparece, selecciona la casilla _Datos_ bajo _Tipo de muestra_. Luego haz clic en el desplegable _Etiquetas_ y selecciona la casilla _Medio ambiente_.
    

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/images/environmentfilter.png)

4. En los resultados de búsqueda, haz clic en _UCI: Incendios Forestales_.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/images/UCIForestFires.png)

5. Previsualiza los datos utilizando la opción _Previsualizar_.

![data_UCI_Fires.jpg](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/sXPKUrISlHg51sAZ8d2oeg/data-UCI-Fires.jpg)

##### Explorar los datos

Los datos están relacionados con incendios forestales, donde el objetivo es predecir el área quemada de incendios forestales en la región noreste de Portugal, utilizando datos meteorológicos y otros datos.

**Información de Atributos:**

1. X - coordenada espacial en el eje x dentro del mapa del parque Montesinho: 1 a 9
2. Y - coordenada espacial en el eje y dentro del mapa del parque Montesinho: 2 a 9
3. month - mes del año: 'ene' a 'dic'
4. day - día de la semana: 'lun' a 'dom'
5. FFMC - índice FFMC del sistema FWI: 18.7 a 96.20
6. DMC - índice DMC del sistema FWI: 1.1 a 291.3
7. DC - índice DC del sistema FWI: 7.9 a 860.6
8. ISI - índice ISI del sistema FWI: 0.0 a 56.10
9. temp - temperatura en grados Celsius: 2.2 a 33.30
10. RH - humedad relativa en %: 15.0 a 100
11. wind - velocidad del viento en km/h: 0.40 a 9.40
12. rain - lluvia exterior en mm/m2: 0.0 a 6.4
13. area - el área quemada del bosque (en ha): 0.00 a 1090.84  
    (esta variable de salida está muy sesgada hacia 0.0, por lo que puede tener sentido modelar con la transformación logarítmica).

### Ejercicio 2: Evaluar un conjunto de datos no numérico

Los datos no tienen que basarse únicamente en números. Los datos pueden ser texto, imágenes y otros tipos también. Veamos un conjunto de datos que tiene valores de texto.

1. En la parte superior de la página, selecciona la opción _Centro de recursos_.
    
    ![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/N5dwDitUC-AT1VqyON-IxQ/Gallery.jpg)
2. Escribe _Airbnb_ en la barra de búsqueda.
    
    ![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Do_fUa3gOdBtl1UUGKrcbA/searchairbnb.jpg)
3. Selecciona la opción _Datos de Airbnb para análisis: Reseñas de Trentino_. Puede que necesites desplazarte para encontrarlo.
    

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/images/trentinoreviewscard.png)

4. Previsualiza los datos utilizando la opción _Previsualizar_.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/labs/Module%202/images/Airbnb_preview.png)

##### Explorar los datos

Airbnb, Inc. es una empresa estadounidense que opera un mercado en línea para alojamiento, principalmente estancias en casas para alquiler vacacional y actividades turísticas. Los huéspedes de Airbnb pueden dejar una reseña después de su estancia, y estas pueden ser utilizadas como un indicador de la actividad de Airbnb. La estancia mínima, el precio y el número de reseñas se han utilizado para estimar la tasa de ocupación, el número de noches por año y los ingresos por mes para cada anuncio.

Podrías usar estos datos de múltiples maneras: para analizar las calificaciones de estrellas de los lugares, para analizar las preferencias de ubicación de los clientes, para analizar el tono y el sentimiento de las reseñas de los clientes y muchas más. Airbnb utiliza datos de ubicación para mejorar la satisfacción de los huéspedes.

> 💡 ¿Para qué más podrías usar estos datos?

El conjunto de datos comprende tres tablas principales:

- listings - Datos detallados de listados que muestran 96 atributos para cada uno de los listados. Algunos de los atributos utilizados en el análisis son precio (continuo), longitud (continuo), latitud (continuo), tipo_de_listado (categórico), es_superhost (categórico), vecindario (categórico), calificaciones (continuo) entre otros.
    
- reviews - Reseñas detalladas dadas por los huéspedes con 6 atributos. Los atributos clave incluyen fecha (fecha y hora), id_del_listado (discreto), id_del_revisor (discreto) y comentario (textual).
    
- calendar - Proporciona detalles sobre la reserva para el próximo año por listado. Cuatro atributos en total, incluyendo id_del_listado (discreto), fecha (fecha y hora), disponible (categórico) y precio (continuo).
    

### Ejercicio 3: Evaluar Jupyter Notebook

Regresa al centro de recursos. Selecciona _Notebook_ del menú _Tipo de muestra_ que aparece después de hacer clic en el botón de filtro. En la barra de búsqueda escribe _Encontrar ubicaciones óptimas_. Selecciona la tarjeta que dice _Encontrar ubicaciones óptimas de nuevas tiendas usando…_

![optimallocations.jpg](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/05jUOy8_cGVsSY2sqqKChw/optimallocations.jpg)

Este Jupyter notebook utiliza _Optimización de Decisiones_ con Python para ayudar a determinar la ubicación óptima de una nueva tienda.

Este Notebook tiene como objetivo identificar dónde colocar una cafetería que minimice la distancia total desde las bibliotecas en la zona hasta la tienda, de manera que un lector de libros pueda llegar fácilmente a la tienda.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/labs/Module%202/images/Notebook.png)

Parte del código de Python en el notebook muestra las ubicaciones de las bibliotecas en un mapa.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/labs/Module%202/images/loc_jupyter.png)

Pero con estos datos, no puedes determinar la ubicación ideal de las cafeterías solo mirando el mapa.

El código luego resuelve esto con un modelo de optimización que ayudará a determinar posibles ubicaciones para las cafeterías con la condición de minimizar la distancia entre las bibliotecas y la tienda.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0101EN-SkillsNetwork/labs/Module%202/images/loc2_jupyter.png)

#### Resumen

En este laboratorio, has aprendido a explorar conjuntos de datos y cuadernos en el IBM Cloud Resource Hub.


![[Pasted image 20241218165043.png]]


![[Pasted image 20241218165051.png]]


![[Pasted image 20241218165057.png]]



![[Pasted image 20241218165254.png]]


![[Pasted image 20241218165301.png]]


![[Pasted image 20241218165309.png]]


![[Pasted image 20241218165316.png]]


![[Pasted image 20241218165322.png]]



