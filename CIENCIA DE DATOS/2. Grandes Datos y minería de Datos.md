
# Cómo los macrodatos impulsan la transformación digital


La transformación digital afecta a las operaciones comerciales, actualizando los procesos y operaciones existentes y creando otros nuevos para aprovechar los beneficios de las nuevas tecnologías. Este cambio digital integra la tecnología digital en todas las áreas de una organización, lo que se traduce en cambios fundamentales en su forma de operar y ofrecer valor a los clientes. Se trata de un cambio organizacional y cultural impulsado por la ciencia de los datos y, especialmente, por los macrodatos. La disponibilidad de grandes cantidades de datos y la ventaja competitiva que supone analizarlos han provocado transformaciones digitales en muchos sectores.

Netflix pasó de ser un sistema de préstamo de DVD por correo a convertirse en uno de los principales proveedores de streaming de vídeo del mundo. El equipo de la NBA de los Houston Rockets utilizó los datos recopilados por cámaras de techo para analizar las jugadas más productivas, y Lufthansa analizó los datos de sus clientes para mejorar su servicio. Las organizaciones que nos rodean están cambiando hasta sus cimientos. Veamos un ejemplo para ver cómo los macrodatos pueden desencadenar una transformación digital, no solo en una organización, sino en todo un sector. En 2018, los Rockets de Houston, un equipo de la NBA, elevaron su nivel de juego con el uso del big data. Los Rockets fueron uno de los cuatro equipos de la NBA que instalaron un sistema de seguimiento por vídeo que extraía datos sin procesar de los partidos. Analizaron los datos de seguimiento de vídeo para investigar qué jugadas ofrecían las mejores oportunidades de conseguir puntuaciones altas y descubrieron algo sorprendente. El análisis de los datos reveló que los tiros que ofrecen las mejores oportunidades de conseguir puntuaciones altas son los mates de dos puntos desde dentro de la zona de dos puntos y los tiros de tres puntos desde fuera de la línea de tres puntos, no los tiros de dos puntos a larga distancia desde dentro de la misma.

Este descubrimiento cambió por completo la forma en que el equipo abordaba cada partido, aumentando el número de intentos de tiros de tres puntos. En la temporada 2017-18, los Rockets encestaron más tiros de tres puntos que ningún otro equipo en la historia de la NBA, y esta fue una de las principales razones por las que ganaron más partidos que cualquiera de sus rivales.

En el baloncesto, los macrodatos cambiaron la forma en que los equipos tratan de ganar, transformando la forma de enfocar el juego. La transformación digital no consiste simplemente en duplicar los procesos existentes en formato digital; el análisis en profundidad del funcionamiento de la empresa ayuda a las organizaciones a descubrir cómo mejorar sus procesos y operaciones, y a aprovechar las ventajas de integrar la ciencia de datos en sus flujos de trabajo. La mayoría de las organizaciones se dan cuenta de que la transformación digital requerirá cambios fundamentales en su enfoque de los datos, los empleados y los clientes, y afectará a su cultura organizacional. La transformación digital afecta a todos los aspectos de la organización, por lo que los responsables de la toma de decisiones en los niveles más altos se encargan de ella para garantizar el éxito. El apoyo del director ejecutivo es crucial para el proceso de transformación digital, al igual que el apoyo del director de información y el papel emergente del director de datos.

Pero también requieren el apoyo de los ejecutivos que controlan los presupuestos, las decisiones de personal y las prioridades diarias. Se trata de todo un proceso organizativo. Todos deben apoyarlo para que tenga éxito. No cabe duda de que abordar todos los problemas que surgen en este esfuerzo requiere una nueva mentalidad, pero la transformación digital es la forma de tener éxito ahora y en el futuro.



# Introducción a la nube


Bienvenido a la Introducción a la computación en la nube y a los modelos de despliegue y servicio en la nube. Tras ver este vídeo, podrá describir los conceptos de la computación en la nube, definir los modelos de despliegue y los modelos de servicios en la nube, e identificar las características de la informática en la nube. La computación en nube, también conocida como nube, consiste en la entrega de recursos de computación bajo demanda, como redes, servidores, almacenamiento, aplicaciones, servicios y centros de datos a través de Internet, mediante un sistema de pago por uso. El término «computación en nube» se puede utilizar para describir las aplicaciones y los datos a los que los usuarios acceden a través de Internet y no desde su ordenador local.

Algunos ejemplos de computación en nube son los usuarios que utilizan aplicaciones web en línea, los empleados que utilizan aplicaciones empresariales seguras en línea para realizar su trabajo y los usuarios que almacenan archivos personales en plataformas de almacenamiento basadas en la nube, como Google Drive, OneDrive y Dropbox. Una de las principales ventajas de la computación en nube para los usuarios es que, en lugar de tener que comprar sus propias aplicaciones e instalarlas localmente en su ordenador, pueden utilizar las versiones en línea de esas aplicaciones y pagar una suscripción mensual. Esto no solo suele ser más rentable al principio, sino que los usuarios también pueden acceder a la última versión de la aplicación sin tener que comprar una copia comercial completa de la versión más reciente.

Una ventaja adicional de esto es que el usuario también ahorra mucho espacio de almacenamiento local, ya que la aplicación se aloja en línea. Y lo mejor de la mayoría de las aplicaciones basadas en la nube es que también permiten a los usuarios trabajar en colaboración con sus colegas, trabajar en los mismos archivos en tiempo real y poder ver las ediciones y actualizaciones de los demás. La computación en la nube se compone de cinco características esenciales, tres modelos de implementación y tres modelos de servicio. Empecemos por entender las cinco características esenciales de la nube. El autoservicio bajo demanda significa que puede acceder a los recursos de la nube, como la potencia de procesamiento, el almacenamiento y la red que necesita, mediante una interfaz sencilla, sin necesidad de la interacción humana con cada proveedor de servicios.

El amplio acceso a la red significa que se puede acceder a los recursos de computación en la nube a través de la red a través de mecanismos y plataformas estándar, como teléfonos móviles, tabletas, computadoras portátiles y estaciones de trabajo. La agrupación de recursos es lo que proporciona a los proveedores de servicios en la nube economías de escala, que repercuten en sus clientes, lo que hace que la nube sea rentable. Al utilizar un modelo multiusuario, los recursos informáticos se agrupan para atender a varios consumidores, y los recursos en la nube se asignan y reasignan dinámicamente según la demanda, sin que los clientes necesiten saber la ubicación física de estos recursos.

La rápida elasticidad implica que puede acceder a más recursos cuando los necesita y reducirlos cuando no los necesita, ya que los recursos se aprovisionan y liberan de forma elástica. Y un servicio mesurado significa que solo paga por lo que usa o reserva sobre la marcha. Si no utilizas los recursos, no pagas. El uso de los recursos se monitorea, mide e informa de forma transparente en función de la utilización por parte del consumidor. Como ha visto, la computación en nube consiste realmente en utilizar la tecnología «como un servicio», aprovechando los sistemas remotos a pedido a través de una Internet abierta, ampliándolos y reduciéndolos, y pagando por lo que se utiliza.

Además, ha cambiado la forma en que el mundo consume los servicios informáticos, haciéndolos más rentables y, al mismo tiempo, haciendo que las organizaciones sean más ágiles para responder a los cambios en sus mercados. Los modelos de despliegue en la nube indican dónde reside la infraestructura, quién es su propietario y quién la administra, y cómo se ponen a disposición de los usuarios los recursos y servicios de la nube. Existen tres tipos de modelos de despliegue en la nube: públicos, privados e híbridos. La nube pública consiste en aprovechar los servicios en la nube a través de una Internet abierta en hardware propiedad del proveedor de la nube, pero otras empresas comparten su uso.

La nube privada significa que la infraestructura de la nube se aprovisiona para el uso exclusivo de una sola organización. Puede funcionar de forma local o puede ser propiedad de un proveedor de servicios, gestionado y operado por él. Y cuando se utiliza una combinación de nubes públicas y privadas, que funcionan juntas sin problemas, se denomina modelo de nube híbrida. Ahora, veamos los tres modelos de servicios en la nube que se basan en las tres capas de un conjunto de computación: infraestructura, plataforma y aplicación. Estos modelos de computación en la nube se denominan acertadamente infraestructura como servicio (o IaaS), plataforma como servicio (o PaaS) y software como servicio (o SaaS).

En un modelo de IaaS, puede acceder a la infraestructura y a los recursos informáticos físicos, como los servidores, las redes, el almacenamiento y el espacio del centro de datos, sin necesidad de administrarlos ni operarlos. En un modelo PaaS, puede acceder a la plataforma que incluye las herramientas de hardware y software que normalmente se necesitan para desarrollar e implementar aplicaciones para los usuarios a través de Internet. Además, un SaaS es un modelo de licencia y entrega de software en el que el software y las aplicaciones se alojan de forma centralizada y se licencian mediante suscripción. A veces se lo denomina «software a pedido».

En este vídeo, aprendió lo siguiente: la computación en nube es la entrega de recursos informáticos bajo demanda a través de Internet mediante un pago por su uso. La computación en nube se compone de cinco características esenciales, tres modelos de implementación y tres modelos de servicio. Las cinco características esenciales de la computación en nube son el autoservicio bajo demanda, el amplio acceso a la red, la agrupación de recursos, la rápida elasticidad y un servicio mesurado. Existen tres tipos de modelos de despliegue en la nube: públicos, privados e híbridos. Además, los tres modelos de servicios en la nube se basan en las tres capas de un conjunto de computación (infraestructura, plataforma y aplicación) y se denominan infraestructura como servicio (o IaaS), plataforma como servicio (o PaaS) y software como servicio (o SaaS).




# Nube para la ciencia de datos


La nube es una bendición para los científicos de datos, principalmente porque permite almacenar datos e información en un sistema de almacenamiento central. Esto elimina las limitaciones físicas de las computadoras y sistemas que utilizan, y les da la posibilidad de implementar capacidades de análisis y almacenamiento de máquinas avanzadas que no necesariamente tienen que ser las suyas o las de su empresa. La nube no solo les permite almacenar grandes cantidades de datos en servidores ubicados en lugares como California o Nevada, sino que también les ofrece la capacidad de implementar algoritmos de computación avanzados y realizar cálculos de alto rendimiento utilizando máquinas que no son las suyas.

Imagina que tienes cierta información que no puedes almacenar localmente, por lo que decides enviarla a un espacio de almacenamiento en la nube. Además, en la nube tienes acceso a los algoritmos que necesitas utilizar. Así, la nube te permite implementar esos algoritmos sobre conjuntos de datos muy grandes, incluso cuando tus propios sistemas, máquinas o entornos informáticos no serían capaces de soportarlo. En este sentido, la nube es una herramienta increíblemente poderosa.

Otra de las características destacadas de la nube es que permite que varias entidades trabajen con los mismos datos al mismo tiempo. Puedes colaborar con colegas de distintas partes del mundo, como Alemania, India o Ghana, de manera simultánea. Todos pueden trabajar en los mismos datos, ya que la información, los algoritmos, las herramientas, las respuestas y los resultados están disponibles en un único lugar centralizado, al que llamamos la nube. Esto hace que el trabajo en equipo y la colaboración global sean mucho más fáciles.

El uso de la nube también te proporciona acceso instantáneo a tecnologías de código abierto, como Apache Spark, sin necesidad de instalarlas y configurarlas localmente. Además, la nube te da acceso a las herramientas y bibliotecas más actualizadas sin que tengas que preocuparte por mantenerlas o asegurarte de que estén actualizadas. Puedes acceder a la nube desde cualquier lugar y en cualquier zona horaria, ya sea desde un portátil, tableta o incluso teléfono, lo que facilita enormemente la colaboración en tiempo real.

Plataformas en la nube ofrecidas por grandes empresas de tecnología permiten que los científicos de datos se familiaricen con estas tecnologías en entornos prediseñados. Empresas como IBM, Amazon y Google ofrecen plataformas como IBM Cloud, Amazon Web Services (AWS) y Google Cloud, respectivamente. IBM también proporciona laboratorios de Skills Network para los estudiantes inscritos en sus portales de aprendizaje, donde tienen acceso a herramientas como Jupyter Notebooks y clústeres Spark para desarrollar sus propios proyectos de ciencia de datos. Con práctica y experiencia, los científicos de datos pronto descubrirán cómo la nube puede mejorar drásticamente su productividad.



# Fundamentos de los macrodatos



En este mundo digital, todos dejamos un rastro. Desde nuestros hábitos de viaje hasta nuestros entrenamientos y entretenimiento, la creciente cantidad de dispositivos conectados a Internet con los que interactuamos a diario genera enormes volúmenes de datos sobre nosotros. A este fenómeno se le conoce como Big Data. Ernst and Young define los macrodatos como «los volúmenes de datos dinámicos, grandes y dispares que crean las personas, las herramientas y las máquinas». Estos datos requieren tecnologías innovadoras y escalables para ser recopilados, alojados y procesados, a fin de obtener información empresarial en tiempo real sobre los consumidores, el riesgo, las ganancias, el rendimiento, la gestión de la productividad y el aumento del valor para los accionistas.

Aunque no existe una definición única de macrodatos, sí hay ciertos elementos comunes en todas ellas, como la velocidad, el volumen, la variedad, la veracidad y el valor. Estas características son conocidas como las "V" del Big Data. La velocidad se refiere a la rapidez con la que se acumulan los datos. Los datos se generan a una velocidad extremadamente alta, en un proceso que nunca se detiene. Las tecnologías de streaming en tiempo real, tanto locales como basadas en la nube, permiten procesar esta información rápidamente. El volumen, por su parte, se refiere a la cantidad de datos almacenados y la escala en la que se generan. Los factores que impulsan este volumen incluyen el aumento de las fuentes de datos, la mayor resolución de los sensores y la infraestructura escalable.

La variedad se refiere a la diversidad de los datos. Los datos estructurados se ajustan perfectamente a las filas y columnas de las bases de datos relacionales, mientras que los datos no estructurados, como los tuits, publicaciones de blogs, imágenes, números y vídeos, no siguen una organización predefinida. Además, la variedad también refleja que los datos provienen de diversas fuentes, como máquinas, personas y procesos, tanto internos como externos a las organizaciones. Las tecnologías móviles, las redes sociales, los dispositivos portátiles, la tecnología geográfica, los vídeos y muchos otros factores son los impulsores de esta variedad.

La veracidad está relacionada con la calidad y el origen de los datos, así como con su conformidad con los hechos y su precisión. Los atributos importantes en este caso incluyen la coherencia, la integridad y la falta de ambigüedad. Además, el costo y la trazabilidad juegan un papel importante en la veracidad de los datos. Con la gran cantidad de datos disponibles, persiste el debate sobre la precisión de la información en la era digital: ¿es la información real o falsa? Finalmente, el valor se refiere a nuestra capacidad y necesidad de convertir los datos en algo útil. Este valor no se limita solo a la ganancia económica, sino que también puede tener beneficios médicos, sociales o incluso proporcionar satisfacción personal a clientes y empleados.

Algunos ejemplos prácticos de las "V" en acción incluyen la velocidad, el volumen, la variedad y la veracidad. Por ejemplo, cada 60 segundos, se suben horas de imágenes a YouTube, lo que genera una cantidad significativa de datos en tiempo real. En cuanto al volumen, la población mundial, que ronda los siete mil millones de personas, utiliza ahora dispositivos digitales como teléfonos móviles, ordenadores y dispositivos portátiles, generando aproximadamente 2,5 trillones de bytes de datos al día, lo que equivale a 10 millones de DVD Blu-ray. En términos de variedad, los datos pueden ser texto, imágenes, vídeos, sonidos, datos de salud de dispositivos portátiles y muchos otros tipos provenientes de la Internet de las cosas.

En cuanto a la veracidad, se estima que el 80% de los datos no están estructurados, lo que plantea el desafío de generar información fiable y precisa a partir de estos datos no organizados. Los científicos de datos de hoy en día deben enfrentar el reto de extraer valor de estos enormes volúmenes de datos. La magnitud de los datos hace que no sea posible utilizar herramientas convencionales de análisis. Sin embargo, herramientas como Apache Spark y Hadoop, junto con su ecosistema, permiten procesar grandes volúmenes de datos mediante recursos informáticos distribuidos, lo que facilita la obtención de nuevos conocimientos y perspectivas. Esto brinda a las organizaciones más formas de conectar con sus clientes y enriquecer los servicios que ofrecen.

Así que la próxima vez que uses tu reloj inteligente, desbloquees tu smartphone o realices un seguimiento de tu entrenamiento, recuerda que tus datos están comenzando un viaje que podría llevarlos por todo el mundo, pasando por el análisis de macrodatos, hasta llegar a ti.



# Ciencia de datos y Big Data


Todo el mundo sabe programar, al menos un poco. Todos tienen al menos algo de experiencia en programación, y algunos tienen mucha. Algunos tienen un máster en Ciencias de la Computación, otros son estudiantes de MBA que provienen de campos técnicos y programan todos los días. Y otros, tal vez, hicieron un curso de programación en la universidad hace cuatro o cinco años, pero al menos pueden pensar computacionalmente, que es lo más importante que necesitan.

La ciencia de datos y el análisis empresarial se han convertido en temas muy candentes en los últimos cuatro o cinco años. Tenemos nuevas herramientas, nuevos enfoques y montones de datos que las técnicas tradicionales simplemente no podían almacenar ni procesar. Creo que se ha corrido la voz. Al principio, las empresas y los empleadores no comprendían la necesidad, especialmente en ciertos campos. Recuerdo que hace tres años hablé con un banco importante sobre los macrodatos, y había un pequeño grupo en el banco donde una persona se esforzaba un poco por crear un pequeño clúster. Ahora, ese mismo banco tiene cinco o seis grandes clústeres de macrodatos, y en ellos colocan todos los datos de sus tarjetas de crédito, los procesan de diversas formas y utilizan todo tipo de técnicas de ciencia de datos.

Hace dos años, o creo que fue el año pasado, nuestro curso de pregrado sobre datos tenía 28 estudiantes. Este año tiene 140. Eso significa que los padres están empezando a entenderlo, porque una cosa que entendemos con nuestros estudiantes universitarios es que los padres que pagan matrículas muy altas les dicen a sus hijos e hijas: «Saben, deberían ser contadores», o «Deberías dedicarte a los servicios financieros» o «al marketing», porque ahí es donde está el dinero. Ahora se están dando cuenta de que tal vez deberían tomar más clases de STEM en el instituto y prepararse para dedicarse a la ciencia de datos o a campos en los que la analítica es cada vez más importante.

Depende de quién seas (risas). Tengo mi propia definición de big data. Mi definición de macrodatos son los datos que son lo suficientemente grandes y tienen un volumen y una velocidad suficientes como para que no puedan gestionarse con los sistemas de bases de datos tradicionales. Algunos de nuestros estadísticos piensan que los macrodatos son algo que no cabe en una memoria USB. Los macrodatos, para mí, los creó Google. Cuando Google intentaba averiguar cómo estaban, cuando Larry Page y Sergey Brin querían, básicamente, resolver su algoritmo de posicionamiento de páginas, no había nada al respecto. Intentaban almacenar todas las páginas web del mundo, pero no existía la tecnología ni la forma de hacerlo, por lo que desarrollaron este enfoque, que ahora se ha convertido en Hadoop, que lo ha copiado. Aquí es donde se encuentran todos estos grandes clústeres de macrodatos.

Sin embargo, el big data ahora también se ha expandido, y ahora abarca cómo se analiza. Existen nuevas técnicas analíticas y estadísticas para gestionar estos conjuntos de datos tan, muy, muy grandes. Probablemente lleguemos al aprendizaje profundo en algún momento.



# ¿Qué es Hadoop?


Tradicionalmente, en el cálculo y el procesamiento de datos, llevábamos los datos a la computadora. Querrías programar e incorporar los datos al programa.

En un clúster de macrodatos, lo que les ocurrió a Larry Page y Sergey Brin es muy simple: cogían los datos, los cortaban en pedazos, los distribuían y replicaban cada parte o triplicaban cada parte, y luego los enviaban a miles de ordenadores. Primero eran cientos, pero luego ahora son miles, ahora son decenas de miles. Y luego enviaban el mismo programa a todos los ordenadores del clúster. Cada ordenador ejecutaba el programa en su pequeña parte del archivo y enviaba los resultados. Luego, los resultados se ordenarían y esos resultados se redistribuirían a otro proceso. El primer proceso se denomina proceso de mapa o mapeador y el segundo se denomina proceso de reducción. Son conceptos bastante simples, pero resultó que se podían gestionar montones y montones de tipos diferentes de problemas y conjuntos de datos muy, muy, muy grandes.

Lo bueno de estos clústeres de macrodatos es que se escalan de forma lineal. Si tienes el doble de servidores, obtienes el doble de rendimiento y puedes gestionar el doble de datos. Esto rompió un cuello de botella para todas las principales empresas de redes sociales. Luego, Yahoo se unió. Yahoo contrató a alguien llamado Doug Cutting, que había estado trabajando en un clon o una copia de la arquitectura de big data de Google, y ahora se llama Hadoop. Si buscas en Google Hadoop, verás que ahora es un término muy popular y hay muchísimas empresas que tienen algún tipo de presencia en el mundo de los macrodatos.

La mayoría de los componentes de la ciencia de datos existen desde hace muchísimas décadas, pero ahora todos se están uniendo con algunos matices nuevos. En la base de la ciencia de datos están la probabilidad y la estadística, álgebra lineal, programación y bases de datos. Todos han estado aquí. Pero lo que ha sucedido ahora es que tenemos las capacidades computacionales para aplicar algunas técnicas nuevas: el aprendizaje automático. Ahora podemos tomar conjuntos de datos muy grandes y, en lugar de tomar una muestra e intentar probar alguna hipótesis, podemos tomar conjuntos de datos muy, muy grandes y buscar patrones. Así que, retrocedamos un nivel para pasar de la comprobación de hipótesis a la búsqueda de patrones que tal vez generen hipótesis.

Esto puede molestar a algunos estadísticos tradicionales, y a veces les molesta mucho saber que se supone que se debe tener una hipótesis que no sea independiente de los datos y luego ponerla a prueba. Sin embargo, cuando se pusieron en marcha algunas de estas técnicas de aprendizaje automático, realmente fueron la única forma de analizar algunos de estos enormes conjuntos de datos de redes sociales. Lo que hemos visto es que la combinación de áreas tradicionales (ciencias de la computación, probabilidad, estadística y matemáticas) se unen en lo que llamamos Ciencias de la Decisión. Nuestro departamento en Stern está muy bien posicionado entre las escuelas de negocios, ya que somos una de las pocas escuelas de negocios que tiene un verdadero departamento de estadística con verdaderos especialistas en estadística de nivel doctoral, un departamento de gestión de operaciones y un departamento de sistemas de información. Así que tenemos una amplia gama de informáticos, estadísticos e investigadores de operaciones. Estábamos perfectamente posicionados, al igual que otras dos escuelas de negocios, para subirse a este tren y decir: esto es Decision Sciences.

Foster Provost, quien trabaja en mi departamento, fue el primer director del Centro de Ciencia de Datos de la Universidad de Nueva York hace cuatro o cinco años. Creo que este es uno de esos casos en los que puedes buscar en Google y ver con qué frecuencia ocurre y no ves casi nada, y luego de repente un pico. Lo mismo que se podía ver con los macrodatos hace unos siete u ocho años. Así que la ciencia de datos es un término del que probablemente no habías oído hablar hace cinco años.

La primera pregunta es: ¿qué es? Y creo que los profesores y todos siguen intentando entender exactamente qué es la analítica empresarial y qué es la ciencia de datos. No cabe duda de que conocemos sus componentes, pero se está transformando, cambiando y creciendo. En los últimos tres años, el aprendizaje profundo se ha añadido a la mezcla. Las redes neuronales existen desde hace 20 o 30 años. Hace 20 años, enseñaba redes neuronales en clase y no se podía hacer mucho con ellas. Y ahora, algunos investigadores han creado redes neuronales multicapa, particularmente en la Universidad de Toronto. Y esa tecnología se está expandiendo rápidamente y está siendo utilizada por Google, Facebook y muchas empresas.




# Herramientas de procesamiento de Big Data: Hadoop, HDFS, Hive y Spark




Las tecnologías de procesamiento de big data proporcionan formas de trabajar con grandes conjuntos de datos estructurados, semiestructurados y no estructurados para que el valor se pueda derivar de los macrodatos. En algunos de los otros vídeos, analizamos las tecnologías de macrodatos, como las bases de datos NoSQL y los lagos de datos. En este vídeo, vamos a hablar de tres tecnologías de código abierto y del papel que desempeñan en el análisis de macrodatos: Apache Hadoop, Apache Hive y Apache Spark.

Hadoop es un conjunto de herramientas que proporciona almacenamiento y procesamiento distribuidos de macrodatos. Hive es un almacén de datos para consultas y análisis de datos creado sobre Hadoop. Spark es un marco de análisis de datos distribuido diseñado para realizar análisis de datos complejos en tiempo real.

Hadoop, un marco de código abierto basado en Java, permite el almacenamiento y el procesamiento distribuidos de grandes conjuntos de datos en clústeres de ordenadores. En el sistema distribuido de Hadoop, un nodo es un único ordenador y un conjunto de nodos forma un clúster. Hadoop puede ampliarse de un solo nodo a cualquier número de nodos, cada uno de los cuales ofrece almacenamiento y computación locales. Hadoop proporciona una solución fiable, escalable y rentable para almacenar datos sin requisitos de formato.

Con Hadoop, puede: incorporar formatos de datos emergentes, como la transmisión de audio, vídeo, opiniones en redes sociales y datos de secuencias de clics, junto con datos estructurados, semiestructurados y no estructurados que no se utilizan tradicionalmente en un almacén de datos. Además, proporciona acceso de autoservicio en tiempo real a todas las partes interesadas. También optimiza y agiliza los costes del almacén de datos empresarial consolidando los datos de toda la organización y trasladando los datos "inactivos", es decir, los datos que no se utilizan con frecuencia, a un sistema basado en Hadoop.

Uno de los cuatro componentes principales de Hadoop es el sistema de archivos distribuido de Hadoop, o HDFS, que es un sistema de almacenamiento para macrodatos que se ejecuta en varios equipos básicos conectados a través de una red. El HDFS proporciona un almacenamiento de macrodatos escalable y fiable mediante la partición de los archivos en varios nodos. Divide archivos de gran tamaño en varios ordenadores, lo que permite el acceso paralelo a ellos. Por lo tanto, los cálculos pueden ejecutarse en paralelo en cada nodo donde se almacenan los datos. También replica bloques de archivos en diferentes nodos para evitar la pérdida de datos, lo que lo hace tolerante a errores.

Vamos a entender esto a través de un ejemplo. Considere un archivo que incluya los números de teléfono de todos los habitantes de los Estados Unidos; los números de las personas cuyo apellido comience por A podrían almacenarse en el servidor 1, B en el servidor 2, etc. Con Hadoop, partes de esta agenda se almacenarían en todo el clúster. Para reconstruir toda la agenda telefónica, el programa necesitaría los bloques de todos los servidores del clúster. HDFS también replica estas piezas más pequeñas en dos servidores adicionales de forma predeterminada, lo que garantiza la disponibilidad cuando un servidor falla.

Además de una mayor disponibilidad, HDFS ofrece múltiples ventajas. Permite que el clúster de Hadoop divida el trabajo en partes más pequeñas y ejecute esas tareas en todos los servidores del clúster para mejorar la escalabilidad. También obtiene las ventajas de la ubicación de los datos, lo que significa acercar el cálculo al nodo en el que residen los datos. Esto es fundamental cuando se trabaja con conjuntos de datos de gran tamaño, ya que minimiza la congestión de la red y aumenta el rendimiento.

Algunas de las otras ventajas que se obtienen al usar HDFS incluyen una recuperación rápida en caso de fallas de hardware, ya que HDFS está diseñado para detectar fallas y recuperarse automáticamente. También tiene acceso a la transmisión de datos, ya que admite altas velocidades de procesamiento de datos. Además, puede alojar grandes conjuntos de datos, ya que puede ampliarse a cientos de nodos en un único clúster. Es portátil, ya que el HDFS se puede transportar a varias plataformas de hardware y es compatible con una variedad de sistemas operativos subyacentes.

Hive es un software de almacenamiento de datos de código abierto para leer, escribir y administrar archivos de grandes conjuntos de datos que se almacenan directamente en HDFS o en otros sistemas de almacenamiento de datos, como Apache HBase. Hadoop está diseñado para escaneos secuenciales prolongados y, dado que Hive está basado en Hadoop, las consultas tienen una latencia muy alta, lo que hace que Hive sea menos adecuado para aplicaciones que necesitan tiempos de respuesta muy rápidos.

Además, Hive se basa en la lectura y, por lo tanto, no es adecuado para el procesamiento de transacciones, que normalmente implica un alto porcentaje de operaciones de escritura. Hive es más adecuado para tareas de almacenamiento de datos, como la ETL, la elaboración de informes y el análisis de datos, e incluye herramientas que permiten un fácil acceso a los datos mediante SQL.

Esto nos lleva a Spark, un motor de procesamiento de datos de uso general diseñado para extraer y procesar grandes volúmenes de datos para una amplia gama de aplicaciones, incluidas la analítica interactiva, el procesamiento de flujos, el aprendizaje automático, la integración de datos y la ETL. Aprovecha el procesamiento en memoria para aumentar significativamente la velocidad de los cálculos y el almacenamiento en el disco solo cuando la memoria es limitada.

Spark cuenta con interfaces para los principales lenguajes de programación, como Java, Scala, Python, R y SQL. Puede ejecutarse con su tecnología de agrupamiento en clústeres independiente, así como sobre otras infraestructuras, como Hadoop. Además, puede acceder a los datos de una gran variedad de fuentes de datos, incluidas HDFS y Hive, lo que lo hace muy versátil. La capacidad de procesar datos de streaming con rapidez y realizar análisis complejos en tiempo real es el principal caso de uso de Apache Spark.

# Estableciendo Metas de Minería de Datos

El primer paso en la minería de datos requiere que establezcas metas para el ejercicio. Obviamente, debes identificar las preguntas clave que necesitan ser respondidas. Sin embargo, más allá de identificar las preguntas clave, están las preocupaciones sobre los costos y beneficios del ejercicio. Además, debes determinar, de antemano, el nivel esperado de precisión y utilidad de los resultados obtenidos de la minería de datos. Si el dinero no fuera un objeto, podrías invertir tantos fondos como fuera necesario para obtener las respuestas requeridas. Sin embargo, el equilibrio entre costo y beneficio siempre es fundamental para determinar las metas y el alcance del ejercicio de minería de datos. El nivel de precisión esperado de los resultados también influye en los costos. Altos niveles de precisión en la minería de datos costarían más y viceversa. Además, más allá de cierto nivel de precisión, no obtienes mucho del ejercicio, dado el rendimiento decreciente. Así, los compromisos entre costo y beneficio para el nivel deseado de precisión son consideraciones importantes para las metas de minería de datos.

## Selección de Datos

La salida de un ejercicio de minería de datos depende en gran medida de la calidad de los datos que se utilizan. A veces, los datos están fácilmente disponibles para su posterior procesamiento. Por ejemplo, los minoristas a menudo poseen grandes bases de datos de compras de clientes y demografía. Por otro lado, los datos pueden no estar fácilmente disponibles para la minería de datos. En tales casos, debes identificar otras fuentes de datos o incluso planificar nuevas iniciativas de recolección de datos, incluidas encuestas. El tipo de datos, su tamaño y la frecuencia de recolección tienen un impacto directo en el costo del ejercicio de minería de datos. Por lo tanto, identificar el tipo correcto de datos necesarios para la minería de datos que pueda responder a las preguntas a costos razonables es fundamental.

## Preprocesamiento de Datos

El preprocesamiento de datos es un paso importante en la minería de datos. A menudo, los datos en bruto son desordenados, conteniendo datos erróneos o irrelevantes. Además, incluso con datos relevantes, a veces falta información. En la etapa de preprocesamiento, identificas los atributos irrelevantes de los datos y eliminas dichos atributos de la consideración futura. Al mismo tiempo, es necesario identificar los aspectos erróneos del conjunto de datos y marcarlos como tales. Por ejemplo, el error humano podría llevar a una fusión inadvertida o a un análisis incorrecto de la información entre columnas. Los datos deben someterse a verificaciones para garantizar su integridad. Por último, debes desarrollar un método formal para tratar los datos faltantes y determinar si los datos faltan de manera aleatoria o sistemática.

Si los datos faltaran de manera aleatoria, un conjunto simple de soluciones sería suficiente. Sin embargo, cuando los datos faltan de manera sistemática, debes determinar el impacto de los datos faltantes en los resultados. Por ejemplo, un subconjunto particular de individuos en un gran conjunto de datos puede haber rechazado revelar su ingreso. Los hallazgos que dependen del ingreso de un individuo como entrada excluirían los detalles de aquellos individuos cuyo ingreso no fue reportado. Esto llevaría a sesgos sistemáticos en el análisis. Por lo tanto, debes considerar de antemano si las observaciones o variables que contienen datos faltantes deben ser excluidas de todo el análisis o de partes de él.

## Transformación de Datos

Después de que se han retenido los atributos relevantes de los datos, el siguiente paso es determinar el formato apropiado en el que deben almacenarse los datos. Una consideración importante en la minería de datos es reducir el número de atributos necesarios para explicar los fenómenos. Esto puede requerir la transformación de datos. Los algoritmos de reducción de datos, como el Análisis de Componentes Principales (demostrado y explicado más adelante en el capítulo), pueden reducir el número de atributos sin una pérdida significativa de información. Además, puede ser necesario transformar variables para ayudar a explicar el fenómeno que se está estudiando. Por ejemplo, el ingreso de un individuo puede registrarse en el conjunto de datos como ingreso salarial; ingresos de otras fuentes, como propiedades en alquiler; pagos de apoyo del gobierno, y similares. Agregar los ingresos de todas las fuentes desarrollará un indicador representativo del ingreso individual.

A menudo es necesario transformar variables de un tipo a otro. Puede ser prudente transformar la variable continua de ingreso en una variable categórica donde cada registro en la base de datos se identifique como individuo de ingresos bajos, medios y altos. Esto podría ayudar a capturar las no linealidades en los comportamientos subyacentes.

## Almacenamiento de Datos

Los datos transformados deben almacenarse en un formato que facilite la minería de datos. Los datos deben almacenarse en un formato que otorgue privilegios de lectura/escritura irrestrictos e inmediatos al científico de datos. Durante la minería de datos, se crean nuevas variables, las cuales se escriben de nuevo en la base de datos original, por lo que el esquema de almacenamiento de datos debe facilitar la lectura y escritura de manera eficiente en la base de datos. También es importante almacenar los datos en servidores o medios de almacenamiento que mantengan la seguridad de los datos y que eviten que el algoritmo de minería de datos busque innecesariamente fragmentos de datos dispersos en diferentes servidores o medios de almacenamiento. La seguridad y privacidad de los datos deben ser una preocupación principal al almacenar datos.

## Minería de Datos

Después de que los datos se procesan, transforman y almacenan adecuadamente, están sujetos a la minería de datos. Este paso abarca métodos de análisis de datos, incluidos métodos paramétricos y no paramétricos, así como algoritmos de aprendizaje automático. Un buen punto de partida para la minería de datos es la visualización de datos. Las vistas multidimensionales de los datos utilizando las capacidades avanzadas de gráficos del software de minería de datos son muy útiles para desarrollar una comprensión preliminar de las tendencias ocultas en el conjunto de datos.

_Las secciones posteriores de este capítulo detallan los algoritmos y métodos de minería de datos._

## Evaluación de Resultados de Minería de Datos

Después de que se han extraído los resultados de la minería de datos, se realiza una evaluación formal de los resultados. La evaluación formal podría incluir la prueba de las capacidades predictivas de los modelos en datos observados para ver cuán efectivos y eficientes han sido los algoritmos en reproducir datos. Esto se conoce como un “pronóstico dentro de la muestra”. Además, los resultados se comparten con los principales interesados para obtener comentarios, que luego se incorporan en las iteraciones posteriores de la minería de datos para mejorar el proceso.

La minería de datos y la evaluación de los resultados se convierten en un proceso iterativo de tal manera que los analistas utilizan algoritmos mejores y mejorados para mejorar la calidad de los resultados generados a la luz de los comentarios recibidos de los principales interesados.


![[Pasted image 20241218160503.png]]


![[Pasted image 20241218160510.png]]


![[Pasted image 20241218160517.png]]




# Resumen de la lección: Big Data y minería de datos


En esta lección sobre macrodatos y minería de datos, se explora cómo los macrodatos están impactando diversos aspectos de la sociedad, desde las operaciones empresariales hasta los deportes, y cómo impulsan la transformación digital. Se destacan las principales características de los macrodatos, como el valor, volumen, velocidad, variedad y veracidad, que permiten obtener información valiosa en tiempo real para la toma de decisiones empresariales y la mejora de la productividad y la gestión. La disponibilidad de grandes cantidades de datos, provenientes de personas, herramientas y máquinas, ha generado la necesidad de tecnologías nuevas y escalables. Además, el desarrollo de la computación en la nube ha facilitado el manejo de estos grandes volúmenes de datos, ofreciendo acceso a recursos informáticos bajo demanda. Las características esenciales de la computación en la nube, como el acceso bajo demanda, la elasticidad y el servicio mesurado, permiten un procesamiento de datos eficiente y rentable.

Las herramientas de código abierto como Apache Hadoop, Apache Hive y Apache Spark son fundamentales para la gestión de los macrodatos. Hadoop permite el almacenamiento y procesamiento distribuido en clústeres de ordenadores, mientras que Hive actúa como un almacén de datos para realizar consultas y análisis, y Spark proporciona un motor de procesamiento de datos para extraer y procesar grandes volúmenes de información. Estos avances requieren de un proceso de minería de datos que se compone de seis pasos: establecimiento de objetivos, selección de fuentes de datos, preprocesamiento, transformación, extracción y evaluación. El proceso debe ser iterativo, ajustándose según los resultados obtenidos, con el objetivo de optimizar la eficiencia de los algoritmos y modelos predictivos. En resumen, los macrodatos, junto con las tecnologías en la nube y las herramientas de código abierto, están transformando todos los sectores y la vida cotidiana, facilitando la extracción de información útil para la toma de decisiones.



![[Pasted image 20241218161413.png]]


![[Pasted image 20241218161419.png]]


![[Pasted image 20241218161426.png]]

**Glosario de la Lección sobre Macrodatos y Minería de Datos**

¡Bienvenido! Este glosario contiene muchos de los términos utilizados en esta lección. Estos términos son importantes para que los reconozcas cuando trabajes en la industria, participes en grupos de usuarios o en otros programas de certificación.

|**Término**|**Definición**|**Video donde se introduce el término**|
|---|---|---|
|**Análisis**|El proceso de examinar datos para sacar conclusiones y tomar decisiones informadas es un aspecto fundamental de la ciencia de datos, que implica análisis estadísticos e información basada en datos.|**Data Scientists at New York University**|
|**Big Data (Macrodatos)**|Grandes cantidades de datos estructurados, semi-estructurados y no estructurados caracterizados por su volumen, velocidad, variedad y valor, que, cuando se analizan, pueden proporcionar ventajas competitivas y generar transformaciones digitales.|**How Big Data is Driving Digital Transformation**|
|**Big Data Cluster (Clúster de Macrodatos)**|Un entorno de computación distribuida compuesto por miles o decenas de miles de computadoras interconectadas que almacenan y procesan grandes conjuntos de datos.|**What is Hadoop?**|
|**Acceso a la Red Amplio**|La capacidad de acceder a los recursos de la nube mediante mecanismos y plataformas estándar, como dispositivos móviles, laptops y estaciones de trabajo a través de redes.|**Introduction to Cloud**|
|**Chief Data Officer (CDO)**|Un rol emergente responsable de supervisar iniciativas, gobernanza y estrategias relacionadas con los datos, asegurando que los datos jueguen un papel central en los esfuerzos de transformación digital.|**How Big Data is Driving Digital Transformation**|
|**Chief Information Officer (CIO)**|Un ejecutivo responsable de gestionar la tecnología de la información y los sistemas informáticos de una organización, contribuyendo a los aspectos tecnológicos de la transformación digital.|**How Big Data is Driving Digital Transformation**|
|**Computación en la Nube**|La entrega de recursos informáticos bajo demanda, incluidos redes, servidores, almacenamiento, aplicaciones, servicios y centros de datos, a través de Internet con pago por uso.|**Introduction to Cloud**|
|**Modelos de Implementación en la Nube**|Categorías que indican dónde se encuentra la infraestructura de la nube, quién la gestiona y cómo los recursos y servicios de la nube están disponibles para los usuarios, incluidos los modelos público, privado e híbrido.|**Introduction to Cloud**|
|**Modelos de Servicio en la Nube**|Modelos basados en las capas de una pila informática, incluidos Infraestructura como Servicio (IaaS), Plataforma como Servicio (PaaS) y Software como Servicio (SaaS), que representan diferentes ofertas de computación en la nube.|**Introduction to Cloud**|
|**Hardware Común**|Componentes estándar de hardware de uso general utilizados en un clúster de macrodatos, ofreciendo soluciones rentables para almacenamiento y procesamiento sin depender de hardware especializado.|**What is Hadoop?**|
|**Algoritmos de Datos**|Procedimientos computacionales y modelos matemáticos utilizados para procesar y analizar datos accesibles en la nube, que los científicos de datos pueden desplegar en grandes conjuntos de datos de manera eficiente.|**Cloud for Data Science**|
|**Replicación de Datos**|Una estrategia en la que los datos se duplican en múltiples nodos de un clúster para garantizar la durabilidad y disponibilidad de los datos, reduciendo el riesgo de pérdida debido a fallos de hardware.|**What is Hadoop?**|
|**Ciencia de Datos**|Un campo interdisciplinario que implica extraer información y conocimiento de los datos utilizando diversas técnicas, incluidas la programación, la estadística y las herramientas analíticas.|**Data Scientists at New York University**|
|**Deep Learning (Aprendizaje Profundo)**|Un subconjunto del aprendizaje automático que involucra redes neuronales artificiales inspiradas en el cerebro humano, capaces de aprender y tomar decisiones complejas a partir de los datos por sí solas.|**Data Scientists at New York University**|
|**Cambio Digital**|La integración de la tecnología digital en los procesos y operaciones comerciales conduce a mejoras e innovaciones en cómo las organizaciones operan y entregan valor a los clientes.|**How Big Data is Driving Digital Transformation**|
|**Transformación Digital**|Un cambio organizacional estratégico y cultural impulsado por la ciencia de datos, especialmente Big Data, para integrar la tecnología digital en todas las áreas de la organización, resultando en cambios operativos fundamentales y en la entrega de valor.|**How Big Data is Driving Digital Transformation**|
|**Datos Distribuidos**|La práctica de dividir los datos en fragmentos más pequeños y distribuirlos a través de múltiples computadoras dentro de un clúster, lo que permite el procesamiento paralelo para el análisis de datos.|**What is Hadoop?**|
|**Hadoop**|Un marco distribuido de almacenamiento y procesamiento utilizado para manejar y analizar grandes conjuntos de datos, especialmente adecuado para el análisis de Big Data y aplicaciones de ciencia de datos.|**Data Scientists at New York University**|
|**Hadoop Distributed File System (HDFS)**|Un sistema de almacenamiento dentro del marco Hadoop que particiona y distribuye archivos a través de múltiples nodos, facilitando el acceso paralelo a los datos y la tolerancia a fallos.|**What is Hadoop?**|
|**Infraestructura como Servicio (IaaS)**|Un modelo de servicio en la nube que proporciona acceso a infraestructura informática, incluidos servidores, almacenamiento y redes, sin necesidad de que los usuarios los gestionen o operen.|**Introduction to Cloud**|
|**Marco Basado en Java**|Hadoop está implementado en Java, un lenguaje de programación de código abierto y de alto nivel, que proporciona la base para construir soluciones de almacenamiento y procesamiento distribuidos.|**Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark**|
|**Proceso de Mapeo**|El paso inicial en el modelo de programación MapReduce de Hadoop, donde los datos se procesan en paralelo en nodos individuales del clúster, utilizado comúnmente para tareas de transformación de datos.|**What is Hadoop?**|
|**Servicio Medido**|Una característica donde los usuarios son facturados por los recursos de la nube según su uso real, con la utilización de los recursos monitorizada, medida e informada de forma transparente.|**Introduction to Cloud**|
|**Servicio Automático Bajo Demanda**|La capacidad para que los usuarios accedan y aprovisionen recursos en la nube como potencia de procesamiento, almacenamiento y redes usando interfaces simples, sin interacción humana con los proveedores de servicios.|**Introduction to Cloud**|
|**Elasticidad Rápida**|La capacidad de escalar rápidamente los recursos de la nube hacia arriba o hacia abajo según la demanda, permitiendo a los usuarios acceder a más recursos cuando los necesiten y liberarlos cuando no los usen.|**Introduction to Cloud**|
|**Proceso de Reducción**|El segundo paso en el modelo MapReduce de Hadoop, donde los resultados del proceso de mapeo se agregan y procesan más para producir la salida final, generalmente utilizada para análisis.|**What is Hadoop?**|
|**Replicación**|El acto de crear copias de piezas de datos dentro de un clúster de Big Data mejora la tolerancia a fallos y garantiza la disponibilidad de los datos en caso de fallos de hardware o nodos.|**What is Hadoop?**|
|**Agrupación de Recursos**|Una característica de la nube en la que los recursos informáticos se comparten y asignan dinámicamente a varios consumidores, promoviendo economías de escala y eficiencia en costos.|**Introduction to Cloud**|
|**Laboratorios de la Red de Habilidades (SN Labs)**|Recursos de aprendizaje proporcionados por IBM, incluidos herramientas como Jupyter Notebooks y clústeres Spark, disponibles para los aprendices en proyectos de ciencia de datos en la nube y desarrollo de habilidades.|**Cloud for Data Science**|
|**Desbordamiento a Disco**|Una técnica utilizada en situaciones de memoria limitada donde los datos se escriben temporalmente en almacenamiento en disco cuando los recursos de memoria se agotan, asegurando un procesamiento ininterrumpido.|**Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark**|
|**Clases STEM**|Cursos de Ciencia, Tecnología, Ingeniería y Matemáticas (STEM) generalmente impartidos en las escuelas secundarias que preparan a los estudiantes para carreras técnicas, incluida la ciencia de datos.|**Data Scientists at New York University**|
|**Variedad**|La diversidad de tipos de datos, incluidos datos estructurados y no estructurados de diversas fuentes como texto, imágenes, videos, y más, lo que plantea desafíos en la gestión de datos.|**Foundations of Big Data**|
|**Velocidad**|La rapidez con la que se acumulan y generan los datos, a menudo en tiempo real o casi en tiempo real, lo que impulsa la necesidad de procesamiento y análisis rápidos de los datos.|**Foundations of Big Data**|
|**Veracidad**|La calidad y exactitud de los datos, asegurando que se ajusten a los hechos y sean consistentes, completos y libres de ambigüedades, lo que impacta en la fiabilidad y confianza de los datos.|**Foundations of Big Data**|
|**Sistema de Seguimiento de Video**|Un sistema utilizado para capturar y analizar datos de video de los juegos, permitiendo un análisis profundo de los movimientos de los jugadores y la dinámica del juego, contribuyendo a la toma de decisiones basada en datos en los deportes.|**How Big Data is Driving Digital Transformation**|
|**Volumen**|La escala de los datos generados y almacenados es impulsada por un aumento en las fuentes de datos, sensores de mayor resolución e infraestructura escalable.|**Foundations of Big Data**|
|**V's de Big Data**|Un conjunto de características comunes a través de las definiciones de Big Data, que incluye Velocidad, Volumen, Variedad, Veracidad y Valor, destacando la rápida generación, escala, diversidad, calidad y valor de los datos.|**Foundations of Big Data**|



![[Pasted image 20241218161911.png]]



![[Pasted image 20241218161918.png]]


![[Pasted image 20241218161924.png]]


![[Pasted image 20241218161932.png]]


![[Pasted image 20241218161941.png]]















