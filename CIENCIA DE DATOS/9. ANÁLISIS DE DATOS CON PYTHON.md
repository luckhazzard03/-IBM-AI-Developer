

# Comprender los datos



El conjunto de datos que vamos a analizar está relacionado con los precios de los coches usados y fue proporcionado por Jeffrey C. Schlimmer. Este conjunto de datos está en formato CSV, lo que significa que los valores están separados por comas, facilitando su importación en diversas herramientas o aplicaciones. Cada línea del archivo representa una fila dentro del conjunto de datos. En el laboratorio práctico de este módulo, tendrás la oportunidad de descargar y utilizar este archivo CSV.

Un detalle importante es la primera fila del conjunto de datos. En muchos casos, la primera fila suele ser un encabezado que contiene los nombres de las columnas, pero en este caso, no es así. En vez de ser una fila de encabezado, es simplemente otra fila de datos. A continuación, se ofrece la documentación sobre lo que representa cada una de las 26 columnas. Aunque hay varias columnas, repasaremos algunos de los nombres de las más relevantes. Si deseas más detalles, puedes consultar el enlace que se encuentra en la parte inferior de la diapositiva.

Uno de los primeros atributos es la simbología, que corresponde al nivel de riesgo de seguro de un automóvil. A cada coche se le asigna un símbolo de factor de riesgo basado en su precio. Si un coche tiene un riesgo más alto, el símbolo se ajusta hacia un valor más alto en la escala. Un valor de +3 indica que el automóvil es riesgoso, mientras que -3 sugiere que probablemente sea seguro. Otro atributo importante es las pérdidas normalizadas, que refleja la cantidad media de pérdidas por año para un vehículo asegurado. Este valor se normaliza para diferentes categorías de coches (por ejemplo, coches de dos puertas, pequeños, camionetas, vehículos deportivos, etc.), y refleja la pérdida media por automóvil y año. Los valores de este atributo oscilan entre 65 y 256.

El resto de los atributos también son bastante comprensibles. No obstante, si deseas más información, puedes consultar el enlace en la diapositiva. Una vez que comprendamos el significado de todas las características, podemos observar que el atributo número 26 es el precio. Este es el valor objetivo o la etiqueta del conjunto de datos. Es decir, el precio es el valor que intentaremos predecir a partir de los demás atributos del conjunto de datos, tales como la simbología, las pérdidas normalizadas, las ganancias, entre otros.

El objetivo de este proyecto es predecir el precio de un automóvil basándonos en las demás características disponibles en el conjunto de datos. Cabe destacar que este conjunto de datos corresponde al año 1985, por lo que los precios de los coches pueden parecer algo bajos en comparación con los estándares actuales. Sin embargo, el propósito de este ejercicio es aprender a analizar y trabajar con los datos.


# Paquetes Python para la ciencia de datos


Para realizar un análisis de datos en Python, es importante conocer los principales paquetes que facilitan este proceso. Una biblioteca de Python es un conjunto de funciones y métodos que permiten realizar diversas acciones sin necesidad de escribir todo el código desde cero. Las bibliotecas suelen incluir módulos integrados que proporcionan funcionalidades específicas que puedes utilizar directamente. Existen muchas bibliotecas en Python, y en este caso, las hemos dividido en tres grupos principales.

El primer grupo está compuesto por las **bibliotecas de computación científica**. Una de las más importantes es **Pandas**, que proporciona estructuras de datos y herramientas eficaces para la manipulación y análisis de datos. Su principal instrumento es el **DataFrame**, una tabla bidimensional que consta de etiquetas de columnas y filas, lo que facilita el acceso rápido a los datos estructurados. Además, Pandas está diseñado para ofrecer una funcionalidad de indexación sencilla, lo que mejora la eficiencia en el manejo de grandes volúmenes de datos.

Otra biblioteca clave es **NumPy**, que se especializa en el manejo de matrices para las entradas y salidas de datos. NumPy también permite extender estos objetos a matrices multidimensionales y, con algunos ajustes en el código, facilita un procesamiento rápido de matrices.

Por su parte, **SciPy** es una biblioteca que incluye funciones matemáticas avanzadas, además de herramientas para la visualización de datos. La visualización es fundamental en el análisis de datos, ya que ayuda a comunicar los resultados de manera clara y efectiva. Para esto, existen varias bibliotecas especializadas.

En el ámbito de la visualización de datos, **Matplotlib** es una de las bibliotecas más conocidas. Es ideal para crear gráficos y diagramas personalizables. Ofrece una gran flexibilidad para ajustar los gráficos según las necesidades del usuario. Además, existe **Seaborn**, una biblioteca de alto nivel que se basa en Matplotlib. Seaborn facilita la creación de gráficos complejos, como mapas de calor, series temporales y gráficos de violín, con poco código.

Finalmente, en el contexto del **aprendizaje automático**, existen bibliotecas que nos permiten desarrollar modelos para obtener predicciones a partir de nuestros conjuntos de datos. Estas bibliotecas algorítmicas abordan tareas de aprendizaje automático que varían desde las más sencillas hasta las más complejas. Un ejemplo es **Scikit-learn**, que proporciona herramientas para modelado estadístico, incluyendo regresión, clasificación y agrupamiento. Esta biblioteca está basada en **NumPy**, **SciPy** y **Matplotlib**, lo que asegura que se pueda integrar fácilmente con otras herramientas de análisis de datos.

Otro paquete útil es **Statsmodels**, que permite explorar datos, estimar modelos estadísticos y realizar pruebas estadísticas. Esta herramienta es especialmente valiosa para quienes necesitan realizar análisis más complejos y pruebas formales sobre sus datos.

En resumen, estas bibliotecas de Python forman un conjunto poderoso de herramientas que permiten realizar análisis de datos desde la manipulación básica hasta el modelado y la visualización avanzada.


# Importar y exportar datos en Python


En este caso, vamos a ver cómo leer datos utilizando el paquete **Pandas** de Python, una herramienta clave para el análisis de datos. Una vez que tengamos los datos cargados en Python, podremos realizar diversos procedimientos de análisis según nuestras necesidades. El proceso de adquisición de datos consiste en cargar y leer información desde varias fuentes en un cuaderno de Python. Para poder leer cualquier tipo de dato con Pandas, hay dos aspectos fundamentales a tener en cuenta: el **formato** del archivo y la **ruta** donde se encuentra.

El formato hace referencia a la manera en que los datos están codificados. Normalmente, podemos identificar el formato de un archivo observando su extensión. Algunos formatos comunes incluyen **CSV**, **JSON**, **XLSX** o **HDF**, entre otros. La ruta del archivo indica el lugar donde los datos están almacenados, que puede ser en nuestra computadora o, en muchos casos, en línea a través de Internet. En nuestro ejemplo, utilizamos un conjunto de datos de coches usados que se obtuvo desde una dirección web, la cual aparece en la diapositiva.

Cuando Jerry ingresó la dirección web en su navegador, pudo ver algo similar a lo siguiente: cada fila representa un punto de datos, y cada propiedad está separada por comas. Esto nos sugiere que el formato de los datos es **CSV** (valores separados por comas). Al principio, estos números no tienen mucho sentido para los seres humanos, pero una vez que los leemos con Pandas, podemos empezar a entenderlos mejor.

Para leer estos datos en Pandas, utilizamos el método **read_csv**, que permite importar archivos CSV y convertirlos en un **DataFrame**, una estructura de datos bidimensional en Pandas. El proceso de lectura en Pandas es rápido y sencillo. Primero, importamos la biblioteca Pandas, luego definimos una variable con la ruta del archivo y, finalmente, usamos el método **read_csv** para cargar los datos. Es importante mencionar que **read_csv** asume que los datos contienen un encabezado de columna, pero en nuestro caso, los datos de los vehículos usados no tienen encabezados. Por lo tanto, debemos especificar que no hay encabezados configurando el parámetro `header=None`.

Una vez que leemos el conjunto de datos, es recomendable visualizar el DataFrame para asegurarnos de que todo se haya cargado correctamente. Imprimir todo el conjunto de datos puede consumir muchos recursos y tiempo, por lo que podemos usar el método **head** de Pandas para ver las primeras filas del DataFrame, lo que nos permitirá obtener una visión rápida de los datos. De manera similar, podemos utilizar **tail** para ver las últimas filas. En este caso, mostramos las primeras cinco filas del conjunto de datos, y parece que los datos se han cargado correctamente. Pandas ha asignado automáticamente un encabezado numérico (enteros) a las columnas porque configuramos `header=None` al leer el archivo.

Sin embargo, trabajar con un DataFrame que no tiene nombres significativos en las columnas es complicado. Afortunadamente, podemos asignar nombres a las columnas en Pandas. En este caso, los nombres de las columnas están disponibles en un archivo separado en línea. Primero, colocamos estos nombres en una lista llamada **encabezados** y luego asignamos esta lista al DataFrame mediante el comando `df.columns = encabezados`. Al utilizar el método **head** nuevamente, ahora veremos que los encabezados correctos se han insertado en la parte superior de cada columna.

Finalmente, en algún momento después de manipular el DataFrame, es posible que queramos exportarlo a un nuevo archivo CSV. Esto se puede hacer fácilmente con el método **to_csv**. Para guardar el DataFrame en un archivo, solo necesitamos especificar la ruta y el nombre del archivo en el que queremos escribir los datos. Por ejemplo, si deseamos guardar el DataFrame **df** como "automobile.csv", usamos la sintaxis `df.to_csv`. En este curso, nos centraremos en leer y guardar archivos CSV, pero es importante destacar que Pandas también admite la importación y exportación de muchos otros formatos de archivo, como JSON o Excel. La sintaxis para leer y guardar otros formatos de datos es muy similar a la de los archivos CSV.

En resumen, el proceso de lectura y escritura de datos en Pandas es simple y directo, lo que nos permite empezar a analizar los datos rápidamente una vez que los hemos cargado en el entorno de Python.




# Introducción al análisis de datos en Python


En este vídeo, presentamos algunos de los métodos más sencillos y útiles de **Pandas** que todo científico o analista de datos debe conocer al trabajar con Python y datos. En este punto, asumimos que ya hemos cargado el conjunto de datos y es momento de explorar y comprender su estructura. Pandas ofrece varios métodos integrados que permiten observar el tipo de datos de las características y la distribución de los datos dentro del conjunto. El uso de estos métodos proporciona una visión general del conjunto de datos y puede ayudar a identificar problemas, como características con tipos de datos incorrectos, que pueden necesitar corrección más adelante.

Los datos que manipulamos en Pandas pueden ser de diferentes tipos. Los tipos principales de datos que se almacenan en los objetos de Pandas son **object**, **float**, **int** y **datetime**. Es importante destacar que los nombres de los tipos de datos en Pandas son ligeramente diferentes de los tipos nativos de Python. Algunos de estos tipos son bastante similares, como los tipos de datos numéricos **int** y **float**. El tipo **object** en Pandas funciona de manera similar a las cadenas de texto en Python, con la diferencia de que tiene un nombre distinto. El tipo **datetime** es especialmente útil para manejar datos relacionados con series temporales.

Existen dos razones clave para verificar los tipos de datos en un conjunto de datos. En primer lugar, Pandas asigna automáticamente los tipos de datos en función de la codificación que detecta en la tabla original. Sin embargo, esta asignación no siempre es precisa. Por ejemplo, si una columna que se espera que contenga datos numéricos, como el precio de un automóvil, se asigna al tipo **object**, podría generar problemas. En este caso, sería más apropiado que esa columna tuviera el tipo **float**. Por lo tanto, es importante revisar y ajustar manualmente los tipos de datos cuando sea necesario. La segunda razón para revisar los tipos de datos es que permite a un científico de datos saber qué funciones de Python son aplicables a cada columna. Algunas funciones matemáticas solo funcionan con datos numéricos, por lo que intentar aplicarlas a columnas con datos no numéricos podría generar errores.

Cuando se utiliza el método `dtypes` en Pandas, se obtiene una serie que muestra el tipo de datos de cada columna. Un análisis intuitivo de los tipos de datos en un conjunto de datos nos permite detectar posibles problemas. Por ejemplo, los nombres de los coches, que deberían ser de tipo **object** (cadenas de texto), tienen sentido, pero otras columnas, como el diámetro de un motor, deberían ser numéricas. Si esta columna se ha asignado incorrectamente al tipo **object**, será necesario corregirlo en etapas posteriores.

A continuación, podemos revisar un resumen estadístico de cada columna para comprender mejor la distribución de los datos. Las métricas estadísticas pueden ayudar a los científicos de datos a identificar problemas como valores atípicos extremos o grandes desviaciones, los cuales pueden requerir atención. Para obtener este resumen de forma rápida, utilizamos el método **describe** de Pandas. Este método devuelve varias métricas clave de cada columna, como el número de entradas (recuento), el valor promedio (media), la desviación estándar (STD), los valores mínimo y máximo, así como los límites de los cuartiles. De manera predeterminada, **describe** solo incluye columnas numéricas en el resumen.

Si deseamos que **describe** también incluya columnas de tipo **object**, podemos añadir un argumento llamado `include='all'` dentro de la función. Esto generará un resumen completo que incluye las 26 columnas, tanto numéricas como de tipo objeto. Para las columnas de tipo **object**, el resumen incluirá estadísticas como el número de valores únicos (unique), el valor que aparece con mayor frecuencia (top) y la frecuencia de ese valor (freq). Por ejemplo, si tenemos una columna con el nombre de los modelos de los coches, veremos cuántos valores distintos hay, cuál es el modelo más común y cuántas veces aparece ese modelo.

Algunos valores en el resumen pueden aparecer como **NaN** (Not a Number), lo que indica que no se puede calcular una métrica estadística para esa columna en particular. Esto puede suceder cuando intentamos calcular una estadística inaplicable a un tipo de datos específico, como la media de una columna de tipo objeto.

Finalmente, otro método útil para explorar un conjunto de datos es **dataframe.info()**, que ofrece un resumen conciso del DataFrame. Este método imprime información clave sobre el conjunto de datos, como el índice, los tipos de datos de las columnas, el número de valores no nulos en cada columna y el uso de memoria. Es una herramienta rápida y eficiente para obtener una visión general del estado del conjunto de datos.

En resumen, Pandas proporciona varias herramientas útiles para explorar y comprender el tipo y la distribución de los datos en un conjunto, lo que ayuda a los científicos de datos a identificar y corregir problemas antes de proceder con un análisis más profundo.


# Acceso a bases de datos con Python


En este vídeo, aprenderás cómo acceder a bases de datos utilizando Python. Las bases de datos son herramientas esenciales para los científicos de datos, ya que permiten almacenar, organizar y consultar grandes cantidades de información. Al final de este módulo, serás capaz de explicar los conceptos básicos sobre cómo conectar Python con bases de datos. Mostraremos cómo un usuario típico accede a una base de datos usando código Python escrito en un **Jupyter Notebook**, que es un editor basado en la web.

Para que Python se comunique con un **DBMS** (sistema de gestión de bases de datos), existe un mecanismo mediante el cual el código Python interactúa con la base de datos a través de **llamadas a la API**. A continuación, explicaremos los conceptos básicos de las **APIs SQL** y las **APIs de bases de datos en Python**. Una **API** (Interfaz de Programación de Aplicaciones) es un conjunto de funciones que un programa puede llamar para acceder a un servicio específico.

Una **API SQL** permite que el programa de Python pase sentencias SQL al DBMS. El programa realiza llamadas a funciones de la API para ejecutar consultas y obtener información de estado del DBMS. El flujo básico de una API SQL típica es el siguiente: el programa de Python establece una conexión a la base de datos, luego crea y pasa una sentencia SQL al DBMS, y finalmente gestiona el estado de la solicitud y los posibles errores. Cuando el programa termina su trabajo, realiza una llamada para desconectarse de la base de datos.

La **API DB** es el estándar de Python para acceder a bases de datos relacionales. Esta API permite escribir un programa que sea compatible con diferentes tipos de bases de datos relacionales, lo que significa que puedes conectar tu código con distintas bases de datos sin necesidad de escribir código específico para cada una. Al aprender las funciones de la API de bases de datos, puedes utilizar Python con cualquier base de datos relacional, lo que aumenta la flexibilidad y eficiencia de tus proyectos.

Dentro de la API de bases de datos de Python, existen dos conceptos fundamentales: los **objetos de conexión** y los **objetos de cursor**. El **objeto de conexión** se utiliza para conectar Python con la base de datos y gestionar las transacciones. Por otro lado, los **objetos de cursor** se emplean para ejecutar consultas en la base de datos. El cursor es similar a un cursor en un procesador de texto, donde se desplaza por el conjunto de resultados y permite a la aplicación insertar los datos obtenidos.

Los cursores son esenciales para escanear los resultados de las consultas. Los objetos de conexión ofrecen varios métodos útiles, tales como:

- **cursor()**: Este método devuelve un nuevo objeto de cursor mediante la conexión.
- **commit()**: Se utiliza para confirmar cualquier transacción pendiente en la base de datos.
- **rollback()**: Permite revertir cualquier transacción pendiente, devolviendo la base de datos al estado anterior.
- **close()**: Se utiliza para cerrar la conexión con la base de datos, lo cual es importante para liberar recursos y evitar conexiones no utilizadas.

Ahora, veamos cómo se usa la API de bases de datos en una aplicación práctica de Python. Primero, debemos importar el módulo adecuado de la API de conexión. Luego, para abrir una conexión a la base de datos, utilizamos la función de conexión y proporcionamos los parámetros necesarios, como el nombre de la base de datos, el usuario y la contraseña. Esta función devuelve un objeto de conexión. Una vez que tenemos la conexión, podemos crear un objeto de cursor a través de la conexión y usar este cursor para ejecutar consultas y obtener los resultados.

Una vez ejecutadas las consultas, el cursor también se utiliza para obtener los resultados de estas. Finalmente, cuando se termina de trabajar con la base de datos, es crucial cerrar la conexión para liberar los recursos y evitar problemas de rendimiento.

En resumen, aprender a trabajar con bases de datos en Python te permite acceder y manipular grandes volúmenes de datos de manera eficiente. La API de Python proporciona herramientas poderosas para interactuar con bases de datos relacionales y facilita la realización de consultas y la gestión de transacciones de manera eficaz. Recuerda siempre cerrar tus conexiones para optimizar el uso de recursos y garantizar el buen funcionamiento de tus aplicaciones. ¡Gracias por ver este vídeo!


- Cada línea de un conjunto de datos es una fila, y las comas separan los valores.
    
- Para comprender los datos, debe analizar los atributos de cada columna de datos.
    
- Las bibliotecas de Python son colecciones de funciones y métodos que facilitan diversas funcionalidades sin necesidad de escribir código desde cero y se clasifican en Computación científica, Visualización de datos y Algoritmos de aprendizaje automático.
    
- Muchas bibliotecas de ciencia de datos están interconectadas; por ejemplo, Scikit-learn está construida sobre NumPy, SciPy y Matplotlib.
    
- El formato de los datos y la ruta del archivo son dos factores clave para la lectura de datos con Pandas.
    
- El método **read_CSV** en Pandas puede leer archivos en formato CSV en un Pandas DataFrame.
    
- Pandas tiene tipos de datos únicos como object, float, Int, y datetime.
    
- Utilice el método **dtype** para comprobar el tipo de datos de cada columna; los tipos de datos mal clasificados pueden necesitar una corrección manual.
    
- Conocer los tipos de datos correctos ayuda a aplicar las funciones Python adecuadas a columnas específicas.
    
- El uso de **resumen estadístico** con **describe(** ) proporciona los rangos de recuento, media, desviación estándar, mínimo, máximo y cuartil para columnas numéricas.
    
- También puede utilizar **include='all'** como argumento para obtener resúmenes para columnas de tipo objeto.
    
- El resumen estadístico ayuda a identificar posibles problemas, como valores atípicos, que requieren mayor atención.
    
- Utilizando el **método info(** ) se obtiene un resumen de las 30 filas superiores e inferiores del DataFrame, útil para una rápida inspección visual.
    
- Algunas métricas estadísticas pueden devolver "NaN", lo que indica que faltan valores, y el programa no puede calcular estadísticas para ese tipo de datos específico.
    
- Python puede conectarse a bases de datos mediante código especializado, a menudo escrito en cuadernos Jupyter.
    
- Las interfaces de programación de aplicaciones (API) SQL y las API DB de Python (las más utilizadas) facilitan la interacción entre Python y el SGBD.
    
- **Las** API SQL se conectan al SGBD con una o varias llamadas API, construyen sentencias SQL como una cadena de texto y utilizan llamadas API para enviar sentencias SQL al SGBD y recuperar resultados y estados.
    
- **DB-API**, El estándar de Python para interactuar con bases de datos relacionales, utiliza **objetos** de conexión para establecer y gestionar conexiones a bases de datos y **objetos cursor** para ejecutar consultas y desplazarse por los resultados.
    
- Los métodos de los objetos de conexión incluyen los comandos cursor(), commit(), rollback() y close().
    
- Puede importar el módulo de base de datos, utilizar la **API de conexión** para abrir una conexión y, a continuación, crear un objeto cursor para ejecutar consultas y obtener resultados.
    
- Recuerde cerrar la conexión a la base de datos para liberar recursos.


![[Pasted image 20241227083757.png]]


![[Pasted image 20241227083807.png]]





### Adquisición de Datos

Un conjunto de datos típicamente es un archivo que contiene datos almacenados en uno de varios formatos. Los formatos comunes de archivo que contienen conjuntos de datos incluyen: .csv, .json, .xlsx, etc. El conjunto de datos puede almacenarse en diferentes lugares: en tu máquina local, en un servidor, en un sitio web, en almacenamiento en la nube, etc.

Para analizar datos en un cuaderno de Python, necesitamos cargar el conjunto de datos en el cuaderno. En esta sección, aprenderás cómo cargar un conjunto de datos en nuestro Jupyter Notebook.

En nuestro caso, el conjunto de datos de automóviles es una fuente en línea y está en formato CSV (valores separados por comas). Usaremos este conjunto de datos como ejemplo para practicar la lectura de datos.

**Fuente de datos**: [https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data](https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data)  
**Tipo de datos**: CSV

La biblioteca **Pandas** es una herramienta muy popular y útil que nos permite leer varios conjuntos de datos en un DataFrame. Las plataformas de Jupyter Notebook tienen integrada la biblioteca Pandas, por lo que todo lo que necesitamos hacer es importarla sin necesidad de instalarla.



``` PYTHON
# uncomment the lines below if you need to install specific version of libraries if using this notebook 
# in an environment where these libraries are not installed 
#! mamba install pandas==1.3.3  -y
#! mamba install numpy=1.21.2 -y
```


```PYTHON
# import pandas library
import pandas as pd
import numpy as np

```
### Leer datos

Utilizamos la función pandas.read_csv() para leer archivos CSV. Sin embargo, en esta versión del laboratorio, que opera en JupyterLite, el conjunto de datos debe descargarse a la interfaz mediante el código proporcionado a continuación.

Las siguientes funciones descargarán el conjunto de datos en su navegador:


```python
from pyodide.http import pyfetch

async def download(url, filename):
    response = await pyfetch(url)
    if response.status == 200:
        with open(filename, "wb") as f:
            f.write(await response.bytes())
```

```python

file_path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv'

```

Para obtener el conjunto de datos, utilice la función download() como se define anteriormente:


```c
await download(file_path, "auto.csv")
file_name="auto.csv"
```

Utilice el método Pandas read_csv() para cargar los datos en un marco de datos.

```python
df = pd.read_csv(file_name)
```

Nota: Esta versión del laboratorio funciona en JupyterLite, lo que requiere que el conjunto de datos se descargue en la interfaz. Mientras trabajan en la versión descargada de este cuaderno en sus máquinas locales (Jupyter Anaconda), los alumnos pueden simplemente omitir los pasos anteriores. y simplemente use la URL directamente en la función pandas.read_csv(). Puede descomentar y ejecutar las declaraciones en la celda a continuación.


```Python
#filepath = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv"
#df = pd.read_csv(filepath, header=None)
```


Después de leer el conjunto de datos, podemos usar el método data_frame.head(n) para verificar las n filas superiores del marco de datos, donde n es un número entero. Al contrario de data_frame.head(n), data_frame.tail(n) le mostrará las n filas inferiores del marco de datos.



```python
# show the first 5 rows using dataframe.head() method
print("The first 5 rows of the dataframe") 
df.head(5)
```

¡Entendido! Aquí tienes los primeros 5 registros del conjunto de datos en formato de tabla, con sus respectivas columnas:

### Primeras 5 filas del conjunto de datos


![[Pasted image 20241227094702.png]]

### Explicación de las columnas:

- **symboling**: Índice que indica el nivel de riesgo asegurado (por ejemplo, 3, 0, 1).
- **normalized-losses**: Pérdidas normalizadas (puede contener valores "?" para indicar datos faltantes).
- **make**: Marca del automóvil (por ejemplo, 'alfa-romero', 'audi').
- **fuel-type**: Tipo de combustible ('gas' o 'diesel').
- **aspiration**: Tipo de aspiración del motor ('std' para estándar, 'turbo' si es turboalimentado).
- **num-of-doors**: Número de puertas del automóvil ('two' o 'four').
- **body-style**: Estilo de la carrocería del automóvil (por ejemplo, 'convertible', 'sedan').
- **drive-wheels**: Tipo de tracción ('rwd' para tracción trasera, 'fwd' para tracción delantera).
- **engine-location**: Ubicación del motor ('front' o 'rear').
- **wheel-base**: Distancia entre ejes del automóvil.
- **length**: Longitud total del automóvil.
- **width**: Ancho del automóvil.
- **height**: Altura del automóvil.
- **curb-weight**: Peso en vacío del automóvil.
- **engine-type**: Tipo de motor (por ejemplo, 'mpfi').
- **num-of-cylinders**: Número de cilindros del motor.
- **engine-size**: Tamaño del motor (medido en centímetros cúbicos).
- **fuel-system**: Sistema de combustible.
- **bore**: Diámetro del cilindro del motor.
- **stroke**: Carrera del pistón.
- **compression-ratio**: Relación de compresión del motor.
- **horsepower**: Potencia del motor (medida en caballos de fuerza).
- **peak-rpm**: RPM máxima del motor.
- **city-mpg**: Consumo de combustible en ciudad (millas por galón).
- **highway-mpg**: Consumo de combustible en carretera (millas por galón).
- **price**: Precio del automóvil en dólares.

Esta tabla proporciona una vista detallada de los primeros 5 registros del conjunto de datos, con información sobre cada una de las 26 columnas del DataFrame.



### Pregunta #1: 
Verifique las 10 filas inferiores del marco de datos "df".

```PYTHON
# Write your code below and press Shift+Enter to execute 
print("The first 10 rows of the dataframe") 
df.head(10)
```

![[Pasted image 20241227094612.png]]



### Agregar Encabezados

Echa un vistazo al conjunto de datos. Pandas asigna automáticamente el encabezado con un número entero comenzando desde 0.

Para describir mejor los datos, puedes agregar un encabezado. Esta información está disponible en: [https://archive.ics.uci.edu/ml/datasets/Automobile](https://archive.ics.uci.edu/ml/datasets/Automobile).

Por lo tanto, tendrás que agregar los encabezados manualmente.

Primero, crea una lista llamada "headers" que contenga todos los nombres de las columnas en orden. Luego, utiliza `dataframe.columns = headers` para reemplazar los encabezados con la lista que creaste.



```python
# create headers list
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]
print("headers\n", headers)
```

respuesta:

```python
headers
 ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']
```


Reemplace los encabezados y vuelva a verificar nuestro marco de datos:


```python
df.columns = headers
df.columns
```

respuesta:


```python
Index(['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',
       'num-of-doors', 'body-style', 'drive-wheels', 'engine-location',
       'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',
       'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke',
       'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',
       'highway-mpg', 'price'],
      dtype='object')
```


También puede ver las primeras 10 entradas del marco de datos actualizado y observar que los encabezados están actualizados.

```python
df.head(10)
```

![[Pasted image 20241227095047.png]]



Ahora necesitamos reemplazar el "?" símbolo con NaN para que dropna() pueda eliminar los valores faltantes:


```python
df1=df.replace('?',np.NaN)

```

Puede colocar los valores faltantes en la columna "precio" de la siguiente manera:


```python
df=df1.dropna(subset=["price"], axis=0)
df.head(20)
```

![[Pasted image 20241227095226.png]]


Aquí, eje = 0 significa que el contenido de toda la fila se eliminará siempre que se encuentre que el 'precio' de la entidad es NaN

Ahora, ha leído con éxito el conjunto de datos sin procesar y ha agregado los encabezados correctos al marco de datos.


### Pregunta #2: 

Busque el nombre de las columnas del marco de datos.

```python
# Write your code below and press Shift+Enter to execute 
print(df.columns)
```

Respuesta:

```python
Index(['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration',
       'num-of-doors', 'body-style', 'drive-wheels', 'engine-location',
       'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',
       'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke',
       'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',
       'highway-mpg', 'price'],
      dtype='object')
```


## Save Dataset[](https://cf-courses-data.static.labs.skills.network/jupyterlite/2.5.5/lab/index.html?mode=learn&env_type=jupyterlite&notebook_url=https%3A%2F%2Fcf-courses-data.static.labs.skills.network%2FIBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork%2Flabs%2F200524.016_labs%2FModule_1%2FDA0101EN-Review-Introduction-20231003-1696291200.jupyterlite.ipynb&file_path=DA0101EN%2F200524+016+labs%2FModule+1%2FDA0101EN-Review-Introduction.ipynb#Save-Dataset)

Correspondingly, Pandas enables you to save the data set to CSV. By using the `dataframe.to_csv()` method, you can add the file path and name along with quotation marks in the brackets.

For example, if you save the data frame **df** as **automobile.csv** to your local machine, you may use the syntax below, where `index = False` means the row names will not be written.


```python
df.to_csv("automobile.csv", index=False)
```


You can also read and save other file formats. You can use similar functions like **`pd.read_csv()`** and **`df.to_csv()`** for other data formats. The functions are listed in the following table:


![[Pasted image 20241227095808.png]]




### Información básica del conjunto de datos

Después de leer los datos en el marco de datos de Pandas, es hora de explorar el conjunto de datos.
Hay varias formas de obtener información esencial sobre los datos para ayudarle a comprenderlos mejor.

### Tipos de datos

Los datos tienen una variedad de tipos.
Los principales tipos almacenados en los marcos de datos de Pandas son object, float, int, bool y datetime64. Para conocer mejor cada atributo, siempre debes conocer el tipo de datos de cada columna. En pandas:

```c
df.dtypes
```


## respuesta:

![[Pasted image 20241227095940.png]]


```c
# check the data type of data frame "df" by .dtypes
print(df.dtypes)
```

![[Pasted image 20241227100021.png]]


Como se muestra arriba, puede ver claramente que el tipo de datos de "simbolización" y "peso en vacío" son int64, "pérdidas normalizadas" es objeto y "base entre ejes" es float64, etc.

Estos tipos de datos se pueden cambiar; Aprenderá cómo lograr esto en un módulo posterior.


### Describir

Si deseamos obtener un resumen estadístico de cada columna, como el recuento, el valor medio de la columna, la desviación estándar de la columna, etc., utilice el método de descripción:

```python
dataframe.describe()
```

Este método proporcionará varias estadísticas resumidas, excluyendo los valores NaN (no es un número).

```python
df.describe()
```

![[Pasted image 20241227100231.png]]



Esto muestra el resumen estadístico de todas las columnas de tipo numérico (int, float).
Por ejemplo, el atributo "simbolización" tiene 205 recuentos, el valor medio de esta columna es 0,83, la desviación estándar es 1,25, el valor mínimo es -2, el percentil 25 es 0, el percentil 50 es 1, el percentil 75 es 2 y el valor máximo es 3.

Sin embargo, ¿qué sucede si también desea verificar todas las columnas, incluidas aquellas que son de tipo objeto?


Puede agregar un argumento include = "todos" dentro del paréntesis. Inténtalo de nuevo.

```python
# describe all the columns in "df" 
df.describe(include = "all")
```

![[Pasted image 20241227100354.png]]


Ahora proporciona el resumen estadístico de todas las columnas, incluidos los atributos tipo objeto.
Ahora puede ver cuántos valores únicos hay, cuál es el valor superior y la frecuencia del valor superior en las columnas escritas por objetos.

Algunos valores en la tabla anterior muestran "NaN". Esos números no están disponibles con respecto a un tipo de columna en particular.


### Pregunta #3:
Puedes seleccionar las columnas de un dataframe indicando el nombre de cada columna. Por ejemplo, puede seleccionar las tres columnas de la siguiente manera:

```
dataframe[[' column 1 ',column 2', 'column 3']]
```

Donde "columna" es el nombre de la columna, puede aplicar el método ".describe()" para obtener las estadísticas de esas columnas de la siguiente manera:

```
dataframe[[' column 1 ',column 2', 'column 3'] ].describe()
```

Aplique el método a ".describe()" a las columnas 'longitud' y 'relación de compresión'.


```python
# Seleccionar las columnas 'length' y 'compression-ratio' y aplicar .describe()
df[['length', 'compression-ratio']].describe()
```


![[Pasted image 20241227100627.png]]

Información
También puede utilizar otro método para verificar su conjunto de datos:
marco de datos.info()
```python
dataframe.info()
```
Proporciona un resumen conciso de su marco de datos.

Este método imprime información sobre un marco de datos, incluido el tipo de índice y las columnas, valores no nulos y uso de memoria.


```python
# look at the info of "df"
df.info()
```

![[Pasted image 20241227100759.png]]



## Parámetros

Los parámetros utilizados en el conjunto de datos son:

1. **Fabricante**  
    La empresa que fabricó el portátil.
    
2. **Categoría**  
    La categoría a la que pertenece el portátil: Este parámetro se mapea a valores numéricos de la siguiente manera:
    
    |Categoría|Valor Asignado|
    |---|---|
    |Gaming|1|
    |Netbook|2|
    |Notebook|3|
    |Ultrabook|4|
    |Workstation|5|
    
3. **GPU**  
    El fabricante de la GPU. Este parámetro se mapea a valores numéricos de la siguiente manera:
    
    |GPU|Valor Asignado|
    |---|---|
    |AMD|1|
    |Intel|2|
    |NVidia|3|
    
4. **SO**  
    El tipo de sistema operativo (Windows o Linux): Este parámetro se mapea a valores numéricos de la siguiente manera:
    
    |SO|Valor Asignado|
    |---|---|
    |Windows|1|
    |Linux|2|
    
5. **CPU_core**  
    El tipo de procesador utilizado en el portátil: Este parámetro se mapea a valores numéricos de la siguiente manera:
    
    |CPU_core|Valor Asignado|
    |---|---|
    |Intel Pentium i3|3|
    |Intel Pentium i5|5|
    |Intel Pentium i7|7|
    
6. **Tamaño_Pantalla_cm**  
    El tamaño de la pantalla del portátil se registra en cm.
    
7. **Frecuencia_CPU**  
    La frecuencia a la que opera la CPU, en GHz.
    
8. **RAM_GB**  
    El tamaño de la RAM del sistema en GB.
    
9. **Almacenamiento_GB_SSD**  
    El tamaño del almacenamiento SSD en GB instalado en el portátil.
    
10. **Peso_kg**  
    El peso del portátil está en kgs.
    
11. **Precio**  
    El precio del portátil está en USD.




# Análisis de Datos con Python

## Hoja de trucos: Importación de conjuntos de datos

|**Paquete/Método**|**Descripción**|**Ejemplo de código**|
|---|---|---|
|**Leer conjunto de datos CSV**|Leer el archivo CSV que contiene un conjunto de datos en un marco de datos de pandas|`df = pd.read_csv(<CSV_path>, header = None)` # cargar sin encabezado `df = pd.read_csv(<CSV_path>, header = 0)` # cargar usando la primera fila como encabezado|
|**Imprimir las primeras entradas**|Imprimir las primeras entradas (por defecto 5) del marco de datos de pandas|`df.head(n)` #n=número de entradas; por defecto 5|
|**Imprimir las últimas entradas**|Imprimir las últimas entradas (por defecto 5) del marco de datos de pandas|`df.tail(n)` #n=número de entradas; por defecto 5|
|**Asignar nombres de encabezado**|Asignar nombres de encabezado apropiados al marco de datos|`df.columns = headers`|
|**Reemplazar "?" con NaN**|Reemplazar las entradas "?" con la entrada NaN de la biblioteca Numpy|`df = df.replace("?", np.nan)`|
|**Recuperar tipos de datos**|Recuperar los tipos de datos de las columnas del marco de datos|`df.dtypes`|
|**Recuperar descripción estadística**|Recuperar la descripción estadística del conjunto de datos. El uso predeterminado es solo para tipos de datos numéricos. Usa `include="all"` para crear un resumen de todas las variables|`df.describe()` #uso predeterminado `df.describe(include="all")`|
|**Recuperar resumen del conjunto de datos**|Recuperar el resumen del conjunto de datos que se está utilizando, desde el marco de datos|`df.info()`|
|**Guardar marco de datos en CSV**|Guardar el marco de datos procesado en un archivo CSV con una ruta especificada|`df.to_csv(<output CSV path>)`|

Este cuadro resume los comandos más comunes para trabajar con conjuntos de datos en Python utilizando la biblioteca Pandas.


![[Pasted image 20241227102630.png]]


![[Pasted image 20241227102639.png]]


![[Pasted image 20241227102648.png]]

![[Pasted image 20241227102655.png]]























