
Los datos son información no organizada que se procesa para que sea significativa. En general, los datos se componen de hechos, observaciones, percepciones, números, caracteres, símbolos e imágenes que se pueden interpretar para derivar significados. Una de las formas en que los datos se pueden clasificar es por su estructura. Los datos pueden ser: estructurados, semi-estructurados o no estructurados.

Los datos estructurados tienen una estructura bien definida o se adhieren a un modelo de datos especificado. Estos pueden almacenarse en esquemas bien definidos como bases de datos y, en muchos casos, pueden representarse de forma tabular con filas y columnas. Los datos estructurados son hechos y números objetivos que pueden ser recogidos, exportados, almacenados y organizados en bases de datos típicas. Algunas de las fuentes de datos estructurados incluyen bases de datos SQL y sistemas de procesamiento de transacciones en línea (OLTP) que se centran en las transacciones comerciales, hojas de cálculo como Excel y Google Spreadsheets, formularios en línea, sensores como los sistemas de posicionamiento global (GPS) y las etiquetas de identificación por radiofrecuencia (RFID), y registros de red y servidor web. Normalmente, los datos estructurados se almacenan en bases de datos relacionales o SQL y se pueden analizar fácilmente con métodos y herramientas estándar de análisis de datos.

Los datos semi-estructurados son datos que tienen algunas propiedades organizacionales, pero carecen de un esquema fijo o rígido. No pueden almacenarse en la forma de filas y columnas como en las bases de datos. Sin embargo, contienen etiquetas y elementos, o metadatos, que se utilizan para agrupar los datos y organizarlos en una jerarquía. Algunas de las fuentes de datos semi-estructurados incluyen correos electrónicos, XML y otros lenguajes de marcado, ejecutables binarios, paquetes TCP/IP, archivos comprimidos e integración de datos de diferentes fuentes. XML y JSON permiten a los usuarios definir etiquetas y atributos para almacenar datos en una forma jerárquica, lo que los hace ampliamente utilizados para almacenar e intercambiar datos semi-estructurados.

Los datos no estructurados son datos que no tienen una estructura fácilmente identificable y, por lo tanto, no pueden ser organizados en una base de datos relacional convencional en forma de filas y columnas. No siguen ningún formato, secuencia, semántica o regla en particular. Los datos no estructurados pueden lidiar con la heterogeneidad de las fuentes y tienen una variedad de aplicaciones en inteligencia de negocio y analíticas. Algunas de las fuentes de datos no estructurados incluyen páginas web, feeds de redes sociales, imágenes en diferentes formatos de archivo (como JPEG, GIF y PNG), archivos de vídeo y audio, documentos y archivos PDF, presentaciones de PowerPoint, registros de medios y encuestas. Los datos no estructurados se almacenan generalmente en archivos y documentos (como un documento de Word) para análisis manual o en bases de datos NoSQL, que tienen sus propias herramientas de análisis para examinar este tipo de datos.

En resumen, los datos estructurados son datos bien organizados en formatos que se pueden almacenar en bases de datos y se prestan a métodos y herramientas estándar de análisis de datos. Los datos semi-estructurados son datos que están algo organizados y se basan en metadatos para agrupar y jerarquizar. Los datos no estructurados son datos que no se organizan convencionalmente en forma de filas y columnas en un formato determinado.


# Fuentes de datos



Como mencionamos anteriormente, las fuentes de datos nunca han sido tan dinámicas y diversas como lo son hoy en día. Las organizaciones disponen de múltiples fuentes que varían entre internas y externas. Algunas de las fuentes comunes incluyen bases de datos relacionales, archivos planos y conjuntos de datos XML, API y servicios web, la extracción de datos, los flujos y otras fuentes de datos.

Por lo general, las organizaciones tienen aplicaciones internas que les ayudan a gestionar sus actividades comerciales diarias, las transacciones con los clientes, las actividades de recursos humanos y sus flujos de trabajo. Estos sistemas utilizan bases de datos relacionales, como SQL Server, Oracle, MySQL e IBM DB2, para almacenar los datos de forma estructurada. Los datos almacenados en bases de datos y almacenes de datos se pueden utilizar como fuente de análisis. Por ejemplo, los datos de un sistema de transacciones minoristas se pueden usar para analizar las ventas en diferentes regiones, y los datos de un sistema de gestión de relaciones con los clientes se pueden utilizar para hacer proyecciones de ventas.

Fuera de la organización, existen otros conjuntos de datos disponibles tanto de forma pública como privada. Por ejemplo, las organizaciones gubernamentales publican conjuntos de datos demográficos y económicos de forma continua. Además, hay empresas que venden datos específicos, como los datos de puntos de venta, financieros o meteorológicos, que las empresas pueden usar para definir estrategias, predecir la demanda y tomar decisiones relacionadas con las promociones, distribución o marketing, entre otras. Estos conjuntos de datos suelen estar disponibles como archivos planos, archivos de hojas de cálculo o documentos XML. Los archivos planos almacenan los datos en formato de texto plano, con un registro o fila por línea, y cada valor separado por delimitadores como comas, puntos y comas o tabulaciones.

Los datos de un archivo plano se asignan a una sola tabla, a diferencia de las bases de datos relacionales que contienen varias tablas. Uno de los formatos de archivo plano más comunes es el CSV, donde los valores están separados por comas. Los archivos de hoja de cálculo son un tipo especial de archivos planos que también organizan los datos en formato tabular (filas y columnas). Sin embargo, una hoja de cálculo puede contener varias hojas de cálculo, y cada hoja de trabajo puede asignarse a una tabla diferente. Aunque los datos de las hojas de cálculo están en texto plano, los archivos se pueden almacenar en formatos personalizados e incluir información adicional, como el formato y las fórmulas. Algunos ejemplos comunes de hojas de cálculo son Microsoft Excel, Google Sheets, Apple Numbers y LibreOffice.

Los archivos XML, por otro lado, contienen valores de datos que se identifican o marcan mediante etiquetas. Mientras que los datos de los archivos planos son «planos» o se asignan a una sola tabla, los archivos XML pueden admitir estructuras de datos más complejas, como las jerárquicas. Algunos usos comunes del XML incluyen datos de encuestas en línea, extractos bancarios y otros conjuntos de datos no estructurados.

Muchos proveedores de datos y sitios web proporcionan API, o interfaces de programas de aplicaciones, y servicios web, con los que varios usuarios o aplicaciones pueden interactuar y obtener datos para su procesamiento o análisis. Las API y los servicios web suelen detectar las solicitudes entrantes, que pueden ser solicitudes web de los usuarios o solicitudes de red de las aplicaciones, y devuelven los datos en texto plano, XML, HTML, JSON o archivos multimedia. Algunos ejemplos populares de API incluyen el uso de las API de Twitter y Facebook para obtener datos de tuits y publicaciones, lo que permite realizar tareas como la extracción de opiniones o el análisis de sentimientos, que ayuda a medir la cantidad de aprecio o crítica sobre un tema determinado, como las políticas de un gobierno, un producto o la satisfacción del cliente.

El raspado de páginas web, conocido también como "web scraping", permite extraer datos relevantes de fuentes no estructuradas. Este proceso permite descargar datos específicos de páginas web según parámetros definidos. Los raspadores web pueden extraer texto, información de contacto, imágenes, vídeos, artículos de productos y más. Algunos usos comunes del raspado web incluyen recopilar detalles de productos de minoristas y sitios de comercio electrónico para comparaciones de precios, generar oportunidades de ventas a través de fuentes de datos públicas, y obtener conjuntos de datos de entrenamiento para modelos de aprendizaje automático. Herramientas populares de raspado web incluyen BeautifulSoup, Scrapy, Pandas y Selenium.

Los flujos de datos son otra fuente ampliamente utilizada para agregar flujos constantes de datos que provienen de fuentes como dispositivos IoT, datos GPS de vehículos, programas de computadora, sitios web y publicaciones en redes sociales. Estos datos generalmente tienen una marca de tiempo y también pueden estar etiquetados geográficamente. Algunos ejemplos de flujos de datos incluyen cotizaciones bursátiles para operaciones financieras, flujos de transacciones minoristas para predecir la gestión de la demanda, transmisiones de vigilancia para detectar amenazas, y fuentes de datos de sensores para monitorear maquinaria industrial o agrícola. Herramientas como Apache Kafka, Apache Spark Streaming y Apache Storm se utilizan para procesar estos flujos de datos.

Finalmente, los feeds RSS (Really Simple Syndication) también son una fuente popular para capturar datos actualizados de foros en línea y sitios de noticias donde los datos se actualizan continuamente. A través de un lector de feeds, que convierte los archivos de texto RSS en un flujo de datos actualizado, las actualizaciones se transmiten a los dispositivos de los usuarios.


# Metadatos y Gestión de Metadatos

# Objetivos

Después de completar esta lectura, podrás:

- Definir qué es la metadata
- Describir qué es la gestión de metadata
- Explicar la importancia de la gestión de metadata
- Enumerar herramientas populares para la gestión de metadata

# ¿Qué es la metadata?

La metadata es datos que proporcionan información sobre otros datos.

Esta es una definición muy amplia. Aquí consideraremos el concepto de metadata dentro del contexto de bases de datos, almacenamiento de datos, sistemas de inteligencia empresarial y todo tipo de repositorios y plataformas de datos.

Consideraremos los siguientes tres tipos principales de metadata:

- Metadata técnica
- Metadata de proceso, y
- Metadata empresarial

## Metadatos técnicos

Los metadatos técnicos son metadatos que definen las estructuras de datos en repositorios o plataformas de datos, principalmente desde una perspectiva técnica.

Por ejemplo, los metadatos técnicos en un almacén de datos incluyen activos como:

- Tablas que registran información sobre las tablas almacenadas en una base de datos, como:
    
    - el nombre de cada tabla
    - el número de columnas y filas que tiene cada tabla
- Un catálogo de datos, que es un inventario de tablas que contienen información, como:
    
    - el nombre de cada base de datos en el almacén de datos de la empresa
    - el nombre de cada columna presente en cada base de datos
    - los nombres de cada tabla en la que se encuentra cada columna
    - el tipo de datos que contiene cada columna

Los metadatos técnicos para bases de datos relacionales se almacenan típicamente en tablas especializadas en la base de datos llamadas Catálogo del Sistema.

## Metadatos del proceso

Los metadatos del proceso describen los procesos que operan detrás de sistemas empresariales como almacenes de datos, sistemas contables o herramientas de gestión de relaciones con clientes.

Muchos sistemas empresariales importantes son responsables de recopilar y procesar datos de diversas fuentes. Tales sistemas críticos necesitan ser monitoreados para detectar fallos y cualquier anomalía de rendimiento que surja. Los metadatos del proceso para tales sistemas incluyen el seguimiento de cosas como:

- tiempos de inicio y fin del proceso
- uso del disco
- de dónde se movieron los datos y a dónde, y
- cuántos usuarios acceden al sistema en un momento dado

Este tipo de datos es invaluable para la solución de problemas y la optimización de flujos de trabajo y consultas ad hoc.

## Metadatos empresariales

Los usuarios que desean explorar y analizar datos dentro y fuera de la empresa suelen estar interesados en _el descubrimiento de datos_. Necesitan poder encontrar datos que sean significativos y valiosos para ellos y saber de dónde se puede acceder a esos datos. Por lo tanto, estos usuarios con mentalidad empresarial están interesados en los metadatos empresariales, que son información sobre los datos descrita de maneras fácilmente interpretables, como:

- cómo se adquieren los datos
- qué están midiendo o describiendo los datos
- la conexión entre los datos y otras fuentes de datos

Los metadatos empresariales también sirven como documentación para todo el sistema de almacenamiento de datos.

# Gestión de metadatos

La gestión de metadatos incluye el desarrollo y la administración de políticas y procesos para garantizar que la información pueda ser accedida e integrada desde diversas fuentes y compartida adecuadamente en toda la empresa.

La creación de un catálogo de datos confiable y fácil de usar es un objetivo principal de un modelo de gestión de metadatos. El catálogo de datos es un componente clave de un sistema moderno de gestión de metadatos, sirviendo como el principal activo alrededor del cual se administra la gestión de metadatos. Sirve como la base sobre la cual las empresas pueden inventariar y organizar de manera eficiente sus sistemas de datos. Un modelo moderno de gestión de metadatos incluirá una interfaz de usuario basada en la web que permite a los ingenieros y usuarios de negocios buscar y encontrar fácilmente información sobre atributos clave como CustomerName o ProductType. Este tipo de modelo es central para cualquier iniciativa de Gobernanza de Datos.

# ¿Por qué es importante la gestión de metadatos?

Una buena gestión de metadatos tiene muchos beneficios valiosos. Tener acceso a un catálogo de datos bien implementado mejora enormemente el descubrimiento de datos, la repetibilidad, la gobernanza y también puede facilitar el acceso a los datos.

Los metadatos bien gestionados te ayudan a comprender tanto el contexto empresarial asociado con los datos de la empresa como la línea de datos, lo que ayuda a mejorar la gobernanza de datos. La línea de datos proporciona información sobre el origen de los datos y cómo se transforman y trasladan, y así facilita el rastreo de errores de datos hasta su causa raíz. La gobernanza de datos es un concepto de gestión de datos que se refiere a la capacidad que permite a una organización garantizar que exista alta calidad de datos a lo largo de todo el ciclo de vida de los datos, y se implementan controles de datos que apoyan los objetivos empresariales.

Las áreas clave de enfoque de la gobernanza de datos incluyen disponibilidad, usabilidad, consistencia, integridad de datos y seguridad de datos, e incluye el establecimiento de procesos para asegurar una gestión efectiva de datos en toda la empresa, como la responsabilidad por los efectos adversos de la mala calidad de los datos y asegurando que los datos que tiene una empresa puedan ser utilizados por toda la organización.

# Herramientas populares para la gestión de metadatos

Las herramientas populares para la gestión de metadatos incluyen:

- IBM InfoSphere Information Server
- CA Erwin Data Modeler
- Oracle Warehouse Builder
- SAS Data Integration Server
- Talend Data Fabric
- Alation Data Catalog
- SAP Information Steward
- Microsoft Azure Data Catalog
- IBM Watson Knowledge Catalog
- Oracle Enterprise Metadata Management (OEMM)
- Adaptive Metadata Manager
- Unifi Data Catalog
- data.world
- Informatica Enterprise Data Catalog

# Resumen

En esta lectura, aprendiste que:

- Los metadatos son datos que proporcionan información sobre otros datos, e incluyen tres tipos principales: metadatos técnicos, de proceso y de negocio.
- Los metadatos técnicos para bases de datos relacionales se almacenan típicamente en tablas especializadas en la base de datos llamadas catálogo del sistema.
- Un objetivo principal de la gestión de metadatos de negocio es la creación y mantenimiento de un catálogo de datos confiable y fácil de usar.
- Tener acceso a un catálogo de datos bien implementado mejora enormemente el descubrimiento de datos, la repetibilidad, la gobernanza y también puede facilitar el acceso a los datos.
- Las herramientas de gestión de metadatos de IBM incluyen InfoSphere Information Server y Watson Knowledge Catalogo.

![[Pasted image 20241219085619.png]]


![[Pasted image 20241219085626.png]]


# Resumen de la lección: Comprender los datos


En esta lección, hemos aprendido que los datos son la base fundamental de la ciencia de datos, y es esencial comprenderlos en sus diferentes formas: estructurados, semiestructurados y no estructurados. Los datos estructurados tienen una estructura bien definida, adherida a un modelo de datos específico, y pueden ser almacenados en esquemas organizados, como bases de datos, en tablas con filas y columnas. Los datos semiestructurados, aunque tienen ciertas propiedades organizativas, carecen de un esquema rígido, y se organizan con etiquetas, elementos o metadatos que crean una jerarquía.

Los metadatos proporcionan información valiosa sobre los datos, y pueden clasificarse en tres tipos: técnicos, de proceso y empresariales. La gestión de estos metadatos es clave para garantizar el acceso y la integración adecuada de la información, compartiéndola correctamente dentro de la organización, y un catálogo de datos bien implementado mejora el descubrimiento, la repetibilidad, la gobernanza y el acceso a los datos.

Los datos no estructurados provienen de diversas fuentes y son más difíciles de organizar, a menudo requiriendo inteligencia artificial para su análisis. Estos datos tienen muchas aplicaciones en análisis e inteligencia empresarial. Además, los datos necesarios para análisis generalmente provienen de cualquier cosa con una huella electrónica, y muchas veces es necesario convertir registros analógicos, como los en papel, a formatos electrónicos para su procesamiento efectivo.

Las organizaciones utilizan aplicaciones internas para gestionar sus actividades diarias y flujos de trabajo, y los datos almacenados en bases de datos y almacenes de datos pueden ser fuentes valiosas para análisis. Además, los conjuntos de datos públicos y privados, incluidos los datos patentados, pueden ser adquiridos o consultados para análisis. Estos datos a menudo se presentan en archivos planos, como CSV, hojas de cálculo o XML, y más recientemente en formato JSON, que es fácil de leer y transferir entre estructuras de datos.

El uso de interfaces de programación de aplicaciones (API) es común para acceder a los datos, y muchos proveedores, como Twitter y Facebook, ofrecen API para acceder a sus datos y realizar tareas de análisis, como la extracción de opiniones. Los científicos de datos necesitan ser flexibles, ya que trabajan con grandes volúmenes de datos que provienen de diversas fuentes, como aplicaciones de IoT, sensores y redes sociales, y se actualizan continuamente.

En resumen, los científicos de datos deben comprender profundamente el ecosistema moderno de organización, almacenamiento, manipulación y recuperación de datos antes de poder analizarlos de manera efectiva.


# Resumen de la lección: Comprender los datos



Los datos son la base fundamental de la ciencia de datos, y para ser un científico de datos exitoso es crucial comprenderlos en sus diversas formas: estructurados, semiestructurados y no estructurados. Los datos estructurados tienen una estructura bien definida y se adhieren a un modelo de datos específico. Estos datos pueden almacenarse en esquemas bien organizados, como bases de datos, y comúnmente se representan en tablas con filas y columnas. El esquema define las relaciones entre estas tablas. Por otro lado, los datos semiestructurados poseen algunas propiedades organizativas pero carecen de un esquema fijo o rígido. Estos datos no se almacenan en filas ni columnas, sino que se organizan mediante etiquetas y elementos, o metadatos, que crean una jerarquía.

Los metadatos proporcionan información valiosa sobre los datos. Existen tres tipos principales de metadatos: técnicos, de proceso y empresariales. Los metadatos deben gestionarse correctamente para garantizar el acceso y la integración de la información desde diversas fuentes y compartirla adecuadamente en toda la organización, lo que generalmente se realiza mediante un catálogo de datos. Un catálogo de datos bien implementado mejora el descubrimiento de los datos, la repetibilidad, la gobernanza y el acceso a los mismos.

Los datos no estructurados, por su parte, son heterogéneos y provienen de una amplia variedad de fuentes. Estos datos tienen diversas aplicaciones en el análisis y la inteligencia empresarial, pero a menudo requieren el uso de inteligencia artificial para extraer información útil de ellos. Los datos pueden provenir de cualquier fuente con huella electrónica, ya sea que se generen y almacenen automáticamente o a través de esfuerzos manuales. Aunque algunos registros antiguos se encuentran en formatos analógicos, como en papel, generalmente es necesario convertirlos a formatos electrónicos para procesarlos de manera eficaz.

Las organizaciones suelen utilizar aplicaciones internas para gestionar sus actividades diarias y flujos de trabajo empresariales. Los datos almacenados en bases de datos y almacenes de datos son fuentes clave para el análisis. Además, los científicos de datos pueden buscar conjuntos de datos públicos y privados o incluso adquirir datos patentados. Estos conjuntos suelen estar disponibles como archivos planos, como CSV o hojas de cálculo. Durante años, los archivos planos almacenaron información sobre su estructura en formato XML. Sin embargo, los datos de texto en formato JSON han ganado popularidad recientemente. El formato JSON es sencillo de leer tanto para humanos como para máquinas, y no tiene un esquema predefinido, lo que facilita su transferencia entre estructuras de datos que evolucionan con el tiempo.

En el ecosistema moderno de datos, las aplicaciones de software basadas en la nube permiten acceder a los datos a través de interfaces de programación de aplicaciones (API). Por ejemplo, plataformas como Twitter y Facebook ofrecen API para acceder a sus datos, lo que permite analizar las opiniones de los usuarios sobre diferentes temas, como la satisfacción del cliente. Aunque la recopilación y gestión de datos suele ser responsabilidad de los ingenieros de datos, los científicos de datos deben transferir estos datos para su análisis.

Trabajar con grandes conjuntos de datos, que a menudo alcanzan tamaños de terabytes, exige flexibilidad. Estos datos suelen actualizarse continuamente y provienen de fuentes como aplicaciones de IoT, sensores y redes sociales, recopilando información de millones de usuarios. Los científicos de datos deben comprender a fondo los datos con los que trabajan, y antes de analizarlos, es esencial entender el ecosistema moderno de organización, almacenamiento, manipulación y recuperación de datos.


Glosario de la Lección sobre Cómo Entender los Datos

¡Bienvenido! Este glosario alfabetizado contiene muchos de los términos que se presentan en este curso. Estos términos son importantes para que los reconozcas cuando trabajes en la industria, participes en grupos de usuarios y otros programas de certificación.

|Término|Definición|Video donde se introduce el término|
|---|---|---|
|Valores separados por comas (CSV)|Archivos de texto delimitados donde el delimitador es una coma. Se utilizan para almacenar datos estructurados.|Entendiendo los Diferentes Tipos de Formatos de Archivos|
|Formatos de archivo de texto delimitado|Los archivos de texto se utilizan para almacenar datos donde cada línea o fila tiene valores separados por un delimitador. Un delimitador es una secuencia de uno o más caracteres que especifican el límite entre los valores. Los delimitadores comunes incluyen coma, tabulación, dos puntos, barra vertical y espacio.|Entendiendo los Diferentes Tipos de Formatos de Archivos|
|Bases de datos NoSQL|Bases de datos diseñadas para almacenar y gestionar datos no estructurados y proporcionar herramientas de análisis para examinar este tipo de datos.|Tipos de Datos|
|Sistemas de Procesamiento de Transacciones en Línea (OLTP)|Sistemas que se centran en manejar transacciones comerciales y almacenar datos estructurados.|Tipos de Datos|
|Bases de datos relacionales|Bases de datos diseñadas para almacenar datos estructurados con esquemas bien definidos y admitir métodos y herramientas estándar de análisis de datos.|Tipos de Datos|
|Sensores|Dispositivos como Sistemas de Posicionamiento Global (GPS) y etiquetas de Identificación por Radiofrecuencia (RFID) que generan datos estructurados.|Tipos de Datos|
|Hojas de cálculo|Aplicaciones de software como Excel y Google Sheets utilizadas para organizar y analizar datos estructurados.|Tipos de Datos|
|Bases de datos SQL|Bases de datos que utilizan el Lenguaje de Consulta Estructurado (SQL) para definir, manipular y consultar datos en formatos estructurados.|Tipos de Datos|
|Valores separados por tabulaciones (TSV)|Archivos de texto delimitados donde el delimitador es una tabulación. Se usan como alternativa a los CSV cuando hay comas literales presentes en los datos de texto.|Entendiendo los Diferentes Tipos de Formatos de Archivos|




![[Pasted image 20241219094719.png]]


![[Pasted image 20241219094726.png]]


![[Pasted image 20241219094859.png]]


# Recogida y organización de datos


Un repositorio de datos es un término general que se utiliza para referirse a los datos que se han recopilado, organizado y aislado, con el fin de ser utilizados en operaciones comerciales o extraídos para informes y análisis de datos. Estos repositorios pueden ser infraestructuras pequeñas o grandes, que contienen una o más bases de datos diseñadas para recopilar, administrar y almacenar conjuntos de datos. Existen diferentes tipos de repositorios en los que pueden residir los datos, como bases de datos, almacenes de datos y almacenes de macrodatos, los cuales se examinarán con más detalle en otros contextos.

Una base de datos es una colección de datos o información diseñada para la entrada, almacenamiento, búsqueda, recuperación y modificación de datos. Un sistema de administración de bases de datos (DBMS) es un conjunto de programas que crea y mantiene la base de datos, permitiendo almacenar, modificar y extraer información mediante una función llamada consulta. Por ejemplo, si se desea buscar clientes que han estado inactivos durante seis meses o más, el sistema de administración de bases de datos utilizaría una consulta para recuperar los datos correspondientes. Aunque una base de datos y un DBMS son conceptos distintos, los términos suelen utilizarse de manera intercambiable.

Existen diferentes tipos de bases de datos que se eligen según diversos factores, como el tipo y la estructura de los datos, los mecanismos de consulta, los requisitos de latencia, las velocidades de transacción y el uso previsto de los datos. Es importante mencionar dos tipos principales de bases de datos: las bases de datos relacionales y no relacionales. Las bases de datos relacionales (RDBMS) se basan en los principios organizativos de los archivos planos, con los datos organizados en un formato tabular con filas y columnas, siguiendo una estructura y esquema bien definidos. Sin embargo, a diferencia de los archivos planos, los RDBMS están optimizados para operaciones y consultas de datos que implican muchas tablas y grandes volúmenes de datos. El lenguaje de consulta estructurado (SQL) es el estándar para bases de datos relacionales.

Por otro lado, las bases de datos no relacionales, también conocidas como NoSQL, surgieron en respuesta al volumen, la diversidad y la velocidad a la que se generan los datos en la actualidad. Influenciadas principalmente por los avances en la computación en la nube, el Internet de las cosas y las redes sociales, las bases de datos no relacionales ofrecen velocidad, flexibilidad y escalabilidad, permitiendo almacenar datos de manera libre o sin esquemas definidos. Estas bases de datos se utilizan principalmente para procesar macrodatos.

Un almacén de datos funciona como un repositorio central que combina información procedente de diversas fuentes, consolidándola mediante el proceso de extracción, transformación y carga (ETL) en una base de datos integral para su análisis e inteligencia empresarial. A nivel general, el proceso ETL permite extraer datos de diferentes fuentes, transformarlos en un estado limpio y útil, y cargarlos en el repositorio de datos de la empresa. Los conceptos de Data Marts y Data Lakes están relacionados con los almacenes de datos, y aunque tradicionalmente estos han sido relacionales, el uso de tecnologías NoSQL y nuevas fuentes de datos también ha permitido el uso de repositorios de datos no relacionales para almacenamiento.

Finalmente, los grandes almacenes de datos incluyen infraestructuras computacionales y de almacenamiento distribuidas, diseñadas para almacenar, escalar y procesar conjuntos de datos muy grandes. En general, los repositorios de datos ayudan a aislar los datos, haciendo que los informes y los análisis sean más eficientes y confiables, mientras sirven como archivos de datos.


# Sistema de administración de bases de datos relacionales


Una base de datos relacional es una colección de datos organizados en una estructura de tabla, donde las tablas pueden estar vinculadas o relacionadas según los datos comunes a cada una de ellas. Las tablas están formadas por filas y columnas, siendo las filas los "registros" y las columnas los "atributos". Por ejemplo, en una tabla de clientes que mantiene los datos de cada cliente en una empresa, las columnas pueden ser la identificación de la empresa, el nombre, la dirección y el teléfono principal de la empresa; mientras que cada fila representa un registro de cliente.

Las tablas pueden estar relacionadas entre sí a través de datos comunes. Por ejemplo, junto con la tabla de clientes, la empresa puede tener una tabla de transacciones que contiene datos sobre diversas transacciones de cada cliente. Las columnas de la tabla de transacciones pueden incluir la fecha de la transacción, el ID del cliente, el importe de la transacción y el método de pago. Ambas tablas pueden estar relacionadas a través del campo ID del cliente. Esto permite generar informes, como un extracto de clientes que consolide todas las transacciones de un período específico. Esta capacidad de vincular tablas mediante datos comunes permite extraer información nueva a partir de múltiples tablas con una sola consulta, lo que facilita obtener nuevos conocimientos para tomar decisiones más informadas.

Las bases de datos relacionales utilizan el lenguaje de consulta estructurado, o SQL, para realizar consultas a los datos. Aprenderemos más sobre SQL en otras partes del curso. Las bases de datos relacionales se basan en los principios organizacionales de los archivos planos, como las hojas de cálculo, con datos organizados en filas y columnas según una estructura y un esquema bien definidos. Sin embargo, las bases de datos relacionales son ideales para el almacenamiento, recuperación y procesamiento optimizado de grandes volúmenes de datos, a diferencia de las hojas de cálculo, que tienen un número limitado de filas y columnas.

Cada tabla en una base de datos relacional tiene un conjunto único de filas y columnas, y las relaciones pueden definirse entre tablas para minimizar la redundancia de datos. Además, es posible restringir los campos de las bases de datos a tipos y valores específicos, lo que mejora la consistencia y la integridad de los datos. Las bases de datos relacionales son capaces de procesar millones de registros y recuperar grandes cantidades de datos rápidamente, gracias a su estructura optimizada. Además, la arquitectura de seguridad de estas bases de datos proporciona un acceso controlado a los datos, asegurando el cumplimiento de normas y políticas.

Las bases de datos relacionales van desde sistemas pequeños de escritorio hasta grandes sistemas basados en la nube. Estas pueden ser de código abierto y apoyadas internamente, de código abierto con soporte comercial, o sistemas comerciales de código cerrado. Ejemplos populares de bases de datos relacionales incluyen IBM DB2, Microsoft SQL Server, MySQL, Oracle Database y PostgreSQL. Las bases de datos relacionales basadas en la nube, también conocidas como Database-as-a-Service, están ganando popularidad debido a las capacidades ilimitadas de computación y almacenamiento que ofrece la nube. Algunas de las bases de datos relacionales más populares en la nube son Amazon RDS, Google Cloud SQL, IBM DB2 on Cloud, Oracle Cloud y SQL Azure.

El uso de bases de datos relacionales tiene varias ventajas. Una de las más importantes es la capacidad de crear informes significativos al unir varias tablas. Otras ventajas incluyen flexibilidad, ya que se pueden añadir nuevas columnas y tablas, y cambiar relaciones mientras la base de datos está en funcionamiento. También reduce la redundancia de los datos, y facilita las copias de seguridad y la recuperación ante desastres, dado que las exportaciones se pueden realizar mientras la base de datos está activa. Las bases de datos relacionales basadas en la nube también realizan duplicaciones continuas, lo que reduce al mínimo la pérdida de datos en caso de restauración. Además, cumplen con los principios ACID (Atomicidad, Consistencia, Aislamiento y Durabilidad), lo que asegura la precisión y consistencia de los datos.

Algunos de los casos de uso de las bases de datos relacionales incluyen el procesamiento de transacciones en línea (OLTP), donde las bases de datos se utilizan para manejar transacciones a alta velocidad; el análisis de datos históricos en almacenes de datos, que se optimizan para análisis en línea (OLAP); y las soluciones de IoT, que requieren bases de datos ligeras para procesar los datos de dispositivos de borde. Sin embargo, las bases de datos relacionales tienen limitaciones. No funcionan bien con datos semi-estructurados y no estructurados, lo que las hace menos adecuadas para análisis exhaustivos de dichos datos. Además, la migración entre dos RDBMS requiere que los esquemas y tipos de datos sean idénticos en las tablas de origen y destino. Las bases de datos relacionales también tienen un límite en la longitud de los campos de datos, lo que impide almacenar más información de la que un campo puede acomodar.

A pesar de las limitaciones, las bases de datos relacionales siguen siendo la tecnología predominante para trabajar con datos estructurados, especialmente en la era de Big Data, la computación en la nube, los dispositivos IoT y las redes sociales.

# NoSQL


NoSQL, que significa «no solo SQL» o «no SQL», es un tipo de base de datos no relacional que ofrece esquemas flexibles para el almacenamiento y la recuperación de datos. Aunque las bases de datos NoSQL existen desde hace mucho tiempo, su popularidad ha crecido recientemente debido a la expansión de la nube, los macrodatos y las aplicaciones web y móviles de gran volumen. Estas bases de datos se destacan por su capacidad de escalar, su alto rendimiento y su facilidad de uso. Es importante señalar que el "No" en "NoSQL" significa "no solo" y no se refiere a la negación de SQL. Las bases de datos NoSQL están diseñadas para modelos de datos específicos y ofrecen esquemas flexibles que permiten a los desarrolladores crear y administrar aplicaciones modernas.

A diferencia de las bases de datos tradicionales que utilizan filas, columnas y tablas con esquemas fijos, las bases de datos NoSQL no siguen este diseño y generalmente no utilizan SQL para consultar datos, aunque algunas pueden ofrecer interfaces similares a SQL. NoSQL permite almacenar datos sin esquemas predefinidos o en formatos libres, lo que facilita el manejo de datos estructurados, semi-estructurados y no estructurados. Según cómo se almacenen los datos, existen cuatro tipos comunes de bases de datos NoSQL: basadas en clave-valor, basadas en documentos, basadas en columnas y basadas en gráficos.

1. **Almacenamiento clave-valor**: En este tipo de base de datos, los datos se almacenan como pares clave-valor, donde la clave es un identificador único y el valor puede ser cualquier tipo de dato, desde simples números o cadenas hasta documentos JSON complejos. Este modelo es ideal para almacenar sesiones de usuarios, preferencias, recomendaciones en tiempo real y datos en memoria caché. Sin embargo, no es adecuado si se necesitan relaciones entre los datos o consultas complejas. Ejemplos populares son Redis, Memcached y DynamoDB.
    
2. **Basado en documentos**: En las bases de datos basadas en documentos, cada registro y sus datos asociados se almacenan en un único documento. Estos modelos permiten realizar consultas flexibles e indexación potente, lo que los hace útiles para plataformas de comercio electrónico, almacenamiento de registros médicos, CRM y análisis de documentos. Sin embargo, no son la mejor opción para realizar consultas complejas o transacciones multioperativas. Ejemplos de bases de datos basadas en documentos incluyen MongoDB, DocumentDB, CouchDB y Cloudant.
    
3. **Basado en columnas**: Este modelo organiza los datos en columnas agrupadas, conocidas como familias de columnas, en lugar de filas. Los datos se almacenan en celdas agrupadas por columnas, lo que facilita el acceso rápido y eficiente a datos de series temporales, datos meteorológicos e IoT. No obstante, no es ideal si se necesitan consultas complejas o patrones de consulta frecuentes. Ejemplos de bases de datos basadas en columnas son Cassandra y HBase.
    
4. **Basado en gráficos**: Las bases de datos gráficas utilizan un modelo de nodos y relaciones para representar y almacenar los datos. Este tipo de base de datos es excelente para trabajar con datos altamente interconectados, como las redes sociales, las recomendaciones de productos en tiempo real y la detección de fraudes. No son ideales para el procesamiento de grandes volúmenes de transacciones, ya que no están optimizadas para análisis de gran volumen. Ejemplos populares incluyen Neo4J y CosmosDB.
    

**Ventajas de NoSQL**: NoSQL se desarrolló como respuesta a las limitaciones de las bases de datos relacionales. Una de sus principales ventajas es su capacidad para manejar grandes volúmenes de datos estructurados, semi-estructurados y no estructurados. Otras ventajas incluyen la capacidad de operar como sistemas distribuidos escalados en varios centros de datos, lo que permite aprovechar la infraestructura de computación en la nube. Además, tiene una arquitectura escalable y rentable que mejora el rendimiento a medida que se agregan nodos.

En cuanto a las diferencias entre bases de datos relacionales (RDBMS) y NoSQL, las bases de datos relacionales requieren esquemas rígidos que definen cómo deben escribirse y organizarse los datos, mientras que NoSQL permite almacenar y manipular datos no estructurados y semi-estructurados. Además, mantener bases de datos relacionales comerciales de alta gama puede ser costoso, mientras que las bases de datos NoSQL están diseñadas para hardware más asequible. Las bases de datos relacionales son más maduras y bien documentadas, lo que hace que los riesgos sean menores, mientras que NoSQL es una tecnología relativamente nueva, pero se está utilizando cada vez más para aplicaciones de misión crítica.

En resumen, aunque las bases de datos NoSQL presentan algunas limitaciones y diferencias frente a las bases de datos relacionales, su flexibilidad, escalabilidad y rendimiento las han convertido en una opción importante para las aplicaciones modernas que requieren manejar grandes volúmenes de datos.


# Data Marts, Data Lakes, ETL y Data Pipelines



En el curso anterior, se discutieron las bases de datos, los almacenes de datos y el almacenamiento de big data. Ahora, profundizaremos más en los almacenes de datos, mercados de datos y lagos de datos, así como en el proceso ETL y las tuberías de datos. Un **almacén de datos** es un repositorio centralizado diseñado para almacenar datos ya modelados y estructurados para fines específicos, lo que facilita el análisis. Las organizaciones eligen un almacén de datos cuando tienen grandes cantidades de datos de sus sistemas operativos que necesitan estar fácilmente disponibles para informes y análisis.

Los almacenes de datos sirven como la "única fuente de verdad", almacenando tanto los datos actuales como históricos que han sido limpiados, conformados y categorizados. Un **data mart** (mercado de datos) es una subsección de un almacén de datos, construida para un propósito o función de negocio específica. Se utiliza para proporcionar a los grupos de interés los datos más relevantes para ellos, cuando lo necesiten, como los equipos de ventas o finanzas que requieren datos para sus informes y proyecciones. Los data marts también ofrecen seguridad y rendimiento aislado.

En contraste, un **lago de datos** es un repositorio que puede almacenar grandes cantidades de datos estructurados, semi-estructurados y no estructurados en su formato nativo. Los datos en un lago de datos están clasificados y etiquetados con metadatos, pero a diferencia de un almacén de datos, un lago de datos conserva todos los datos de origen sin exclusiones. Un lago de datos es ideal cuando se manejan grandes volúmenes de datos en una forma continua y no se quiere estar restringido a casos de uso específicos. Además, se utiliza a veces como área de preparación para un almacén de datos y es fundamental en analíticas predictivas y avanzadas.

Ahora bien, el proceso clave para convertir los datos en bruto en datos listos para el análisis es el **ETL** (Extracción, Transformación y Carga). Este proceso automatizado recopila datos de las fuentes identificadas, los extrae, los limpia, normaliza y transforma en un formato utilizable y luego los carga en un repositorio de datos. Aunque el proceso ETL es genérico, su implementación puede variar en función de su uso y complejidad.

1. **Extracción**: Es el paso donde los datos se recogen desde sus ubicaciones de origen para su transformación. La extracción puede ser mediante:
    
    - **Procesamiento por lotes**: Los datos se extraen en grandes bloques desde el origen hacia el sistema de destino a intervalos programados. Herramientas para esto incluyen Stitch y Blendo.
    - **Procesamiento de flujos**: Los datos se extraen en tiempo real desde la fuente y se transforman mientras están en tránsito. Herramientas para el procesamiento de flujos incluyen Apache Samza, Apache Storm y Apache Kafka.
2. **Transformación**: En este paso, los datos se ajustan mediante reglas y funciones para que sean consistentes y utilizables para el análisis. Esto puede incluir la conversión de formatos de fecha, la eliminación de datos duplicados, la aplicación de reglas de negocio y validaciones de datos, entre otros.
    
3. **Carga**: Es el paso donde los datos procesados se transportan al repositorio de destino. Puede ser:
    
    - **Carga inicial**: Llenar todos los datos en el repositorio.
    - **Carga incremental**: Aplicación de actualizaciones periódicas según sea necesario.
    - **Actualización completa**: Borrar y recargar datos actualizados.

Una parte importante de este paso es la **verificación de la carga**, que incluye la comprobación de valores ausentes, el rendimiento del servidor y la supervisión de los fallos de carga.

El ETL ha sido tradicionalmente utilizado para cargas de trabajo por lotes, pero con la aparición de herramientas de transmisión de datos, también se utiliza para procesar eventos en tiempo real. Es común ver los términos ETL y **pipelines de datos** utilizados de manera intercambiable. Aunque ambos términos se refieren al traslado de datos de una fuente a un destino, un **pipeline de datos** es un concepto más amplio que abarca todo el proceso de traslado de datos, de los cuales el ETL es solo una parte.

Los **pipelines de datos** pueden ser diseñados para procesar datos por lotes, en tiempo real o una combinación de ambos. En el caso de los datos en flujo, el procesamiento y transformación se realiza de manera continua, lo que es útil para datos que necesitan actualizaciones constantes, como los de monitoreo de sensores. Un pipeline de datos es un sistema de alto rendimiento que puede manejar tanto consultas largas como interactivas. El destino de un pipeline de datos suele ser un lago de datos, pero también puede ser otra aplicación o una herramienta de visualización. Algunas de las herramientas populares para crear pipelines de datos incluyen **Apache Beam** y **DataFlow**.


![[Pasted image 20241219103245.png]]



![[Pasted image 20241219103254.png]]


![[Pasted image 20241219103302.png]]



![[Pasted image 20241219104036.png]]


![[Pasted image 20241219104043.png]]






























