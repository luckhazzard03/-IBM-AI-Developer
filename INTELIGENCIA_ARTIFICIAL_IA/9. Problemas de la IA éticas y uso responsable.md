#InteligenciaIA #ProblemasUsoresponsableIA


En 2014, Amazon desarrolló una herramienta de inteligencia artificial para automatizar la preselección de solicitantes de empleo. Sin embargo, el sistema mostró un sesgo de género al penalizar los términos asociados a mujeres debido al predominio de candidatos masculinos en los datos de capacitación. A pesar de los esfuerzos por corregir este sesgo, Amazon suspendió la herramienta en 2015 para mantener su compromiso con la diversidad y la igualdad. Este caso ilustra la importancia de considerar los aspectos éticos en el desarrollo de la IA, garantizando que estas tecnologías beneficien a la sociedad y minimicen sus posibles daños.

La inteligencia artificial ha transformado diversas industrias al mejorar la eficiencia, la innovación y los procesos de toma de decisiones. Desde la atención médica y la educación hasta las finanzas y el entretenimiento, los sistemas de IA pueden analizar grandes cantidades de datos, identificar patrones y hacer predicciones que tienen el potencial de salvar vidas, mejorar los servicios e impulsar el crecimiento económico. Sin embargo, un gran poder conlleva una gran responsabilidad, por lo que es fundamental comprender los aspectos clave de la IA ética y su uso responsable.

Una de las principales preocupaciones éticas de la IA es la privacidad y la seguridad de los datos. Estos sistemas dependen en gran medida de los datos para funcionar de manera eficaz, y estos datos suelen incluir información personal confidencial. Por ello, es esencial implementar medidas robustas de protección para evitar el acceso no autorizado y las infracciones de seguridad. Las organizaciones deben cumplir con normativas de protección de datos, como el Reglamento General de Protección de Datos (GDPR) y la Ley de Privacidad del Consumidor de California (CCPA), asegurando que los datos sean obtenidos de manera ética y anonimizada cuando sea posible.

En 2020, el New York Times reveló que la empresa de reconocimiento facial Clearview AI había recopilado miles de millones de imágenes de redes sociales y otros sitios web sin el consentimiento de los usuarios. Esta compañía proporcionó software de reconocimiento facial a organismos de seguridad, lo que generó un intenso debate sobre las violaciones de la privacidad y el uso ético de la IA, subrayando la necesidad de reforzar la protección de los datos personales.

Otro desafío ético importante en la IA es el sesgo en los datos. Los sistemas de IA pueden amplificar y perpetuar los sesgos existentes en sus datos de entrenamiento, lo que puede resultar en discriminación en áreas como la contratación, los préstamos y la aplicación de la ley. Para mitigar esto, es crucial que los desarrolladores utilicen conjuntos de datos diversos, implementen algoritmos que promuevan la equidad y realicen auditorías continuas para identificar y corregir los sesgos.

La transparencia y la rendición de cuentas también son aspectos clave de la ética en la IA. Es fundamental que los usuarios comprendan cómo los sistemas de IA toman decisiones, qué datos utilizan y cómo se maneja su información personal. Además, debe existir un marco claro de responsabilidad para determinar quién es responsable si un sistema de IA causa daños o toma decisiones incorrectas, lo cual es esencial para garantizar un uso ético de estas tecnologías.

A medida que los sistemas de IA se vuelven más autónomos, la supervisión humana sigue siendo esencial. En áreas críticas como la atención médica, el transporte y el ámbito militar, los sistemas autónomos deben diseñarse de manera que incluyan mecanismos para la intervención humana. Esto asegura que las decisiones del sistema estén alineadas con los valores éticos y que se puedan prevenir resultados peligrosos o inmorales.

Además, el acceso equitativo a la IA y sus beneficios es una prioridad. Si solo unos pocos tienen acceso a esta tecnología, puede agravar las desigualdades existentes. Es importante democratizar la IA, asegurando que sea accesible y beneficiosa para todos los segmentos de la sociedad, incluidas las comunidades marginadas. Esto requiere inversiones en educación y en infraestructura para reducir la brecha digital.

Cada aplicación de la IA plantea desafíos éticos únicos. En la atención médica, por ejemplo, la IA debe priorizar la confidencialidad y el bienestar de los pacientes. En la aplicación de la ley, el uso de IA para la vigilancia y el análisis predictivo genera preocupaciones sobre la privacidad y las libertades civiles. Cada ámbito de aplicación debe tener pautas éticas específicas para abordar sus particularidades y garantizar un uso responsable.

Otro aspecto fundamental es el impacto ambiental de la IA. El entrenamiento de grandes modelos de IA consume una gran cantidad de energía, lo que contribuye a las emisiones de carbono. Por lo tanto, los desarrolladores y las organizaciones deben crear algoritmos eficientes desde el punto de vista energético y adoptar prácticas sostenibles, como el uso de fuentes de energía renovable en los centros de datos.

La IA ética no es un objetivo estático, sino un proceso continuo. Es esencial monitorear y evaluar regularmente el impacto de los sistemas de IA, estableciendo mecanismos para recibir retroalimentación de los usuarios y las partes interesadas. Los sistemas deben ser mejorados continuamente en función de nuevos avances tecnológicos, conocimientos éticos y necesidades sociales.

En resumen, las consideraciones éticas en la IA abarcan áreas como la privacidad y la seguridad de los datos, los sesgos y la equidad, la transparencia y la rendición de cuentas, la supervisión humana en sistemas autónomos, el acceso equitativo a la tecnología, y la eficiencia energética. Para garantizar un uso responsable de la IA, es fundamental contar con políticas, prácticas y enfoques que protejan tanto los derechos individuales como el bienestar colectivo.


# Consideraciones en torno a la IA generativa

La IA generativa está transformando las industrias, ampliando los límites de la creatividad y la innovación. Sin embargo, como ocurre con cualquier herramienta poderosa, viene con su propio conjunto de desafíos y consideraciones. En 2018, una obra de arte titulada _Edmond de Bellamy_, creada por un algoritmo de inteligencia artificial llamado GAN (Generative Adversarial Network), se vendió en una subasta por más de 400,000 dólares. Esto planteó dudas sobre quién poseía los derechos de la obra: ¿los desarrolladores que programaron la IA o los compradores de la obra? Las ventas generaron debates sobre los derechos de propiedad intelectual en las creaciones generadas por IA. A medida que los sistemas de IA se vuelvan más creativos, esta pregunta será cada vez más pertinente, por lo que es necesario establecer directrices y políticas claras que aborden estos problemas, equilibrando los intereses de los desarrolladores, los usuarios y la sociedad en general. Desarrollar regulaciones apropiadas para regular el uso de la IA, al mismo tiempo que se fomente la innovación, es un delicado equilibrio que requiere cuidadosa consideración.

Aclarar los derechos de propiedad intelectual relacionados con el contenido generado por IA es un tema complejo y en constante evolución. Es importante que los sistemas de IA cumplan con las leyes y regulaciones existentes, como las relacionadas con la protección de datos y los derechos de los consumidores. Otra preocupación fundamental en la IA generativa es la privacidad y la confidencialidad. Los modelos de IA generativa requieren grandes cantidades de datos para su entrenamiento, y en ocasiones, estos datos pueden incluir información confidencial o personal. Por ello, la implementación de medidas sólidas de protección de datos es esencial para garantizar que la información utilizada no viole la privacidad individual. Para abordar estos problemas, las organizaciones recurren cada vez más a la IA privada. La IA privada se refiere a un entorno creado específicamente para una organización, diseñado para mantener la confidencialidad y la seguridad de los datos, evitando el acceso no autorizado y el uso indebido.

A pesar de su poder, la IA generativa aún presenta defectos. A veces, puede producir resultados inexactos o completamente inventados, fenómeno conocido como _alucinación_. Por ejemplo, un abogado usó ChatGPT para elaborar una moción legal que contenía opiniones judiciales falsas y citaciones legales inventadas. El abogado, que desconocía la capacidad de ChatGPT para crear casos ficticios, enfrentó sanciones por presentar dicho documento. Este fenómeno resalta la importancia del pensamiento crítico cuando se utiliza contenido generado por IA. Es crucial validar y verificar siempre la información generada por la IA para evitar la difusión de información errónea. Los desarrolladores y usuarios deben implementar estrategias para minimizar el riesgo de alucinaciones, como mejorar la calidad de los datos de entrenamiento, refinar los algoritmos y establecer procesos de validación sólidos.

Las organizaciones pueden usar modelos privados para abordar los problemas de alucinaciones. Un ejemplo de ello es CustomGPT.ai, una plataforma que desarrolla chatbots GPT personalizados basados en el contenido específico de una empresa, ofreciendo respuestas precisas y confiables sin generar información falsa. Al aprovechar los modelos privados, las empresas pueden reducir las alucinaciones y garantizar que los datos de entrenamiento sean relevantes y precisos, lo que conduce a respuestas más confiables y específicas para cada contexto. Para entrenar modelos eficaces, se requieren conjuntos de datos diversos y de alta calidad, ya que la cantidad y calidad de los datos desempeñan un papel crucial en el rendimiento del modelo. Los modelos más complejos, aunque más potentes, también requieren más recursos computacionales y pueden ser más difíciles de interpretar. Además, la capacidad de escalar el uso de la IA generativa en diferentes aplicaciones y entornos es clave para una adopción generalizada, por lo que los modelos de IA deben ser robustos, capaces de gestionar diversas entradas y adaptarse a escenarios inesperados.

Finalmente, las implicaciones éticas de la IA generativa son vastas. Desde los sesgos persistentes hasta el posible uso indebido de la tecnología, el panorama ético de la IA es complejo. Los sistemas de IA pueden replicar involuntariamente los sesgos de sus datos de entrenamiento, lo que lleva a resultados injustos. Es fundamental que los desarrolladores prioricen la equidad, la responsabilidad y la transparencia, trabajando activamente para identificar y corregir los sesgos y asegurando que los sistemas de IA se usen de manera responsable. La IA generativa también puede ser utilizada indebidamente para crear _deepfakes_, que son videos, imágenes o grabaciones manipuladas para parecer reales. Los deepfakes pueden ser muy convincentes y usarse para fines dañinos, como crear noticias falsas, difundir desinformación, acosar a personas o cometer fraude. Pueden incluso generar documentos de identidad falsificados, lo que podría facilitar fraudes financieros y otros delitos de seguridad.

La ética en la IA no solo se trata de prevenir daños, sino también de fomentar resultados positivos y garantizar que las tecnologías beneficien a la sociedad en su conjunto. Esto incluye usar la IA para abordar desafíos sociales, mejorar el acceso a los recursos y elevar la calidad de vida. Es crucial que los desarrolladores y las organizaciones creen sistemas de IA que apoyen iniciativas de justicia social, sostenibilidad ambiental y bienestar humano.

En resumen, aprendiste sobre los desafíos de derechos de autor y propiedad asociados al contenido creado por IA generativa, y lo importante que es que desarrolladores y usuarios se familiaricen con los marcos legales y se mantengan al día con la evolución de las leyes. También aprendiste a proteger la privacidad y confidencialidad de los datos de los usuarios mediante modelos de IA privados, así como a aplicar el pensamiento crítico para garantizar la precisión de los resultados generados por la IA. Además, conociste las consideraciones éticas que instan a los desarrolladores a priorizar la equidad, la responsabilidad y los beneficios sociales en el desarrollo de la IA.

# Por qué alucinan los Modelos de lenguaje grandes


La IA generativa, como los grandes modelos lingüísticos (LLM) utilizados en herramientas como ChatGPT y Bing Chat, puede ser increíblemente poderosa para generar texto fluido y coherente sobre una variedad de temas. Sin embargo, también tiene la tendencia de generar _alucinaciones_, que son errores o inventos que suenan plausibles pero son inexactos o totalmente falsos. Un ejemplo de esto son tres hechos aparentemente reales: la distancia de la Tierra a la Luna es de 54 millones de kilómetros (cuando en realidad esa es la distancia a Marte), el hecho de que trabajaste en una aerolínea australiana (cuando fue tu hermano quien lo hizo), y la afirmación de que el telescopio James Webb tomó las primeras fotos de un exoplaneta fuera de nuestro sistema solar (cuando en realidad se capturaron en 2004). Estos son ejemplos de cómo los LLM pueden inventarse hechos o mezclar información de manera incorrecta.

Las alucinaciones ocurren cuando los LLM se desvían de los hechos o de la lógica contextual, y pueden variar desde pequeñas inconsistencias hasta declaraciones completamente inventadas o contradictorias. Existen diferentes tipos de alucinaciones, como contradicciones simples, en las que una oración contradice directamente a otra, o contradicciones fácticas, donde se presentan hechos incorrectos (como afirmar que Barack Obama fue el primer presidente de los EE. UU.). También hay alucinaciones absurdas, como afirmar que París es tanto la capital de Francia como el nombre de un cantante famoso. Estas alucinaciones pueden ser confusas porque, aunque suenan plausibles, carecen de base real.

Una de las causas principales de las alucinaciones en los LLM es la calidad de los datos con los que fueron entrenados. Los LLM se entrenan con grandes cantidades de texto, que pueden contener errores, sesgos o inconsistencias. Por ejemplo, algunos modelos se entrenan extrayendo contenido de Wikipedia y Reddit, pero no toda la información en estas plataformas es precisa. A pesar de que los datos de entrenamiento puedan ser confiables en su mayoría, estos modelos pueden generalizar de manera incorrecta sobre temas o dominios en los que no tienen suficiente información, lo que lleva a errores. A medida que los modelos mejoran en su razonamiento, las alucinaciones tienden a disminuir, pero aún pueden ocurrir debido a la complejidad del procesamiento de la información.

Otro factor que contribuye a las alucinaciones es el método de generación utilizado por los LLM. Dependiendo de cómo generen el texto (por ejemplo, mediante la búsqueda de haces o el aprendizaje por refuerzo), los modelos pueden priorizar la fluidez o la creatividad en lugar de la precisión. Esto puede resultar en respuestas que son gramaticalmente correctas pero factualmente incorrectas. Además, el contexto de entrada también juega un papel crucial. Si la solicitud al modelo no está clara o es contradictoria, el modelo puede generar una respuesta equivocada o confusa. Por ejemplo, si se le pregunta a un LLM si los gatos hablan inglés, la respuesta correcta sería no, a menos que se añada contexto sobre un gato en una tira cómica como Garfield, en cuyo caso la respuesta sería afirmativa.

Para reducir las alucinaciones al interactuar con los LLM, es fundamental proporcionar indicaciones claras y específicas. Cuanto más detallada y precisa sea la solicitud, más probable será que el modelo genere una respuesta relevante y precisa. También existen estrategias como la mitigación activa, que implica ajustar los parámetros del modelo durante la generación, como la temperatura, para controlar la aleatoriedad de las respuestas. Una temperatura baja produce respuestas más conservadoras y precisas, mientras que una temperatura alta fomenta respuestas más creativas pero con mayor riesgo de alucinaciones. Además, se pueden usar solicitudes de disparos múltiples, donde el modelo recibe varios ejemplos de lo que se espera en la respuesta. Esto ayuda al modelo a entender mejor el contexto y las expectativas del usuario, lo que es particularmente útil en tareas que requieren un formato específico, como la generación de código o la escritura creativa.

En conclusión, aunque los LLM son herramientas poderosas, pueden generar alucinaciones que nos desvían de los hechos correctos. Comprender las causas de las alucinaciones y utilizar estrategias como indicaciones claras y ajustes de parámetros puede ayudarnos a reducir estos errores y aprovechar al máximo el potencial de la IA generativa.




# Perspectiva de los actores clave en torno a la ética de la IA


Bienvenido a la perspectiva de los actores clave en torno a la ética de la IA. Después de ver este vídeo, podrás analizar los enfoques y principios adoptados por varias organizaciones para garantizar la adopción ética de la IA. También podrás explicar los pilares de confianza de IBM, el enfoque de Microsoft con respecto a la IA ética y los principios de IA de Google. Garantizar la ética de la IA es un tema complejo que las organizaciones abordan con diferentes enfoques. Organizaciones como IBM, Microsoft y Google han creado enfoques y herramientas para abordar las preocupaciones éticas en el desarrollo y el uso de la IA. Veamos cómo estas organizaciones se aseguran de que la IA sea ética a través de sus métodos e iniciativas únicos.

En su compromiso con la IA ética, IBM ha logrado avances significativos en la promoción de la ética de la IA a través de sus pilares de confianza. Los pilares de confianza de IBM son áreas de enfoque que nos ayudan a tomar medidas para crear y usar la IA de manera ética. Estos pilares son la explicabilidad, la equidad, la solidez, la transparencia y la privacidad. La IA es explicable cuando puede mostrar cómo y por qué llegó a un resultado o recomendación en particular. La IA es justa cuando trata a individuos o grupos de manera equitativa. La IA es sólida cuando puede gestionar eficazmente condiciones excepcionales, como entradas anormales o ataques adversarios. La IA es transparente cuando se comparte la información adecuada con los humanos sobre cómo se diseñó y desarrolló el sistema de IA. Debido a que la IA ingiere tantos datos, es crucial que esté diseñada para priorizar y salvaguardar la privacidad y los derechos de los datos de las personas. IBM ha desarrollado kits de herramientas para respaldar estos pilares. AI Explainability 360 ayuda a comprender e interpretar los modelos de IA para garantizar la transparencia. AI Fairness 360 proporciona métricas y algoritmos para detectar y mitigar los sesgos en los sistemas de IA. Conversational Robustness 360 ayuda a mejorar la seguridad de los sistemas de IA contra los ataques adversarios. Las hojas informativas 360 de AI promueven la transparencia al documentar los modelos, los procesos y el rendimiento de la IA, y AI Privacy 360 protege la privacidad de los datos de los usuarios en las aplicaciones de IA. Estos pilares forman la base del enfoque de IBM para crear una IA confiable que se alinee con los principios éticos y los valores sociales.

Veamos ahora el enfoque de Microsoft para el desarrollo y la implementación éticos de la IA. Microsoft hace hincapié en la participación de los diseñadores, desarrolladores, proveedores de datos y organismos reguladores para garantizar una IA ética. Ha implementado sistemas interconectados que incorporan la supervisión humana para permitir la intervención cuando es necesaria. Microsoft practica la supervisión y la mejora continuas. Mantiene una evaluación continua de los sistemas de IA para abordar las preocupaciones éticas. Microsoft lleva a cabo auditorías periódicas para garantizar el cumplimiento de las normas y estándares éticos. Utiliza evaluaciones de impacto algorítmico (AIA) para identificar y mitigar los sesgos antes de la implementación. Ha establecido organismos como el Octavo Comité de Microsoft, cuya responsabilidad principal es brindar orientación sobre temas, tecnologías, procesos y mejores prácticas para una IA responsable. Por último, Microsoft ha desarrollado un estándar de IA responsable que describe las medidas de rendición de cuentas que incluyen la transparencia, la equidad y la supervisión humana.

Continuando, comprendamos ahora los principios de Google para desarrollar e implementar la IA. Estos principios son, en primer lugar, ser socialmente beneficiosos, lo que significa que la IA debe beneficiar a la sociedad y estar alineada con los principios ampliamente aceptados. En segundo lugar, evitar crear o reforzar el sesgo, lo que implica garantizar la equidad y evitar los impactos injustos. Luego, garantizar que la IA se esté creando y probando para garantizar la seguridad, lo que significa mantener la seguridad y tomar medidas para mitigar los riesgos. Además, Google pone énfasis en rendir cuentas a las personas, lo que subraya la importancia de permitir la dirección y el control humanos adecuados. Incorporar los principios de diseño de privacidad también es fundamental, ya que la IA debe proteger la privacidad y los datos del usuario. Mantener altos estándares de excelencia científica es otro principio, lo que implica mantener el rigor y la integridad en la investigación científica. Y, por último, garantizar que la IA esté disponible para su uso de acuerdo con estos principios, lo que significa restringir el uso a las aplicaciones que sigan estas pautas. Además, la gobernanza de la IA de Google incluye un proceso de revisión para evaluar los riesgos éticos de los proyectos de IA y su cumplimiento de estos principios. Este proceso involucra a un equipo multidisciplinario de expertos que evalúan los proyectos en múltiples etapas.

Además de seguir principios estrictos, las organizaciones están colaborando con las partes interesadas externas para garantizar el uso ético de la IA. Por ejemplo, la Alianza de IA, lanzada conjuntamente por IBM y Meta, junto con organizaciones líderes de varios sectores, se dedica a fomentar una comunidad abierta destinada a acelerar la innovación responsable en materia de IA. Esta iniciativa prioriza el rigor científico, la confianza, la seguridad, la diversidad y la competitividad económica. Al abarcar una amplia gama de organizaciones, la Alianza de IA garantiza que la educación, la investigación, el desarrollo, el despliegue y la gobernanza de la IA se lleven a cabo de manera responsable e inclusiva, beneficiando a todos los involucrados en el ecosistema de la IA. Además de esto, varias consultoras como Deloitte trabajan con clientes y expertos externos para desarrollar estrategias éticas de IA. Proporcionan servicios de consultoría para ayudar a las organizaciones a implementar prácticas éticas de IA.

Varias organizaciones también practican otro aspecto importante del desarrollo de programas éticos: la formación y educación en IA. Por ejemplo, PwC ofrece programas de formación para empleados sobre ética de la IA, enfocándose en educar a su fuerza laboral sobre las implicaciones éticas de la IA y cómo implementar prácticas éticas en sus proyectos. Además, Deloitte ofrece talleres y recursos a los clientes sobre la ética de la IA, ayudando a las organizaciones a comprender y abordar los desafíos éticos en el despliegue de la IA. Por último, muchas organizaciones como Adobe participan en iniciativas de todo el sector, como la Asociación en Materia de IA, para colaborar en la elaboración de estándares éticos de IA. Estas organizaciones contribuyen al desarrollo de directrices y mejores prácticas para el uso responsable de la IA.

En este vídeo, aprendiste sobre los pilares de confianza de IBM. Estos pilares son fundamentales para el enfoque de IBM, que garantiza que los sistemas de IA sean explicables, justos, sólidos, transparentes y preserven la privacidad. También conociste el enfoque de Microsoft con respecto a la IA ética, que implica sistemas humanos integrados, monitoreo y mejora continuos, auditorías periódicas con AIA, organismos de revisión internos y estándares de IA responsables. Has explorado los principios de Google centrados en beneficiar a la sociedad, garantizar la equidad, la rendición de cuentas y mantener altos estándares de excelencia científica.


# La importancia de la gobernanza de la IA


Si has estado siguiendo las noticias y la inteligencia artificial, probablemente ya sepas que este campo está creciendo a un ritmo exponencial. Todos los días escuchamos sobre nuevos casos de uso y aplicaciones de IA con los que ni siquiera habíamos soñado en los últimos años. Sin embargo, en la misma noticia, es probable que también oigas hablar de sistemas de IA que se han implementado en la producción y que no están dando los resultados esperados. Estamos oyendo sobre chatbots que han desviado a los clientes y empleados para que tomen decisiones equivocadas, así como sobre chatbots que dan respuestas alucinantes a los clientes. También hemos escuchado sobre modelos que han generado resultados sesgados. No se puede negar que la inteligencia artificial tiene un enorme potencial, pero el despliegue y la adopción prematuros de los sistemas de IA podrían poner a las empresas en un enorme riesgo de pérdida financiera y de reputación. Esta es exactamente la razón por la que la gobernanza de la inteligencia artificial se ha convertido en uno de los temas más relevantes e importantes de la actualidad.

La gobernanza de la IA se refiere a un conjunto de reglas, estándares y procesos que se han establecido para garantizar el desarrollo y el despliegue responsables y éticos de los sistemas de inteligencia artificial. Piénsalo como un conjunto de barandillas que garantizan el uso ético de estos sistemas de IA, de modo que minimicemos el riesgo en los sistemas y, al mismo tiempo, maximicemos el beneficio potencial. Ahora, todos sabemos cuáles son los beneficios de la inteligencia artificial. La IA reduce los costos, mejora la eficiencia y conduce a la automatización de las tareas manuales y repetitivas. Si bien estos beneficios hacen de la inteligencia artificial la tecnología más solicitada en la actualidad, es el riesgo de la inteligencia artificial lo que hace que la gobernanza de la IA sea aún más importante de debatir.

Para entender los riesgos de la IA, primero debemos comprender qué constituye un sistema de IA. En términos generales, un sistema de IA es aquel diseñado para absorber entradas y producir salidas que imitan, aumentan o ayudan en la toma de decisiones humanas. En el corazón del sistema se encuentra el modelo de IA. Este modelo tiene como objetivo analizar la entrada y generar una salida que normalmente haría un humano. Para que este modelo haga esto correctamente, necesitamos proporcionarle datos generados por humanos. Estos datos pueden estar en cualquier formato: datos estructurados, como columnas y valores; semiestructurados, como archivos XML; o no estructurados, como documentos PDF, archivos de texto, audio, video, entre otros.

Este modelo de IA, que se puede considerar un código de alta ingeniería que utiliza algoritmos matemáticos complejos, analiza los datos, deriva patrones y aprende a imitar el comportamiento humano esperado. Sin embargo, dado que estos datos son generados por humanos, existe la posibilidad de que contengan sesgos. Los humanos no estamos exentos de prejuicios; tenemos varios sesgos cognitivos. Un ejemplo de estos sesgos es que, al tomar decisiones, a veces le damos una importancia indebida a ciertos factores, mientras ignoramos otros. Esto puede provocar sesgos en los datos. Los sesgos no son siempre claramente visibles; no puedes mirar los datos y decir “sí, veo sesgos”, pero están latentes y ocultos. Este código matemático altamente diseñado tiende a captar estos sesgos, y en el peor de los casos, los refleja en los resultados. Eso hace que los sistemas de IA sean susceptibles a los sesgos.

Además de los sesgos, existe otro riesgo relacionado con los datos: la privacidad. Los datos se utilizan para entrenar el modelo, y si no hay una supervisión adecuada, podrían contener información privada o confidencial. Cuando esto sucede, los datos pueden filtrarse en el modelo y aparecer en la salida del sistema, lo que podría llevar a una infracción de la privacidad. En otros casos, si se están utilizando datos no estructurados, como contenido con derechos de autor, esto también podría reflejarse en la salida del modelo, generando infracciones de derechos de autor o de privacidad.

Los modelos de IA también presentan otro desafío: algunos son modelos de “caja negra”. Usamos estos modelos en lugar de los de “caja de cristal” porque los modelos de caja negra tienden a proporcionar un mayor nivel de precisión en los resultados. Sin embargo, cuando se utilizan modelos de caja negra, los desarrolladores del modelo tienen poco control sobre el funcionamiento interno de los algoritmos. Cuando les preguntas por qué su modelo toma una determinada decisión, no pueden darte una explicación clara. Esto significa que el sistema carece de transparencia, lo cual representa un riesgo en términos de confianza. ¿Cómo se puede confiar en un sistema que no puede explicar cómo llegó a una conclusión?

Por último, es importante destacar que los modelos de IA no son estáticos; no siempre generarán resultados de alta calidad de manera constante. Pueden deteriorarse con el tiempo si los datos entrantes se desvían demasiado de los datos con los que fueron entrenados. Debido a este deterioro, es necesario monitorear continuamente los modelos para garantizar que sigan produciendo resultados de alta calidad. Si no se realiza un monitoreo adecuado, los modelos no serán confiables.

Dada la magnitud de estos riesgos, existen organizaciones en todo el mundo que están elaborando reglamentos y directrices sobre cómo administrar estos sistemas. Las directrices, como el reglamento de IA del NIST o el marco de gestión de riesgos de la IA, ofrecen orientación sobre cómo implementar estos sistemas de manera responsable. En cuanto a la normativa, todos hemos oído hablar de la Ley de IA de la UE. Estas regulaciones son mucho más serias porque no solo son guías para el despliegue de sistemas de IA, sino que también pueden penalizar a las empresas por el incumplimiento. Cuando una empresa o un sistema de IA no cumple con las directrices estipuladas, corre el riesgo de perder su reputación y enfrentar pérdidas económicas, además de enfrentar varios dilemas éticos.

Debido a factores como los sesgos, la privacidad, la infracción de derechos de autor, la falta de confianza, la falta de transparencia, la necesidad de monitoreo continuo y las regulaciones, es extremadamente importante que regulen y gestionen sus sistemas de inteligencia artificial. Todos estamos de acuerdo en que la promesa de la IA es innegable, pero los riesgos que plantea son muy reales, y un sistema de IA bien gobernado es esencial para que las organizaciones aprovechen al máximo el potencial de la inteligencia artificial.


# Cómo aplicar la ética de la IA


La ética de la IA es un tema de gran relevancia hoy en día, y las empresas deben evaluar cuidadosamente si sus soluciones de inteligencia artificial corren el riesgo de cruzar algún límite ético. Para ello, es esencial establecer un conjunto de directrices o reglas que se sigan al desarrollar sistemas de IA o al interactuar con ellos. Estas directrices incluyen, primero, que la inteligencia artificial debe servir para aumentar la inteligencia humana y no para reemplazarla. Segundo, los datos y la información deben pertenecer a su creador; es decir, si se utilizan datos de los clientes, siguen siendo de ellos, no de la empresa. Tercero, las soluciones deben ser transparentes y explicables, lo que implica conocer quién entrena el sistema, qué datos se utilizan para entrenarlo y cómo esto afectará las recomendaciones del algoritmo para el usuario final.

Una vez definidas estas reglas, surge la pregunta de cómo verificar que realmente se están siguiendo y no se están infringiendo. Una de las actividades de diseño que se puede realizar es el "mapeo de dicotomía". Esto implica enumerar todas las funciones de la solución y, a continuación, identificar los beneficios y los usos previstos. Por ejemplo, un hotel puede tener un sistema de recomendaciones para sus clientes que determine qué habitación asignarles, si les regalan un obsequio, o incluso si deberían recibir una habitación en un piso superior. Estos beneficios son positivos para el cliente, mejorando su experiencia, pero también es necesario analizar si alguna de estas funciones podría tener efectos negativos, como la venta de datos a anunciantes o la inclusión de personas con capacidades diferentes en el algoritmo.

Con las reglas claras y las actividades realizadas para identificar posibles problemas, el siguiente paso es corregirlos. Para ello, se pueden implementar "barandillas", que son reglas o restricciones que el sistema de IA debe seguir. Por ejemplo, una barandilla sería garantizar que los datos no se vendan a los anunciantes. Además, es crucial estudiar los datos utilizados para entrenar el sistema, asegurándose de que sean diversos y representen a todos los usuarios. En este sentido, herramientas como AI Fairness 360 de IBM pueden ser útiles para detectar y mitigar sesgos en los modelos de aprendizaje automático, así como otras herramientas que ayuden a cumplir con normas de privacidad o detectar la incertidumbre en los modelos.

Finalmente, con reglas claras, métodos para identificar problemas y herramientas para solucionarlos, es posible asegurar que la inteligencia artificial sea responsable y esté construida con un enfoque humano. La IA debe ser diseñada y utilizada de manera que beneficie a todos los usuarios, asegurando que sea segura, inclusiva y alineada con principios éticos.

![[Pasted image 20241125114955.png]]


![[Pasted image 20241125115010.png]]



![[Pasted image 20241125114558.png]]


- Las consideraciones éticas y el uso responsable de la IA se refieren a los principios, prácticas y directrices destinados a garantizar que las tecnologías de inteligencia artificial se desarrollen, desplieguen y utilicen de manera beneficiosa, justa y respetuosa con los derechos humanos y los valores de la sociedad.
    
- Las consideraciones éticas de la IA incluyen la privacidad y la seguridad de los datos, la parcialidad y la equidad, la transparencia y la responsabilidad, los sistemas autónomos y la supervisión humana, y el acceso y la igualdad.
    
- La IA puede utilizarse de forma responsable:
    
    - Utilizando diversos conjuntos de datos
        
    - Diseñando sistemas totalmente autónomos que incluyan mecanismos humanos
        
    - Garantizando la accesibilidad y los beneficios de la IA para todos los segmentos de la sociedad
        
    - Optimizando los algoritmos para la eficiencia energética
        
    - Utilizando fuentes de energía renovables para los centros de datos
        
- La IA generativa implica el uso de algoritmos para crear nuevos contenidos, como texto, imágenes, audio y vídeo, lo que presenta oportunidades y retos únicos.
    
- Algunas consideraciones en torno al uso de la IA generativa son:
    
    - Derechos de autor y propiedad: Es importante que los desarrolladores y usuarios se familiaricen con los marcos legales y se mantengan al día sobre la evolución de las leyes.
        
    - Privacidad y confidencialidad de los datos de los usuarios: Es beneficioso utilizar modelos de IA privados y una combinación de medidas técnicas y legales.
        
    - Deepfakes y desinformación: Es esencial pensar críticamente para garantizar la precisión de los resultados de la IA generativa, y cómo puede mitigar los defectos de la IA como las alucinaciones.
        
    - Consideraciones éticas: Es crucial que los desarrolladores den prioridad a la equidad, la responsabilidad y los beneficios sociales en el desarrollo de la IA.
        
- Examinar las perspectivas de los principales actores puede ayudar a comprender mejor los diversos enfoques de la ética de la IA, los retos que pretenden abordar y las estrategias que emplean para promover el uso responsable de las tecnologías de IA.
    
- Los pilares de confianza de IBM son fundamentales para garantizar que los sistemas de IA sean explicables, justos, sólidos, transparentes y respetuosos con la privacidad.
    
- El planteamiento de Microsoft de la IA ética implica sistemas humanos en bucle, supervisión y mejora continuas, auditorías periódicas utilizando la Ley de Inteligencia Artificial (AIA), órganos de revisión interna y normas de IA responsable.
    
- Los principios de IA de Google se centran en beneficiar a la sociedad, garantizar la equidad y la responsabilidad, y mantener altos niveles de excelencia científica.
    
- El despliegue y la adopción prematuros de sistemas de IA podrían exponer a las empresas a un enorme riesgo de pérdidas financieras y de reputación.
    
- La gobernanza de la IA se refiere a un conjunto de reglas, normas y procesos que se han establecido para garantizar el desarrollo y despliegue responsable y ético de los sistemas de inteligencia artificial.
    
- Riesgos asociados a la IA:
    
    - Sesgo en los modelos de IA:
        
        - Los datos generados por humanos pueden contener sesgos ocultos.
            
        - Los modelos de IA pueden captar estos sesgos y reflejarlos en sus resultados.
            
    - Intimidad y violación de los derechos de autor:
        
        - Los datos sensibles pueden incluirse inadvertidamente en los modelos.
            
        - Datos no estructurados pueden contener material protegido por derechos de autor.
            
    - Falta de transparencia:
        
        - Los modelos de caja negra ofrecen una gran precisión, pero carecen de explicabilidad.
            
        - La falta de transparencia puede erosionar la confianza en los sistemas de IA.
            
    - Deterioro de los modelos:
        
        - Los modelos de IA requieren una supervisión y actualización continuas.
            
        - Los datos entrantes pueden diferir de los datos de entrenamiento, lo que provoca una degradación del rendimiento.
            
- La supervisión continua garantiza que los modelos de IA produzcan resultados coherentes y de alta calidad, y evita el deterioro del rendimiento del modelo con el paso del tiempo.
    
- Organizaciones de todo el mundo están creando normativas y directrices para la IA.
    
    - Algunos ejemplos son el marco de gestión de riesgos de IA del NIST y la Ley de IA de la UE.
        
    - Las normativas pueden penalizar a las empresas que las incumplan, lo que subraya la importancia de adherirse a directrices éticas.
        
- La IA promete importantes beneficios, pero también plantea riesgos reales.
    
- Una gobernanza adecuada es crucial para aprovechar plenamente el potencial de la IA al tiempo que se mitigan los riesgos asociados.




![[Pasted image 20241125120627.png]]


![[Pasted image 20241125120638.png]]


![[Pasted image 20241125120649.png]]

![[Pasted image 20241125120700.png]]

![[Pasted image 20241125120711.png]]








