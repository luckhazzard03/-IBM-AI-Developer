

Bienvenido a aprovechar la IA generativa en el ciclo de vida del desarrollo de software. Este enfoque examina el papel de la IA en las distintas fases del ciclo de vida del desarrollo de software (SDLC), que incluyen la recopilación y análisis de requisitos, el diseño, el desarrollo, las pruebas, la implementación y el mantenimiento. A través de la inteligencia artificial, se pueden automatizar tareas clave como el análisis de requisitos, la generación de código, la validación de calidad y la integración continua.

En la fase de recopilación y análisis de requisitos, la IA facilita la automatización del análisis de los requisitos de los usuarios, ayudando a identificar patrones o tendencias en grandes volúmenes de datos. En el diseño, la IA proporciona recomendaciones para una arquitectura óptima del software y genera documentación basada en datos históricos y mejores prácticas. Esto también incluye la automatización del desarrollo de tareas repetitivas de código y la revisión del código, identificando posibles errores o vulnerabilidades.

Durante las pruebas, la IA optimiza la generación y ejecución de casos de prueba, lo que reduce significativamente el esfuerzo requerido para realizar pruebas manuales. Además, al analizar los cambios en el código, la IA puede identificar áreas específicas del sistema que podrían verse afectadas y realizar pruebas de regresión de manera más eficaz. En la fase de implementación, la IA ayuda a automatizar los procesos de despliegue, garantizando que los mismos sean coherentes y sin errores, además de supervisar el rendimiento del software una vez implementado, alertando sobre cualquier anomalía o problema.

Finalmente, en la fase de mantenimiento, la IA es útil para analizar los archivos de registro y comentarios de los usuarios, permitiendo la identificación de patrones y predicción de problemas antes de que ocurran. También ofrece recomendaciones para la optimización del código y propone soluciones a los errores informados.

El uso de la IA en el ciclo de vida del software es cada vez más eficiente, destacando en áreas como el análisis de requisitos, donde se aplican técnicas de procesamiento de lenguaje natural (PLN) y aprendizaje automático (ML) para extraer información significativa de los requisitos del lenguaje natural. Estas herramientas no solo ayudan a mejorar la calidad al detectar posibles inconsistencias, sino que también automatizan el proceso de validación de los requisitos.

Las tecnologías de IA también permiten mejorar el diseño y la arquitectura, analizando grandes cantidades de datos y patrones de diseño exitosos en proyectos previos. Esto permite la optimización del rendimiento y la escalabilidad del sistema, asegurando que el software sea eficiente y robusto. A través de la IA, se identifican patrones y marcos de diseño reutilizables, lo que ahorra tiempo y esfuerzo en el proceso de desarrollo.

La generación de código, otro proceso fundamental en el desarrollo de software, también se ve beneficiada por la IA. Los modelos de aprendizaje automático predicen y generan fragmentos de código a partir de bases de código existentes, ayudando a completar el código y hacer sugerencias para evitar errores. Además, las herramientas de análisis estático basadas en IA mejoran la calidad del código al identificar errores y vulnerabilidades.

En cuanto a las pruebas y la garantía de calidad, la IA automatiza la generación y ejecución de casos de prueba, mejorando la cobertura y reduciendo el tiempo invertido en pruebas manuales. Los algoritmos de IA también ayudan a predecir y prevenir defectos mediante el análisis de datos históricos de proyectos anteriores, identificando patrones de posibles fallos o áreas de alto riesgo.

El enfoque DevOps, que busca agilizar la entrega de software, también se beneficia de la IA. Al integrar IA en los procesos de DevOps, se facilita la toma de decisiones automatizada y se minimizan los errores humanos. Los algoritmos de IA también optimizan la gestión de versiones y predicen el impacto de los cambios en el rendimiento del sistema, lo que ayuda a tomar decisiones informadas sobre el lanzamiento y la asignación de recursos. Además, la IA permite la supervisión continua del sistema y la optimización del rendimiento, mejorando la experiencia del usuario final.

En resumen, la IA juega un papel crucial en todas las fases del ciclo de vida del desarrollo de software, desde la recopilación de requisitos hasta el mantenimiento, mejorando la eficiencia, la calidad y la toma de decisiones en tiempo real.

# Modelos de lenguaje grande (LLM) y Transformadores

![[Pasted image 20241213105150.png]]



La IA ofrece un sinfín de capacidades, y uno de sus modelos más destacados es el Modelo de Lenguaje Amplio (LLM), que se ha convertido en una poderosa herramienta para la comprensión y generación de texto en varios idiomas. Los LLM se entrenan utilizando técnicas de aprendizaje profundo en grandes conjuntos de datos, los cuales incluyen textos de diversas fuentes como libros, artículos y sitios web. Esta formación les permite reconocer patrones lingüísticos, lo que les habilita para producir textos coherentes y relevantes dentro de un contexto específico.

Los LLM sobresalen especialmente en el procesamiento del lenguaje natural (PLN), lo que les permite realizar tareas complejas como la generación de texto, traducción, resumen, análisis de sentimientos y respuesta a preguntas. Al comprender el contexto, la semántica y los matices del lenguaje, los LLM pueden generar respuestas que imitan la comunicación humana, lo que los hace muy valiosos para aplicaciones que requieren interacción con el lenguaje.

![[Pasted image 20241213105231.png]]


Algunos de los LLM más populares incluyen GPT de OpenAI, que es ampliamente conocido por su uso en el chatbot ChatGPT, y que también impulsa la plataforma de chat Bing de Microsoft. Otros modelos notables son LaMDA de Google, utilizado para su chatbot Bard, LLama de Meta IA, y Claude, desarrollado por Anthropic. Estos modelos han transformado el desarrollo de software de manera significativa.

En el ámbito del desarrollo de software, los LLM juegan un papel crucial. Ayudan a generar código y completarlo automáticamente, lo que facilita la creación de programas y acelera el proceso de desarrollo. Los LLM también analizan bases de código existentes, sugiriendo completaciones o mejoras, y detectan errores de forma automática, lo que contribuye a la calidad del código y reduce la carga de trabajo del desarrollador. Además, sirven como una interfaz de programación en lenguaje natural, permitiendo que las personas se comuniquen con el código utilizando un lenguaje más accesible, lo que mejora la comprensión por parte de quienes no son expertos en programación.

Otra de sus capacidades es generar plantillas y fragmentos de código basados en los requisitos específicos de un proyecto, lo que ahorra tiempo en tareas repetitivas. También ayudan con la documentación y los comentarios, facilitando la legibilidad del código y simplificando su mantenimiento. Los LLM automatizan las pruebas y la validación del código, garantizando la calidad del software y reduciendo la necesidad de pruebas manuales. Además, fomentan la colaboración entre desarrolladores al sugerir mejoras durante las revisiones del código o en las sesiones de programación en pareja.

A pesar de sus ventajas, los LLM también presentan ciertos desafíos. Es fundamental considerar los aspectos éticos y la detección de sesgos, ya que los modelos pueden heredar prejuicios de los datos con los que fueron entrenados. Además, es necesario garantizar la seguridad y la privacidad del código, protegiendo el acceso no autorizado y el uso indebido de la información. Aunque los LLM son muy efectivos en lenguaje natural, pueden tener dificultades con contextos de programación más complejos, lo que requiere una evaluación cuidadosa de su aplicabilidad en casos específicos. Además, es crucial equilibrar la intervención humana con el código generado por máquinas, para evitar errores imprevistos y garantizar que el resultado final cumpla con los estándares deseados.

El futuro de los LLM en el desarrollo de software sigue siendo prometedor, con investigaciones continuas que buscan mejorar sus arquitecturas y metodologías de entrenamiento. Se está explorando la integración de los LLM en entornos de desarrollo integrados (IDEs), facilitando a los desarrolladores el acceso a sus funcionalidades. Además, los LLM se están expandiendo a dominios de programación más especializados, lo que les permite satisfacer necesidades específicas de los desarrolladores. La colaboración entre los desarrolladores y los LLM continuará evolucionando, optimizando los procesos de desarrollo a lo largo de varias etapas del ciclo de vida del software.

En resumen, los LLM son modelos avanzados de IA entrenados para reconocer patrones lingüísticos y generar texto coherente y relevante. Entre los LLM más conocidos se encuentran GPT, LaMDA, LLama, y Claude. Estos modelos son muy útiles en el desarrollo de software, ya que ayudan a generar y completar código, detectar y corregir errores, servir como interfaces de programación en lenguaje natural y mejorar la productividad general. A pesar de sus capacidades, los LLM deben ser utilizados con precaución, considerando aspectos éticos, de seguridad, y asegurando el balance adecuado entre la intervención humana y el código generado automáticamente.



# Procesamiento de lenguaje natural (NLP) y su importancia


Bienvenido al procesamiento del lenguaje natural (PNL) y su importancia. El PNL implica el uso de técnicas computacionales para analizar, comprender y generar el lenguaje humano. Combina aspectos de la lingüística, la ciencia computacional y la inteligencia artificial, permitiendo a las computadoras realizar tareas como análisis de sentimientos, reconocimiento de entidades nombradas (NER), clasificación de textos, traducción automática, entre otras.

**Componentes esenciales del PNL:**

1. **Conceptos lingüísticos**: Incluyen la morfología (estructura de las palabras), la sintaxis (estructura de las oraciones) y la semántica (significado de las palabras y oraciones).
2. **Técnicas de procesamiento y limpieza de textos**: Transforman los datos de texto sin procesar en un formato adecuado para el análisis. Esto incluye procesos como la tokenización (dividir el texto en unidades más pequeñas) y la derivación (reducir las palabras a su forma raíz).

**Bibliotecas y herramientas de PNL comunes**:

- **IBM Watson**
- **Aylien**
- **API de NLP de Google Cloud**
- **NLTK** (una de las bibliotecas más populares de Python)
- **MonkeyLearn**
- **Amazon Comprehend**
- **Stanford Core NLP**
- **TextBlob**
- **SpaCy**
- **GenSim**

**Casos de uso de la PNL en el desarrollo de software:**

1. **Análisis de sentimientos**: Determina el sentimiento expresado en un texto, asignando puntuaciones de polaridad a palabras o frases. Los enfoques incluyen métodos basados en reglas, algoritmos de aprendizaje automático como Naive Bayes y SVM, y modelos de aprendizaje profundo como redes neuronales recurrentes y transformadores.
2. **Reconocimiento de entidades nombradas (NER)**: Identifica y clasifica entidades como nombres de personas, organizaciones o fechas dentro del texto. Los modelos de NER pueden ser supervisados o no supervisados.
3. **Clasificación de textos**: Asigna categorías a fragmentos de texto. Esto se puede lograr utilizando técnicas como la bolsa de palabras, TFIDF, o redes neuronales.
4. **Traducción automática**: La PNL juega un papel crucial en la traducción automática al permitir que las computadoras comprendan y generen diferentes lenguajes humanos.
5. **Chatbots y agentes conversacionales**: Los chatbots simulan conversaciones humanas utilizando PNL para entender consultas y generar respuestas. Los agentes conversacionales avanzados, por su parte, pueden manejar conversaciones complejas y generar respuestas contextualmente relevantes.

**Otras aplicaciones clave de la PNL**:

- **Extracción de información**: Permite extraer datos estructurados de texto no estructurado, lo que es útil en tareas como la minería de datos o la agregación de noticias.
- **Resumen**: Generación automática de resúmenes de textos grandes o documentos.

**Implicaciones éticas del PNL**: El uso de la PNL plantea preocupaciones éticas relacionadas con la privacidad, los prejuicios, la equidad y la transparencia. Es esencial garantizar que los algoritmos sean justos y no discriminen, abordando los sesgos de los datos a lo largo del ciclo de vida del desarrollo. Las prácticas éticas incluyen:



- Obtener el consentimiento informado para el uso de datos.
- Garantizar la transparencia en las decisiones de los algoritmos.
- Evaluar regularmente los modelos para detectar posibles sesgos.
- Ofrecer mecanismos de retroalimentación y recursos para los usuarios.

![[Pasted image 20241213110027.png]]

En resumen, el procesamiento del lenguaje natural permite a las computadoras interactuar de manera más efectiva con los seres humanos. Las bibliotecas y herramientas como IBM Watson, NLTK, y SpaCy son fundamentales para implementar técnicas de PNL en el desarrollo de software. Sin embargo, es esencial considerar las implicaciones éticas de la PNL para asegurar que su uso sea justo, transparente y respetuoso con la privacidad de los usuarios.




**Tokens en la IA Generativa**  


**Objetivos**  
Al completar esta lectura, podrás:

- Definir tokens, límites de tokens y explorar tokens.
- Identificar y explicar las herramientas de tokens.
- Describir el proceso de conteo de tokens.
- Explicar las directrices de OpenAI para el conteo de tokens.
- Resumir cómo estimar los costos de la IA.
- Enumerar los precios actuales de los modelos de IA.

**¿Qué son los tokens en las API de IA?**  
Los tokens son fragmentos cruciales de la API de ChatGPT. Los tokens son fragmentos o segmentos de palabras. Antes de procesar los avisos (prompts), la API divide la entrada en estos tokens individuales.

Estos tokens no necesariamente se alinean exactamente con el inicio o final de las palabras: pueden incluir espacios finales e incluso sub-palabras.

**Límites de tokens**: Normalmente, las solicitudes pueden usar hasta 4097 tokens compartidos entre el aviso y la respuesta, dependiendo del modelo. Por ejemplo, si tu aviso consta de 4000 tokens, la respuesta puede contener un máximo de 97 tokens. Este límite es actualmente una restricción técnica, pero existen formas creativas de trabajar dentro de él, como condensar el aviso o dividir el texto en fragmentos más pequeños.

**Precios de tokens**: La API ofrece varios tipos de modelos con diferentes puntos de precio. Cada modelo tiene un rango de capacidades, siendo "gpt-3.5-turbo" el más capaz. Las solicitudes realizadas a estos diferentes modelos tienen precios distintos. Se puede obtener información detallada sobre los precios de los tokens en la página del producto API.

**Explorando los tokens**: La API trata las palabras según su contexto en los datos del corpus. GPT-3 toma el aviso, convierte la entrada en una lista de tokens, procesa el aviso y luego convierte los tokens predichos de nuevo en palabras como respuesta. Ten en cuenta que lo que podría parecer dos palabras idénticas se genera como tokens diferentes dependiendo de su estructura dentro del texto. Por ejemplo, observa cómo la API genera valores de tokens para la palabra "rojo" según su contexto.

**Herramientas de tokens**  
La herramienta más popular es la herramienta interactiva de tokenización de OpenAI. Esta herramienta ayuda a calcular el número de tokens y observar cómo el texto se descompone en tokens.

Alternativamente, si es necesario tokenizar texto de manera programática, usa Tiktoken, un tokenizador BPE rápido diseñado específicamente para los modelos de OpenAI.

Otras bibliotecas incluyen el paquete transformers para Python o el paquete gpt-3-encoder para Node.js.

**¿Cómo contar los tokens?**  
Para contar los tokens al llamar a una API de OpenAI, sigue estos pasos:

1. **Identificar el endpoint de la API**: Determina qué endpoint de OpenAI planeas llamar.  
    Cada endpoint representa una función o recurso específico que la API proporciona.
    
2. **Revisar la documentación de la API**: Accede a la documentación proporcionada por el proveedor de la API. La documentación indicará la estructura de las solicitudes y respuestas de la API, incluidos los encabezados, parámetros o métodos de autenticación requeridos.
    
3. **Verificar si la API necesita autenticación basada en tokens**: La autenticación basada en tokens implica incluir un token de acceso en el encabezado de la solicitud para autorizar la llamada a la API. El token generalmente se obtiene registrando una aplicación con el proveedor de la API y siguiendo su proceso de autenticación.
    
4. **Obtener un token de acceso**: Si se requiere autenticación basada en tokens, obtén un token de acceso siguiendo el proceso de autenticación especificado por el proveedor de la API. Este paso puede implicar registrar una aplicación, proporcionar credenciales y recibir un token.
    
5. **Contar los tokens por cada API**: Una vez que tengas un token de acceso, cuéntalo como un token por cada llamada a la API que realices. Cada solicitud al endpoint de la API, que incluya el token de acceso en el encabezado, cuenta como un solo token.
    
6. **Rastrear el uso de los tokens**: Mantén un seguimiento de la cantidad de tokens que has usado para asegurarte de no exceder los límites o cuotas de uso que establezca el proveedor de la API. Algunas APIs pueden restringir el número de tokens que puedes usar dentro de un período determinado.
    

**Directrices de OpenAI para el conteo de tokens**  
Según el documento oficial de OpenAI, las siguientes son las estimaciones para el conteo de tokens:

- 1 token ≈ 4 caracteres en inglés
- 1 token ≈ ¾ palabras
- 100 tokens ≈ 75 palabras
- 1-2 oraciones ≈ 30 tokens
- 1 párrafo ≈ 100 tokens
- 1,500 palabras ≈ 2048 tokens

**Ejemplo: Conteo de tokens para llamadas a la API**  
Por ejemplo, tienes un endpoint de OpenAI que requiere una solicitud GET para obtener la información de un usuario. El endpoint tiene los siguientes detalles:

Método de solicitud: GET  
URL del endpoint: [https://api.example.com/users/{userId}](https://api.example.com/users/%7BuserId%7D)  
Parámetro de ruta: userId (valor: "123")  
Parámetro de consulta: includeAddress (valor: true)

Conteo de tokens para esta solicitud:

- El método de solicitud 'GET' generalmente requiere 1 token.
- La URL del endpoint no consume tokens adicionales.
- El parámetro de ruta 'userId' con valor '123' no consume tokens adicionales.
- El parámetro de consulta 'includeAddress' con el valor 'true' consume 2 tokens (1 token para el nombre del parámetro y 1 token para el valor del parámetro).

Sumando los tokens de los pasos 1 a 4, el total de tokens para esta solicitud a la API sería de 3 tokens.

Nota: El método exacto para contar tokens puede variar según la implementación específica de la API o el proveedor de la API. Consulta la documentación de la API para obtener información precisa sobre el uso de tokens y los costos o limitaciones asociados.

**Estimación de costos de IA**  
Ejemplo: Calculando el costo de un aviso de entrada y una salida utilizando una fórmula de cálculo de costos de muestra.

Aviso de entrada: Aviso: “¿Puedes proporcionar un resumen breve de los beneficios del ejercicio?”

Paso 1: Determinar el número de palabras en el aviso.  
En este caso, el aviso tiene 9 palabras.

Paso 2: Calcular el costo del aviso en función del número de tokens. Supón que el costo por 1000 tokens es $0.03 (como se menciona en la tabla de precios).  
Dado que hay 9 palabras en el aviso y la tasa promedio de conversión es de 750 palabras por cada 1000 tokens, el costo del aviso será: (9 / 750) * (0.03) = $0.00036

Paso 3: Si el modelo de IA genera una salida de aproximadamente 500 palabras, usa la misma fórmula para calcular el costo de generar la salida.  
Costo de salida = (500 / 750) * (0.06) = $0.04

Paso 4: Calcular el costo total.  
Precio total estimado = Costo del aviso + Costo de la salida = $0.00036 + $0.04 = $0.04036

Por lo tanto, el precio estimado para generar una salida de aproximadamente 500 palabras utilizando el aviso dado es $0.04036.

De manera similar, si la aplicación llama a la API 1000 veces al día, el costo es el siguiente:

Costo por día = $40.36  
Costo por mes = $1,210.80

Nota: El costo real puede variar dependiendo del número exacto de tokens utilizados para generar el contenido y el tipo de modelo utilizado.

**Precios actuales de los modelos**  
Descargo de responsabilidad: La tabla de precios proporcionada es precisa al momento de la redacción. Sin embargo, puede estar sujeta a cambios en el futuro. Se recomienda consultar la documentación para obtener la información más actualizada y precisa sobre los precios.

Modelos bajo consideración: Varios modelos, cada uno con diferentes capacidades y puntos de precio.

Los precios son por 1,000 tokens. Dado que los tokens son fragmentos de palabras en este modelo, 1,000 tokens equivalen a aproximadamente 750 palabras. [Este párrafo tiene 35 tokens].

**Tabla de precios según el modelo**:

|Modelo|Nombre|Entrada/uso|Salida/uso|
|---|---|---|---|
|GPT-4 Turbo|gpt-4-1106-preview|$0.01 / 1K tokens|$0.03 / 1K tokens|
|gpt-4-1106-vision-preview|$0.01 / 1K tokens|$0.03 / 1K tokens||
|GPT-4|gpt-4|$0.03 / 1K tokens|$0.06 / 1K tokens|
|gpt-4-32k|$0.06 / 1K tokens|$0.12 / 1K tokens||
|GPT-3.5-Turbo|gpt-3.5-turbo-1106|$0.0010 / 1K tokens|$0.0020 / 1K tokens|
|gpt-3.5-turbo-instruct|$0.0015 / 1K tokens|$0.0020 / 1K tokens||
|Code Interpreter|Code Interpreter|$0.03 / sesión||
|DALL·E 3|DALL·E 3 Standard|1024×1024 - $0.040 / imagen||
|DALL·E 3|DALL·E 3 HD|1024×1024 $0.080 / imagen||

**Resumen**  
En esta lectura, aprendiste que:

- Antes de procesar los avisos, la API descompone la entrada en estos tokens individuales. Los tokens son fragmentos o segmentos de palabras.
- Las herramientas de tokenización más comunes son la herramienta interactiva de OpenAI y Tiktoken.
- Para contar los tokens, identifica el endpoint de la API, revisa la documentación de la API, verifica la autenticación de la API, obtiene un token de acceso, cuenta los tokens por cada API y realiza un seguimiento de su uso.


Step 3: Search for the **IntelliCode** extension in the available extensions list. Select it, and then click the **Install** button adjacent to the **IntelliCode** extension to initiate the installation process.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/T2Image3_zoomed.png)  
  

Step 4: Upon completion of the installation, you'll see the window as shown in the screenshot below.

Restart the Visual Studio Code for the changes to take effect.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/T2Image4.png)

### Task 3: Installing Code Spell Checker in Visual Studio Code

The Code Spell Checker is an extension in Visual Studio Code designed to assist developers in identifying and correcting spelling errors within their code.

Step 1: In the Extensions view, search for **Code Spell Checker** in the search bar. Look for the **Code Spell Checker** extension in the search results and select it.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/Code_spell1.png)  
  

Step 2: To begin the installation, click the **Install** button next to the Code Spell Checker extension.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/Code_spell2.png)  
  

Step 3: Select **Trust Workspace and Install**.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/Code_spell3_zoomed.png)  
  

Step 4: Once the installation is completed, you'll see the window as shown in the screenshot below. Restart the Visual Studio Code for the changes to take effect.

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0274EN-SkillsNetwork/images/Code_spell_installed.png)  
  

**Congratulations! You just completed this lab successfully on Setting up the development environment using Visual Studio Code.**




![[Pasted image 20241213111713.png]]


![[Pasted image 20241213111722.png]]


![[Pasted image 20241213111730.png]]








