
#InteligenciaIA #ApIS

**Aplicaciones de las APIs**

Las APIs tienen una amplia gama de aplicaciones, algunas de las cuales son:

- **Plataformas de redes sociales:**  
    Las plataformas de redes sociales como Facebook, Twitter e Instagram utilizan APIs para permitir que los desarrolladores accedan a sus datos y funcionalidades. Esto permite a los desarrolladores crear aplicaciones que interactúan con estas plataformas y proporcionan funcionalidades adicionales a los usuarios.
    
- **Sitios web de comercio electrónico:**  
    Sitios web de comercio electrónico como Amazon y eBay utilizan APIs para permitir que los desarrolladores accedan a sus catálogos de productos y otros datos. Esto permite a los desarrolladores crear aplicaciones que interactúan con estas plataformas y ofrecen funcionalidades adicionales a los usuarios.
    
- **Aplicaciones meteorológicas:**  
    Aplicaciones meteorológicas como AccuWeather y The Weather Channel utilizan APIs para acceder a datos meteorológicos de diversas fuentes. Esto permite a los desarrolladores crear aplicaciones que proporcionan información meteorológica actualizada a los usuarios.
    
- **Aplicaciones de mapas y navegación:**  
    Aplicaciones de mapas y navegación como Google Maps y Waze utilizan APIs para acceder a datos de ubicación y otra información. Esto permite a los desarrolladores crear aplicaciones que proporcionan direcciones, actualizaciones de tráfico y otra información basada en la ubicación.
    
- **Pasarelas de pago:**  
    Las pasarelas de pago como PayPal y Stripe utilizan APIs para permitir que los desarrolladores accedan a sus funcionalidades de procesamiento de pagos. Esto permite a los desarrolladores crear aplicaciones que pueden procesar pagos de manera segura y eficiente.
    
- **Aplicaciones de mensajería:**  
    Aplicaciones de mensajería como WhatsApp y Facebook Messenger utilizan APIs para permitir que los desarrolladores accedan a sus funcionalidades de mensajería. Esto permite a los desarrolladores crear aplicaciones que interactúan con estas plataformas y proporcionan funcionalidades adicionales a los usuarios.
    

### Conclusión

En resumen, las APIs son una parte esencial del desarrollo de software y proporcionan una manera de acceder a datos y funcionalidades de otros sistemas. Se utilizan en una amplia gama de aplicaciones y pueden ayudar a los desarrolladores a ahorrar tiempo y recursos al crear nuevas aplicaciones.



# resumen de Open 


- Python utiliza la función open() y le permite leer y escribir archivos, proporcionando acceso al contenido dentro del archivo para la lectura. También permite sobrescribirlo para escribir y especifica el modo de archivo (por ejemplo, r para leer, w para escribir, a para anexar).
    
    - Para leer un archivo, Python utiliza una función open junto con _r._
        
    - Python utiliza la funciónabrir**con** para leer y procesar un atributo de archivo, es decir, desde abrir hasta cerrar.
        
    - En Python, se utiliza el método **open** para editar o sobrescribir un archivo.
        
    - Para escribir un archivo, Python utiliza la función **abrir** junto con _w._
        
    - En Python, "a" indica que el programa ha anexado al archivo.
        
    - En Python, "\n" significa que el código debe comenzar en una nueva línea.
        
    - Python utiliza varios métodos para imprimir líneas de atributos.
        
- Pandas es una potente biblioteca de Python para la manipulación y el análisis de datos, que proporciona estructuras de datos y funciones para trabajar con datos estructurados como marcos de datos y series.
    
    - Usted importa el archivo (panda) utilizando el comando import seguido del nombre del archivo.
        
    - En Python, se utiliza el comando **as** para proporcionar un nombre más corto para el archivo.
        
    - En Pandas, se utiliza un marco de datos (df) para especificar los archivos a leer.
        
    - Los DataFrames constan de filas y columnas.
        
    - Puede crear nuevos DataFrames utilizando la columna o columnas de un DataFrame específico.
        
    - Podemos trabajar con datos en un DataFrames y guardar los resultados en diferentes formatos.
        
    - En Python, se utiliza el método **Único** para determinar elementos únicos en una columna de los DataFrames.
        
    - Usted utiliza el Operador de desigualdad junto con df para asignar un valor Booleano a la columna seleccionada en los DataFrames.
        
    - Usted guarda un nuevo DataFrame como un DataFrame diferente, que puede contener valores de un DataFrame anterior.
        
- NumPy es una biblioteca de Python para operaciones numéricas y matriciales, que ofrece objetos de matrices multidimensionales y una gran variedad de funciones matemáticas para trabajar con datos de forma eficiente.
    
    - NumPy es la base de Pandas.
        
    - Una matriz NumPy o matriz ND es similar a una lista, normalmente de un tamaño fijo con el mismo tipo de elemento.
        
- Una matriz NumPy unidimensional es una secuencia lineal de elementos con un solo eje, como una lista tradicional, pero optimizada para cálculos numéricos y operaciones con matrices.
    
    - Puede acceder a los elementos de un NumPy mediante un índice.
        
    - Utilice el atributo **dtype** para obtener el tipo de datos de los elementos de la matriz.
        
    - Utilice **nsize** y **ndim** para obtener el tamaño y la dimensión de la matriz, respectivamente.
        
    - Puede utilizar métodos de indexación y troceado en NumPy.
        
    - Las sumas de vectores son operaciones muy utilizadas en Python.
        
    - Es útil representar la suma de vectores con segmentos de línea o flechas.
        
    - Los códigos de NumPy funcionan mucho más rápido, lo que resulta útil con muchos datos.
        
    - La resta de vectores se realiza sustituyendo el signo de suma por un signo negativo.
        
    - Multiplicar una matriz por un escalar en Python implica multiplicar cada elemento de la matriz por el valor escalar, dando lugar a una nueva matriz en la que cada elemento escala por el escalar.
        
    - El producto Hadamard se refiere a la multiplicación elemento a elemento de dos matrices de la misma forma, dando lugar a una nueva matriz en la que cada elemento es el producto de los elementos correspondientes en las matrices de entrada.
        
    - El producto punto en Python es la suma de los productos elemento a elemento de dos matrices, a menudo utilizado en operaciones vectoriales y matriciales para hallar el resultado escalar de multiplicar los elementos correspondientes y sumarlos.
        
    - Cuando se trabaja con NumPy, es habitual utilizar bibliotecas como Matplotlib para crear gráficos y visualizaciones a partir de datos numéricos almacenados en matrices NumPy.
        
- Una matriz NumPy bidimensional es una estructura en forma de cuadrícula con filas y columnas adecuada para representar datos como una matriz o una tabla para cálculos numéricos.
    
    - En NumPy, "forma" se refiere a las dimensiones de una matriz (número de filas y columnas), indicando su tamaño y estructura.
        
    - Utilice el atributo "tamaño" para obtener el tamaño de una matriz.
        
    - Utilice atributos rectangulares para acceder a los distintos elementos de una matriz.
        
    - utilice un escalar para multiplicar elementos en NumPy.
        



![[Pasted image 20241202145020.png]]



![[Pasted image 20241202145030.png]]


![[Pasted image 20241202145039.png]]


![[Pasted image 20241202145046.png]]


![[Pasted image 20241202145056.png]]


![[Pasted image 20241202145104.png]]


![[Pasted image 20241202145115.png]]



# Interfaz de programación de aplicaciones



En este video discutiremos la aplicación de Interfaces de Programación de Aplicaciones (APIs, por sus siglas en inglés). En concreto, analizaremos qué es una API, las bibliotecas de API y las APIs de REST, abordando temas como solicitud y respuesta, y presentando un ejemplo práctico con la librería PyCoinGecko. Una API permite que dos piezas de software se comuniquen entre sí. Por ejemplo, imagina que tienes tu programa con algunos datos, y además, tienes otros componentes de software. En este escenario, utilizas la API para establecer la comunicación entre estos programas a través de entradas y salidas. Al igual que una función en programación, no es necesario entender cómo funciona internamente la API, sino solo conocer qué datos se esperan como entrada y qué datos se devuelven como salida.

Pandas es un buen ejemplo de un conjunto de componentes de software que interactúan a través de una API, muchos de los cuales ni siquiera están escritos en Python. Tienes unos datos, un conjunto de componentes de software, y usas la API de Pandas para procesar esos datos, comunicándote con los diferentes componentes subyacentes. Al crear un diccionario y luego construir un objeto pandas con el constructor `DataFrame`, en la jerga de la API, estás creando una "instancia". Los datos del diccionario se envían a la API de Pandas, y luego puedes usar el marco de datos (DataFrame) para interactuar con la API. Por ejemplo, cuando llamas a un método de encabezado, el DataFrame se comunica con la API para mostrar las primeras filas del conjunto de datos. Al llamar a otro método, la API realiza los cálculos y devuelve los resultados.

Las APIs REST son otro tipo popular de API que permiten la comunicación a través de Internet. Estas APIs facilitan el acceso a recursos como almacenamiento, datos, algoritmos de inteligencia artificial, y más. "REST" significa _Representational State Transfer_, lo que implica que una API REST se basa en la transferencia de datos representacionales entre un cliente y un servidor. En una API REST, tu programa actúa como un "cliente", que interactúa con un "recurso" a través de un servicio web al que se accede mediante Internet. La comunicación entre el cliente y el servicio se realiza mediante solicitudes y respuestas, siguiendo un conjunto específico de reglas. El cliente envía solicitudes al recurso y recibe respuestas. Los métodos HTTP son una forma estándar de transmitir datos por Internet. Cuando envías una solicitud a una API REST, normalmente se utiliza un mensaje HTTP que puede contener un archivo JSON, el cual incluye instrucciones sobre qué operación realizar. Esta solicitud es procesada por el servicio web, que a su vez devuelve la respuesta, generalmente en formato JSON, con los datos solicitados.

Un uso muy común de las APIs es en el contexto de los datos de criptomonedas, ya que estos datos se actualizan constantemente y son cruciales para las transacciones en los mercados de criptomonedas. En este video, se utiliza el cliente de Python PyCoinGecko para interactuar con la API de CoinGecko, que proporciona datos de criptomonedas actualizados cada minuto. PyCoinGecko es fácil de usar, permitiendo que los usuarios se concentren en la tarea de recopilar datos en lugar de lidiar con las complejidades de la API. Para obtener datos, solo necesitas instalar e importar la biblioteca, crear un objeto de cliente y usar una función para hacer una solicitud. En el ejemplo mostrado, se obtienen datos sobre el precio de Bitcoin en dólares estadounidenses durante los últimos 30 días. La respuesta que se recibe es un archivo JSON, que se convierte en un diccionario de Python con listas anidadas que incluyen información como precios, capitalización bursátil y volúmenes de transacciones, con la marca de tiempo de Unix correspondiente.

Una vez que se obtienen los datos, se selecciona únicamente el precio, usando la clave de "precio" en el JSON. Para simplificar la manipulación de los datos, podemos convertir la lista anidada en un DataFrame de Pandas. Al hacerlo, creamos un DataFrame con columnas de marca de tiempo y precios. Sin embargo, la marca de tiempo en formato Unix es difícil de interpretar, por lo que se convierte en una versión legible utilizando la función `to_datetime` de Pandas. Esta función toma la columna de marca de tiempo y la convierte a un formato legible por humanos, utilizando milisegundos como unidad de tiempo. La columna de fechas convertidas se añade a nuestro DataFrame, facilitando el análisis posterior.

En los siguientes pasos, se busca crear un gráfico de velas, que es una representación comúnmente utilizada en los mercados financieros. Para ello, se agrupan los datos por fecha, y se calculan los valores mínimos, máximos, el primer precio y el último precio de cada día. Usando la librería Plotly, se genera el gráfico de velas, que permite visualizar de manera clara cómo ha variado el precio de Bitcoin a lo largo del tiempo. El gráfico se guarda como un archivo HTML, que puede ser visualizado en un navegador web. Al abrir el archivo HTML y hacer clic en "Confiar en HTML" en la esquina superior izquierda, podrás observar el gráfico de velas generado, que debería verse de forma similar a lo que se muestra en el video.

Este proceso ilustra cómo las APIs y las bibliotecas como PyCoinGecko, Pandas y Plotly pueden ser utilizadas para acceder, procesar y visualizar datos, en este caso, datos financieros relacionados con las criptomonedas.


![[Pasted image 20241203082321.png]]


![[Pasted image 20241203082329.png]]


![[Pasted image 20241203082337.png]]



# API REST y solicitudes HTTP - Parte 1

En este vídeo, hablaremos sobre el Protocolo HTTP. Específicamente, abordaremos temas como el Uniform Resource Locator (URL), y los procesos de solicitud y respuesta. En la última sección, ya hemos hablado de las API REST. El Protocolo HTTP puede considerarse un protocolo general para la transferencia de información a través de la web, el cual incluye diversos tipos de API REST. Recordemos que las API REST funcionan enviando una solicitud que se comunica mediante un mensaje HTTP, que generalmente contiene un archivo JSON.

Cuando un cliente utiliza una página web, su navegador envía una solicitud HTTP al servidor donde está alojada la página. El servidor intenta encontrar el recurso solicitado, que por defecto es el archivo "index.html". Si la solicitud tiene éxito, el servidor enviará el objeto solicitado al cliente en una respuesta HTTP, la cual incluye información como el tipo de recurso, la longitud del recurso y otros detalles relevantes. En este contexto, la tabla bajo el Servidor Web representa una lista de recursos almacenados, como archivos HTML, imágenes PNG y archivos de texto.

Un Uniform Resource Locator (URL) es la forma más común de localizar recursos en la web. Un URL se puede dividir en tres partes: primero, el esquema, que es el protocolo, y en este laboratorio siempre será "http://". Luego, la dirección de Internet o URL Base, que se utiliza para encontrar la ubicación, como en [www.ibm.com](http://www.ibm.com/) o [www.gitlab.com](http://www.gitlab.com/). Por último, la ruta, que indica la ubicación específica dentro del servidor web, por ejemplo, /images/IDSNlogo.png.

Repasemos el proceso de solicitud y respuesta. A continuación se presenta un ejemplo del mensaje de solicitud para el método HTTP GET. Existen otros métodos HTTP disponibles, pero en este caso estamos utilizando el método GET, que solicita el archivo "index.html". La cabecera de la solicitud (Request) incluye información adicional, pero en el caso del método GET, la cabecera está vacía. Algunas solicitudes pueden incluir un cuerpo, y más adelante veremos un ejemplo de cómo se vería un cuerpo de solicitud.


![[Pasted image 20241203082904.png]]
La siguiente tabla muestra la respuesta que recibiríamos del servidor. La línea de inicio de la respuesta contiene la versión de HTTP seguida de una frase descriptiva. En este caso, "HTTP/1.0" junto con el código de estado "200", que significa éxito, y la frase "OK". La cabecera de respuesta contiene información adicional, y por último, el cuerpo de la respuesta contiene el archivo solicitado, que en este caso es un documento HTML.

A continuación, veremos algunos ejemplos de códigos de estado. Los códigos de estado se clasifican según su prefijo. Por ejemplo, los códigos que comienzan con "1" son respuestas informativas (como el código 100, que indica que todo va bien hasta el momento). Los códigos que comienzan con "2" indican respuestas exitosas, como el código 200, que significa que la solicitud ha tenido éxito. Los códigos que comienzan con "4" indican errores del cliente; por ejemplo, el código 401 significa que la solicitud no está autorizada. Por último, los códigos que comienzan con "5" indican errores del servidor, como el código 501, que significa "no implementado".

Cuando se realiza una solicitud HTTP, se envía un método HTTP, el cual indica al servidor qué acción debe tomar. A continuación, se presenta una lista de algunos de los métodos HTTP más comunes. En el siguiente vídeo, utilizaremos Python para implementar los métodos GET, que se usa para recuperar datos del servidor, y POST, que se utiliza para enviar datos al servidor.




# API REST y solicitudes HTTP - Parte 2


En este vídeo, analizaremos el protocolo HTTP y el uso de la biblioteca de solicitudes, un método popular para trabajar con el protocolo HTTP en Python. Revisaremos cómo utilizar la biblioteca de Python llamada **requests** para realizar solicitudes HTTP de manera sencilla. Esta biblioteca facilita el trabajo con los protocolos HTTP, y proporcionaremos una descripción general de las solicitudes GET y POST, que son dos de los métodos más comunes en este protocolo.

El módulo **requests** en Python es solo una de varias bibliotecas que permiten trabajar con HTTP, como **httplib** o **urllib**. **Requests** se destaca por su facilidad de uso, ya que te permite enviar solicitudes HTTP/1.1 de manera intuitiva. Para usarla, basta con importarla en tu código. Por ejemplo, para realizar una solicitud GET, simplemente utilizamos el método `get` con la URL correspondiente, como por ejemplo **[www.ibm.com](http://www.ibm.com/)**. El objeto de respuesta, que hemos denominado 'r', contiene información relevante sobre la solicitud, como el estado de la misma.

Podemos ver el código de estado de la solicitud mediante el atributo `status_code`, que en el caso de una solicitud exitosa será 200, indicando que todo está correcto. También podemos ver los encabezados de la solicitud, como el cuerpo, que en el caso de una solicitud GET no tendrá contenido (por lo tanto, el cuerpo será `None`). Además, podemos ver los encabezados de la respuesta HTTP utilizando el atributo `headers`. Este devuelve un diccionario con los encabezados, y podemos consultar valores específicos como la fecha de envío de la solicitud o el tipo de contenido (`Content-Type`), que indica el tipo de datos que recibimos. Si el tipo de contenido es HTML, podemos acceder al atributo `text` para ver el HTML en el cuerpo de la respuesta.

En el laboratorio, utilizaremos **httpbin.org**, un servicio simple que permite realizar solicitudes y recibir respuestas HTTP. Al realizar una solicitud GET, se añadirá la ruta `/get` a la URL base. En la solicitud GET también podemos incluir una cadena de consulta, que se utiliza para enviar información adicional al servidor. Esta cadena de consulta se comienza con un signo de interrogación (`?`), seguido de pares de parámetros y valores separados por el signo igual (`=`) y separados entre sí por el símbolo `&`. Por ejemplo, una cadena de consulta podría ser `?nombre=Joseph&ID=123`.

![[Pasted image 20241203083305.png]]
En un ejemplo práctico en Python, primero definimos la URL base y la concatenamos con el endpoint de la solicitud GET. Luego, creamos un diccionario llamado `payload`, que representa los parámetros de la cadena de consulta. Pasamos este diccionario al parámetro `params` de la función `get()`. Podemos imprimir la URL resultante para ver cómo se forman los parámetros y valores. Si se imprime el cuerpo de la solicitud, veremos que no contiene datos, ya que todo se envía a través de la URL.

A continuación, observamos la respuesta del servidor. Al igual que con las solicitudes GET, podemos imprimir el código de estado y analizar la respuesta como texto. Si la respuesta está en formato JSON, podemos utilizar el método `json()` para convertirla en un diccionario de Python. En este caso, la clave 'args' contendrá los parámetros y valores de la cadena de consulta.

Por otro lado, una solicitud POST también se utiliza para enviar datos a un servidor, pero a diferencia de las solicitudes GET, los datos en una solicitud POST se envían en el cuerpo de la solicitud y no en la URL. Para realizar una solicitud POST, cambiamos la ruta a `POST` y utilizamos el método `post()` de la biblioteca **requests**. Los datos a enviar se incluyen en un diccionario llamado `payload`, que se pasa al parámetro `data` de la función `post()`.

Al comparar una solicitud GET y una POST, vemos que en la solicitud GET los parámetros y valores se incluyen en la URL, mientras que en la solicitud POST no hay parámetros en la URL, sino que los datos se envían en el cuerpo de la solicitud. Además, solo la solicitud POST tendrá un cuerpo con datos, mientras que la GET no.




# Web Scraping and HTML Basics



**Objetivos**

Después de completar esta lectura, podrás:

- Explicar los conceptos clave relacionados con la estructura de HTML y la composición de etiquetas HTML.
- Explorar el concepto de árboles de documentos HTML.
- Familiarizarte con las tablas HTML.
- Obtener conocimientos básicos sobre web scraping usando Python y BeautifulSoup.

**Introducción al web scraping**

El web scraping, también conocido como recolección web o extracción de datos web, es el proceso de extraer información de sitios web o páginas web. Involucra la recuperación automatizada de datos de fuentes web. Las personas lo utilizan para diversas aplicaciones como análisis de datos, minería, comparación de precios, agregación de contenido y más.

**Cómo funciona el web scraping**

**Solicitud HTTP**

El proceso generalmente comienza con una solicitud HTTP. Un scraper web envía una solicitud HTTP a una URL específica, de manera similar a como lo haría un navegador web cuando visitas un sitio web. La solicitud suele ser una solicitud HTTP GET, que recupera el contenido de la página web.

**Recuperación de la página web**

El servidor web que aloja el sitio web responde a la solicitud devolviendo el contenido HTML de la página solicitada. Este contenido incluye el texto visible y los elementos multimedia, así como la estructura HTML subyacente que define el diseño de la página.

**Análisis HTML**

Una vez que se recibe el contenido HTML, es necesario analizarlo. El análisis implica descomponer la estructura HTML en componentes, como etiquetas, atributos y contenido de texto. Puedes usar BeautifulSoup en Python. Esta herramienta crea una representación estructurada del contenido HTML que se puede navegar y manipular fácilmente.

**Extracción de datos**

Con el contenido HTML analizado, los scrapers web ahora pueden identificar y extraer los datos específicos que necesitan. Estos datos pueden incluir texto, enlaces, imágenes, tablas, precios de productos, artículos de noticias y más. Los scrapers localizan los datos buscando etiquetas HTML relevantes, atributos y patrones en la estructura HTML.

**Transformación de datos**

Los datos extraídos pueden necesitar procesamiento y transformación adicionales. Por ejemplo, puedes eliminar las etiquetas HTML del texto, convertir formatos de datos o limpiar datos desordenados. Este paso asegura que los datos estén listos para el análisis o para otros casos de uso.

**Almacenamiento**

Después de la extracción y transformación, puedes almacenar los datos raspados en varios formatos, como bases de datos, hojas de cálculo, JSON o archivos CSV. La elección del formato de almacenamiento depende de los requisitos específicos del proyecto.

**Automatización**

En muchos casos, los scripts o programas automatizan el web scraping. Estas herramientas de automatización permiten la extracción de datos recurrente de varias páginas web o sitios web. El scraping automatizado es especialmente útil para recopilar datos de sitios web dinámicos que actualizan regularmente su contenido.

![[Pasted image 20241203083653.png]]



### Estructura HTML  

El lenguaje de marcado de hipertexto (HTML) sirve como base de las páginas web. Comprender su estructura es crucial para el web scraping.

### Composición de una etiqueta HTML  

Las etiquetas HTML definen la estructura del contenido web y pueden contener atributos.

Una etiqueta HTML consiste en una etiqueta de apertura (de inicio) y una etiqueta de cierre (de fin).  
Las etiquetas tienen nombres ( para una etiqueta de ancla).  
Las etiquetas pueden contener atributos con un nombre y valor de atributo, proporcionando información adicional a la etiqueta.

## Árbol de documentos HTML  

Puedes visualizar los documentos HTML como árboles, donde las etiquetas son nodos.

Las etiquetas pueden contener cadenas y otras etiquetas, lo que las convierte en "hijas" de la etiqueta.  
Las etiquetas dentro de la misma etiqueta padre se consideran hermanas.  
Por ejemplo, la etiqueta contiene tanto las etiquetas como , lo que las convierte en descendientes de pero hijas de . y son hermanas.

![[Pasted image 20241203083905.png]]

![[Pasted image 20241203084308.png]]
## HTML tables


Las tablas HTML son esenciales para presentar datos estructurados.

Define una tabla HTML utilizando la etiqueta .  
Cada fila de la tabla se define con la etiqueta.  
La primera fila generalmente utiliza la etiqueta de encabezado de tabla, que típicamente es  .  
La celda de la tabla se representa mediante las etiquetas  , que definen las celdas individuales en una fila.

![[Pasted image 20241203084308.png]]



## Web scraping


El web scraping implica extraer información de páginas web utilizando Python. Puede ahorrar tiempo y automatizar la recopilación de datos.

### Herramientas necesarias  
El web scraping requiere código en Python y dos módulos esenciales: Requests y Beautiful Soup. Asegúrate de tener ambos módulos instalados en tu entorno de Python.


```PYTHON
1. `# Import Beautiful Soup to parse the web page content`
2. `from bs4 import BeautifulSoup`
```


**Obteniendo y analizando HTML**  

Para comenzar con el web scraping, necesitas obtener el contenido HTML de una página web y analizarlo usando Beautiful Soup. Aquí tienes un ejemplo paso a paso:


```PYTHON
import requests
from bs4 import BeautifulSoup

# Specify the URL of the webpage you want to scrape
url = 'https://en.wikipedia.org/wiki/IBM'

# Send an HTTP GET request to the webpage
response = requests.get(url)

# Store the HTML content in a variable
html_content = response.text

# Create a BeautifulSoup object to parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')

# Display a snippet of the HTML content
print(html_content[:500])
```



**Navegando por la estructura HTML**  

BeautifulSoup representa el contenido HTML como una estructura tipo árbol, lo que permite una navegación fácil. Puedes usar métodos como `find_all` para filtrar y extraer elementos HTML específicos.



```PYTHON
# Find all <a> tags (anchor tags) in the HTML
links = soup.find_all('a')

# Iterate through the list of links and print their text
for the link in links:
    print(link.text)

```



