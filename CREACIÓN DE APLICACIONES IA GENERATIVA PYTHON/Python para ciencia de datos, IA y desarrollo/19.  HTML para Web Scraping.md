

En este artículo repasaremos el **Hypertext Markup Language (HTML)** para **Web Scraping**. En las páginas web hay muchos datos útiles, como precios de bienes inmuebles y soluciones a preguntas de codificación. Por ejemplo, la página web de Wikipedia es un repositorio de información mundial. Si tienes conocimientos de HTML, puedes usar Python para extraer esta información.

El HTML de una página web básica tiene una estructura compuesta de texto rodeado de elementos que se encuentran entre corchetes angulares llamados **etiquetas**. Estas etiquetas indican al navegador cómo mostrar el contenido. Los datos que necesitamos generalmente se encuentran dentro de estas etiquetas.

La primera parte de una página HTML contiene el "DOCTYPE html", que declara que este documento es un archivo HTML. El **elemento "html"** es el elemento raíz de la página, y el **elemento "head"** contiene metainformación sobre la página. A continuación, tenemos el **cuerpo** de la página, que es lo que se muestra en el navegador. Estos son los datos que generalmente nos interesan.

Dentro del cuerpo, podemos encontrar elementos con la etiqueta **h3**. Esta etiqueta se utiliza para encabezados de tipo 3, lo que hace que el texto sea más grande y en negrita. Estas etiquetas contienen los nombres de los jugadores, por ejemplo. Los datos están encerrados entre una etiqueta de apertura y una etiqueta de cierre . Además, hay otra etiqueta llamada **p**, que se utiliza para representar párrafos. Cada etiqueta contiene, por ejemplo, el salario de un jugador.

Ahora veamos más de cerca la **composición de una etiqueta HTML**. Un ejemplo común es la **etiqueta "a"** (anchor), que define un hipervínculo. Por ejemplo, una etiqueta que muestra "IBM" y, cuando haces clic en ella, te redirige a IBM.com.

En una etiqueta HTML, el nombre de la etiqueta, como **a**, se refiere a la clase. Cada instancia de esta etiqueta sería un enlace. Cada etiqueta tiene dos partes: una etiqueta de apertura y una etiqueta de cierre. La etiqueta de cierre es la misma que la de apertura, pero precedida por una barra. Las etiquetas también pueden tener **atributos**, que proporcionan información adicional. Por ejemplo, en el caso de un enlace, el atributo **href** contiene la URL del sitio web de destino.

Las páginas web reales son más complejas. Si deseas inspeccionar el HTML de una página, puedes seleccionar el elemento HTML en tu navegador y hacer clic en "Inspeccionar". Esto te permitirá ver el código HTML directamente.

Cada documento HTML puede considerarse un **árbol de documentos**. Las etiquetas pueden contener cadenas de texto y otras etiquetas. Las etiquetas internas o anidadas se consideran **hijas** de la etiqueta externa. Por ejemplo, dentro de un documento HTML, la etiqueta **html** contiene las etiquetas **head** y **body**, que son **descendientes** de **html**. A su vez, **head** y **body** son **hermanos**, ya que están al mismo nivel en la jerarquía.

Dentro del **body**, encontramos elementos como **heading** (encabezados) y **paragraph** (párrafos). Estas etiquetas también son hijas de **body** y, por lo tanto, son hermanas entre sí. Cada etiqueta forma parte de este árbol, lo que nos ayuda a entender la estructura de la página.

Finalmente, repasemos las **tablas HTML**. Para definir una tabla HTML, usamos la etiqueta . Cada fila de la tabla se define con la etiqueta . La primera fila de la tabla puede usar una etiqueta de encabezado de tabla . Las celdas de la tabla se definen con la etiqueta , y cada celda se coloca dentro de una fila de la tabla.

Ahora que hemos repasado algunos conocimientos básicos de HTML, podemos intentar extraer datos de una página web utilizando estas herramientas y técnicas.
![[Pasted image 20241203095941.png]]

# Web Scraping


![[Pasted image 20241203100057.png]]

Después de ver este artículo, serás capaz de definir el **raspado web**, comprender el papel de los objetos **BeautifulSoup**, aplicar el método `find_all`, y raspar datos de un sitio web.

¿Alguna vez has intentado analizar cientos de puntos de datos para encontrar a los mejores jugadores de un equipo deportivo? ¿Te has encontrado copiando y pegando manualmente la información de diferentes sitios web en una hoja de cálculo? ¿O quizás pasaste horas intentando encontrar los datos correctos, solo para darte por vencido porque la tarea parecía abrumadora? Aquí es donde el **web scraping** puede ser útil. El raspado web es un proceso que se utiliza para extraer automáticamente información de un sitio web, y se puede realizar en cuestión de minutos, no de horas.

Para comenzar, solo necesitamos algo de código en **Python** y la ayuda de dos módulos: **Requests** y **BeautifulSoup**. Supongamos que se te pidió que buscaras el nombre y el salario de los jugadores de una Liga Nacional de Baloncesto en una página web específica. Lo primero que debemos hacer es importar BeautifulSoup. Luego, podemos almacenar el **HTML** de la página web como una cadena en una variable llamada `HTML`.

Para analizar un documento, pasamos esta cadena al constructor de **BeautifulSoup**. Esto nos devuelve un objeto **BeautifulSoup** que representa el documento como una estructura de datos anidada. BeautifulSoup organiza el HTML en forma de un conjunto de objetos tipo árbol, con métodos que nos permiten analizar y navegar por el contenido HTML.

Al revisar el objeto **BeautifulSoup**, utilizando el objeto `soup` que hemos creado, notamos que cada **etiqueta** corresponde a una etiqueta HTML del documento original. Por ejemplo, la etiqueta `<title>` o `<h3>`. Si hay más de una etiqueta con el mismo nombre, **BeautifulSoup** seleccionará el primer elemento con esa etiqueta. En el caso de un nombre como **Lebron James**, el nombre estará dentro de una etiqueta **`<b>`** (en negrita). Para extraerlo, utilizamos la representación en árbol del objeto.

Puedes navegar por este árbol utilizando varios métodos. Por ejemplo, puedes acceder al **elemento hijo** de una etiqueta o navegar por la rama de la siguiente manera: usando el atributo `parent` puedes moverte hacia arriba en el árbol. También puedes encontrar el **hermano** de un objeto de etiqueta utilizando el atributo `next_sibling`.

Además, puedes acceder a los **atributos** de una etiqueta, como un par **clave-valor** en un diccionario, o devolver el contenido como una cadena de texto navegable, lo cual es similar a una cadena en Python pero con la funcionalidad de BeautifulSoup.

Ahora, repasemos el método `find_all`. Este método permite filtrar etiquetas según su nombre, atributos, texto, o una combinación de estos. Por ejemplo, si tienes una lista de **pizzerías** en una página web, primero crearías un objeto **BeautifulSoup** y luego aplicarías el método `find_all()` para buscar todos los descendientes de una etiqueta específica, como `<tr>` para las filas de una tabla. Esto devuelve un iterable, similar a una lista, en la que cada elemento corresponde a una fila de la tabla. De este modo, puedes iterar sobre cada fila, y luego aplicar `find_all()` a cada fila para encontrar las celdas de la tabla. Este proceso se repite para cada fila y cada celda.

Para aplicar **BeautifulSoup** a una página web, también necesitamos la biblioteca **Requests**. El primer paso es importar los módulos necesarios. Luego, usas el método `get` de la biblioteca **Requests** para descargar el HTML de la página. La URL es el parámetro de entrada, y el atributo `text` se utiliza para obtener el contenido de la página. Posteriormente, creas un objeto **BeautifulSoup** (llámalo `soup`) desde la variable `page`, lo que te permitirá analizar la página HTML.

Ahora puedes comenzar a raspar la página y extraer los datos que necesites. Para obtener más información, consulta los laboratorios adicionales sobre el tema.


![[Pasted image 20241203101531.png]]


![[Pasted image 20241203101538.png]]

![[Pasted image 20241203101753.png]]

![[Pasted image 20241203101554.png]]

- Las API simples en Python son interfaces de programación de aplicaciones que proporcionan métodos sencillos y fáciles de usar para interactuar con servicios, bibliotecas o datos, a menudo con una configuración o complejidad mínimas.
    
    - Una API permite que dos piezas de software se comuniquen entre sí.
        
    - Utilizar una biblioteca API en Python implica importar la biblioteca, llamar a sus funciones o métodos para realizar peticiones HTTP y analizar las respuestas para acceder a los datos o servicios proporcionados por la API.
        
    - La API de Pandas procesa los datos comunicándose con los otros componentes del software.
        
    - Se forma una Instancia cuando se crea un diccionario y luego se utiliza el constructor DataFrames para crear un Objeto Pandas.
        
    - El método "head()" mostrará el número mencionado de filas de la parte superior (por defecto 5) de DataFrames, mientras que el método "mean()" calculará la media y devolverá los valores
        
- Las API Rest le permiten comunicarse a través de Internet, aprovechando recursos como el almacenamiento, el acceso a más datos, algoritmos de IA, etc.
    
    - Los métodos HTTP transmiten datos a través de Internet.
        
    - Un mensaje HTTP suele incluir un archivo JSON con instrucciones para realizar operaciones.
        
    - Los mensajes HTTP que contienen archivos JSON se devuelven al cliente como respuesta de los servicios web.
        
    - Tratar con datos de series temporales implica utilizar la función de series temporales de Pandas.
        
    - Puede obtener datos de velas diarias y trazar el gráfico utilizando Plotly con el gráfico de velas.
        
- El protocolo HTTP (HyperText Transfer Protocol) transfiere datos, incluidas páginas web y recursos, entre un cliente (un navegador web) y un servidor en la World Wide Web.
    
    - El protocolo HTTP se utiliza habitualmente para implementar varios tipos de API REST.
        
    - Una respuesta HTTP incluye información como el tipo de recurso, la longitud del recurso, etc
        
    - Uniform Resource Locator (URL) es la forma más popular de encontrar recursos en la web.
        
    - La URL se divide en tres partes: esquema, dirección de Internet o URL base y ruta
        
    - El método GET es uno de los más populares para solicitar información. Algunos otros métodos también pueden incluir el cuerpo.
        
    - El método Response contiene la versión y el cuerpo de la respuesta.
        
    - POST envía datos al servidor, PUT actualiza datos que ya están en el servidor, DELETE elimina datos del servidor
        
- Requests es una biblioteca de Python que le permite enviar peticiones HTTP/1.1 fácilmente
    
    - Puede modificar los resultados de su consulta con el método GET.
        
    - Puede obtener varias peticiones de una URL como nombre, ID, etc. con una Cadena de consulta.
        
- El web scraping en Python consiste en extraer y analizar datos de sitios web para recopilar información para diversas aplicaciones, utilizando bibliotecas como Beautiful Soup y requests.
    
    - El HTML se compone de texto rodeado de elementos de texto azules encerrados entre corchetes angulares llamados etiquetas.
        
    - Puede seleccionar un elemento HTML de una página web para inspeccionarla.
        
    - Las páginas web también pueden contener CSS y JavaScript junto con elementos HTML.
        
    - Cada documento HTML es como un Árbol HTML, que puede contener cadenas y otras etiquetas.
        
    - Cada tabla HTML se compone de etiquetas de tabla y está estructurada con elementos como filas, encabezados, cuerpo, etc.
        
- Los datos tabulares también pueden extraerse de las páginas web mediante el método `read_html` de Pandas.
    
- Beautiful Soup en Python es una biblioteca para analizar y navegar por documentos HTML y XML, haciendo más accesible la extracción y manipulación de datos de páginas web.
    
- Para analizar un documento, páselo por el constructor de Beautiful Soup para obtener un objeto Beautiful Soup que representa el documento como una estructura de datos anidada.
    
- Beautiful soup representa el HTML como un conjunto de objetos en forma de árbol con métodos para analizar el HTML.
    
- Cadena navegable es como una cadena Python que soporta la funcionalidad de sopa hermosa.
    
- find_all es un método utilizado para extraer contenido basándose en el nombre de la etiqueta, sus atributos, el texto de una cadena o alguna combinación de éstos.
    
- El método find_all busca entre los descendientes de una etiqueta y recupera todos los descendientes que coincidan con sus filtros.
    
- El resultado es un iterable de Python como una lista.
    
- Los formatos de archivo se refieren a la estructura específica y a las reglas de codificación utilizadas para almacenar y representar datos en archivos, como .txt para texto sin formato o .csv para valores separados por comas.
    
- Python trabaja con distintos formatos de archivo como CSV, XML, JSON, xlsx, etc
    
- La extensión del nombre de un archivo le permitirá saber qué tipo de archivo es y con qué debe abrirse.
    
- Para acceder a los datos de archivos CSV, podemos utilizar bibliotecas de Python como Pandas.
    
- Del mismo modo, diferentes métodos ayudan a analizar archivos JSON, XML y otros.
    



# Cheat Sheet: API's and Data Collection

|Package/Method|Description|Code Example|
|---|---|---|
|Accessing element attribute|Access the value of a specific attribute of an HTML element.|Syntax:<br><br>1. 1<br><br>1. `attribute = element[(attribute)]`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `href = link_element[(href)]`<br><br>Copied!|
|BeautifulSoup()|Parse the HTML content of a web page using BeautifulSoup. The parser type can vary based on the project.|Syntax:<br><br>1. 1<br><br>1. `soup = BeautifulSoup(html, (html.parser))`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `html = (https://api.example.com/data) soup = BeautifulSoup(html, (html.parser))`<br><br>Copied!|
|delete()|Send a DELETE request to remove data or a resource from the server. DELETE requests delete a specified resource on the server.|Syntax:<br><br>1. 1<br><br>1. `response = requests.delete(url)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `response = requests.delete((https://api.example.com/delete))`<br><br>Copied!|
|find()|Find the first HTML element that matches the specified tag and attributes.|Syntax:<br><br>1. 1<br><br>1. `element = soup.find(tag, attrs)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `first_link = soup.find((a), {(class): (link)})`<br><br>Copied!|
|find_all()|Find all HTML elements that match the specified tag and attributes.|Syntax:<br><br>1. 1<br><br>1. `elements = soup.find_all(tag, attrs)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `all_links = soup.find_all((a), {(class): (link)})</td>`<br><br>Copied!|
|findChildren()|Find all child elements of an HTML element.|Syntax:<br><br>1. 1<br><br>1. `children = element.findChildren()`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `child_elements = parent_div.findChildren()`<br><br>Copied!|
|get()|Perform a GET request to retrieve data from a specified URL. GET requests are typically used for reading data from an API. The response variable will contain the server's response, which you can process further.|Syntax:<br><br>1. 1<br><br>1. `response = requests.get(url)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `response = requests.get((https://api.example.com/data))`<br><br>Copied!|
|Headers|Include custom headers in the request. Headers can provide additional information to the server, such as authentication tokens or content types.|Syntax:<br><br>1. 1<br><br>1. `headers = {(HeaderName): (Value)}`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `base_url = (https://api.example.com/data) headers = {(Authorization): (Bearer YOUR_TOKEN)} response = requests.get(base_url, headers=headers)`<br><br>Copied!|
|Import Libraries|Import the necessary Python libraries for web scraping.|Syntax:<br><br>1. 1<br><br>1. `from bs4 import BeautifulSoup`<br><br>Copied!|
|json()|Parse JSON data from the response. This extracts and works with the data returned by the API. The response.json() method converts the JSON response into a Python data structure (usually a dictionary or list).|Syntax:<br><br>1. 1<br><br>1. `data = response.json()`<br><br>Copied!<br><br>Example:<br><br>1. 1<br>2. 2<br><br>1. `response = requests.get((https://api.example.com/data))` <br>2. `data = response.json()`<br><br>Copied!|
|next_sibling()|Find the next sibling element in the DOM.|Syntax:<br><br>1. 1<br><br>1. `sibling = element.find_next_sibling()`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `next_sibling = current_element.find_next_sibling()`<br><br>Copied!|
|parent|Access the parent element in the Document Object Model (DOM).|Syntax:<br><br>1. 1<br><br>1. `parent = element.parent`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `parent_div = paragraph.parent`<br><br>Copied!|
|post()|Send a POST request to a specified URL with data. Create or update POST requests using resources on the server. The data parameter contains the data to send to the server, often in JSON format.|Syntax:<br><br>1. 1<br><br>1. `response = requests.post(url, data)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `response = requests.post((https://api.example.com/submit), data={(key): (value)})`<br><br>Copied!|
|put()|Send a PUT request to update data on the server. PUT requests are used to update an existing resource on the server with the data provided in the data parameter, typically in JSON format.|Syntax:<br><br>1. 1<br><br>1. `response = requests.put(url, data)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `response = requests.put((https://api.example.com/update), data={(key): (value)})`<br><br>Copied!|
|Query parameters|Pass query parameters in the URL to filter or customize the request. Query parameters specify conditions or limits for the requested data.|Syntax:<br><br>1. 1<br><br>1. `params = {(param_name): (value)}`<br><br>Copied!<br><br>Example:<br><br>1. 1<br>2. 2<br>3. 3<br><br>1. `base_url = "https://api.example.com/data"`<br>2. `params = {"page": 1, "per_page": 10}`<br>3. `response = requests.get(base_url, params=params)`<br><br>Copied!|
|select()|Select HTML elements from the parsed HTML using a CSS selector.|Syntax:<br><br>1. 1<br><br>1. `element = soup.select(selector)`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `titles = soup.select((h1))`<br><br>Copied!|
|status_code|Check the HTTP status code of the response. The HTTP status code indicates the result of the request (success, error, redirection). Use the HTTP status codeIt can be used for error handling and decision-making in your code.|Syntax:<br><br>1. 1<br><br>1. `response.status_code`<br><br>Copied!<br><br>Example:<br><br>1. 1<br>2. 2<br>3. 3<br><br>1. `url = "https://api.example.com/data"`<br>2. `response = requests.get(url)`<br>3. `status_code = response.status_code`<br><br>Copied!|
|tags for find() and find_all()|Specify any valid HTML tag as the tag parameter to search for elements of that type. Here are some common HTML tags that you can use with the tag parameter.|Tag Example:<br><br>1. 1<br>2. 2<br>3. 3<br>4. 4<br>5. 5<br>6. 6<br>7. 7<br>8. 8<br>9. 9<br>10. 10<br><br>1. `- (a): Find anchor () tags.`<br>2. `- (p): Find paragraph ((p)) tags.`<br>3. `- (h1), (h2), (h3), (h4), (h5), (h6): Find heading tags from level 1 to 6 ( (h1),n (h2)).`<br>4. `- (table): Find table () tags.`<br>5. `- (tr): Find table row () tags.`<br>6. `- (td): Find table cell ((td)) tags.`<br>7. `- (th): Find table header cell ((td))tags.`<br>8. `- (img): Find image ((img)) tags.`<br>9. `- (form): Find form ((form)) tags.`<br>10. `- (button): Find button ((button)) tags.`<br><br>Copied!|
|text|Retrieve the text content of an HTML element.|Syntax:<br><br>1. 1<br><br>1. `text = element.text`<br><br>Copied!<br><br>Example:<br><br>1. 1<br><br>1. `title_text = title_element.text`|

![[Pasted image 20241203102234.png]]


![[Pasted image 20241203102240.png]]


![[Pasted image 20241203102247.png]]


![[Pasted image 20241203102254.png]]


![[Pasted image 20241203102303.png]]



![[Pasted image 20241203103959.png]]



![[Pasted image 20241203104007.png]]

![[Pasted image 20241203104015.png]]


![[Pasted image 20241203104023.png]]


![[Pasted image 20241203104033.png]]


![[Pasted image 20241203104041.png]]


![[Pasted image 20241203104047.png]]


![[Pasted image 20241203104055.png]]



![[Pasted image 20241203104112.png]]

![[Pasted image 20241203104153.png]]


![[Pasted image 20241203104202.png]]


![[Pasted image 20241203104211.png]]



![[Pasted image 20241203104219.png]]



![[Pasted image 20241203104228.png]]








