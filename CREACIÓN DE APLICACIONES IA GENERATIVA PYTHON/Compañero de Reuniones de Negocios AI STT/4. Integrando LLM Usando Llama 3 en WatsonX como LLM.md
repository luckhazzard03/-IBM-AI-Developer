## Ejecutando un LLM simple

Comencemos generando texto con LLMs. Crea un archivo Python y nómbralo `simple_llm.py`. Puedes proceder haciendo clic en el enlace a continuación o consultando la imagen adjunta.

 Open **simple_llm.py** in IDE

En caso de que desees usar Llama 3 como una instancia de LLM, puedes seguir las instrucciones a continuación:

> IBM WatsonX utiliza varios modelos de lenguaje, incluido Llama 3 de Meta, que actualmente es el modelo de lenguaje de código abierto más potente.

Así es como funciona el código:

1. **Configurando credenciales**: Las credenciales necesarias para acceder a los servicios de IBM están preestablecidas por el equipo de Skills Network, por lo que no tienes que preocuparte por configurarlas tú mismo.
    
2. **Especificando parámetros**: El código luego define parámetros específicos para el modelo de lenguaje. ‘MAX_NEW_TOKENS’ establece el límite en el número de palabras que el modelo puede generar de una vez. ‘TEMPERATURE’ ajusta cuán creativo o predecible es el texto generado.
    
3. **Configurando el modelo Llama 3**: A continuación, se configura el modelo LLAMA3 utilizando un ID de modelo, las credenciales proporcionadas, los parámetros elegidos y un ID de proyecto.
    
4. **Creando un objeto para Llama 3**: El código crea un objeto llamado llm, que se utiliza para interactuar con el modelo Llama 3. Se crea un objeto de modelo, LLAMA3_model, utilizando la clase Model, que se inicializa con un ID de modelo específico, credenciales, parámetros y un ID de proyecto. Luego, se crea una instancia de WatsonxLLM con LLAMA3_model como argumento, inicializando el objeto del hub del modelo de lenguaje llm.
    
5. **Generando e imprimiendo la respuesta**: Finalmente, se utiliza ‘llm’ para generar una respuesta a la pregunta, “¿Cómo leer un libro de manera efectiva?” La respuesta se imprime a continuación.
    



1. `from ibm_watson_machine_learning.foundation_models import Model`
2. `from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM`
3. `from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams`

5. `my_credentials = {`
6.     `"url"    : "https://us-south.ml.cloud.ibm.com"`
7. `}`

9. `params = {`
10.         `GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.`
11.         `GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.`
12.     `}`

14. `LLAMA2_model = Model(`
15.         `model_id= 'meta-llama/llama-3-8b-instruct',` 
16.         `credentials=my_credentials,`
17.         `params=params,`
18.         `project_id="skills-network",`  
19.         `)`

21. `llm = WatsonxLLM(LLAMA2_model)`  

23. `print(llm("How to read a book effectively?"))`



Puedes ejecutar este script en la terminal usando el siguiente comando:



1. `python3 simple_llm.py`



Al ejecutar el script, deberías ver el texto generado en tu terminal, como se muestra a continuación:

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/llama2_read_book.jpg)

Puedes ver cómo watsonx Llama 2 proporciona una buena respuesta.



