

Crea un nuevo archivo de Python y llámalo `speech_analyzer.py`

 Open **speech_analyzer.py** in IDE

En este ejercicio, configuraremos una instancia de modelo de lenguaje (LLM), que podría ser IBM WatsonxLLM, HuggingFaceHub o un modelo de OpenAI. Luego, estableceremos una plantilla de prompt. Estas plantillas son guías estructuradas para generar prompts para modelos de lenguaje, ayudando en la organización de la salida (más información en [plantilla de prompt de langchain](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/).

A continuación, desarrollaremos una función de transcripción que emplea el modelo OpenAI Whisper para convertir voz a texto. Esta función toma un archivo de audio subido a través de una interfaz de aplicación Gradio (preferiblemente en formato .mp3). El texto transcrito se alimenta luego en un LLMChain, que integra el texto con la plantilla de prompt y lo envía al LLM elegido. La salida final del LLM se muestra en el cuadro de texto de salida de la aplicación Gradio.

La salida debería verse así:

![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/final_result_whisper.jpg)

Observa cómo el LLM corrigió un pequeño error cometido por el modelo de voz a texto, resultando en una salida coherente y precisa.

## Ejercicio: Completa las partes faltantes:



1. `import torch`
2. `import os`
3. `import gradio as gr`
4. `#from langchain.llms import OpenAI`
5. `from langchain.llms import HuggingFaceHub`
6. `from transformers import pipeline`
7. `from langchain.prompts import PromptTemplate`
8. `from langchain.chains import LLMChain`
9. `from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM`
10. `from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods`
11. `from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams`
12. `from ibm_watson_machine_learning.foundation_models import Model`

14. `#######------------- LLM-------------####`

16. `# initiate LLM instance, this can be IBM WatsonX, huggingface, or OpenAI instance`

18. `llm = ###---> write your code here`

20. `#######------------- Prompt Template-------------####`

22. `# This template is structured based on LLAMA2. If you are using other LLMs, feel free to remove the tags`
23. `temp = """`
24. `<s><<SYS>>`
25. `List the key points with details from the context:` 
26. `[INST] The context : {context} [/INST]` 
27. `<</SYS>>`
28. `"""`
29. `# here is the simplified version of the prompt template`
30. `# temp = """`
31. `# List the key points with details from the context:` 
32. `# The context : {context}` 
33. `# """`

35. `pt = PromptTemplate(`
36.     `input_variables=["context"],`
37.     `template= temp)`

39. `prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)`

41. `#######------------- Speech2text-------------####`

43. `def transcript_audio(audio_file):`
44.     `# Initialize the speech recognition pipeline`

46.     `pipe = #------> write the code here`

48.     `# Transcribe the audio file and return the result`
49.     `transcript_txt = pipe(audio_file, batch_size=8)["text"]`
50.     `# run the chain to merge transcript text with the template and send it to the LLM`
51.     `result = prompt_to_LLAMA2.run(transcript_txt)` 

53.     `return result`

55. `#######------------- Gradio-------------####`

57. `audio_input = gr.Audio(sources="upload", type="filepath")`
58. `output_text = gr.Textbox()`

60. `# Create the Gradio interface with the function, inputs, and outputs`
61. `iface = #---> write code here`

63. `iface.launch(server_name="0.0.0.0", server_port=7860)`



Haz clic aquí para la respuesta

Ejecuta tu código:



1. `python3 speech_analyzer.py`



Si no hay error, ejecuta la aplicación web:

 Aplicación web

# Conclusión

¡Felicidades por completar este proyecto! Ahora has sentado una base sólida para aprovechar potentes Modelos de Lenguaje (LLMs) para tareas de generación de texto a partir de voz. Aquí tienes un breve resumen de lo que has logrado:

- Generación de texto con LLM: Has creado un script en Python para generar texto utilizando un modelo del Hugging Face Hub, aprendido sobre algunos parámetros clave que influyen en la salida del modelo y tienes una comprensión básica de cómo cambiar entre diferentes modelos de LLM.
    
- Conversión de voz a texto: Utiliza la tecnología Whisper de OpenAI para convertir grabaciones de conferencias en texto, de manera precisa.
    
- Resumen de contenido: Implementa la IA de IBM Watson para resumir de manera efectiva las conferencias transcritas y extraer puntos clave.
    
- Desarrollo de interfaz de usuario: Crea una interfaz intuitiva y fácil de usar utilizando Hugging Face Gradio, asegurando la facilidad de uso para estudiantes y educadores.
    

## Author(s)

#### Sina Nazeri



- Puede utilizar la tecnología de reconocimiento automático del habla (ASR) para convertir el lenguaje hablado en texto legible.
    
- Puede utilizar un LLM para comprender y resumir el texto de forma eficiente.
    
- IBM watsonx ofrece varios Modelos generativos de IA, entre ellos Llama 2.
    
- Meta Llama 2 puede manejar más datos, generar respuestas más precisas y coherentes, y hacerlo más rápidamente y con menos recursos computacionales que el modelo Llama original.
    
- OpenAI Whisper es un revolucionario sistema de reconocimiento de voz diseñado para transcribir con precisión audio a texto. La API Python de Whisper facilita su integración en aplicaciones.
    

En este módulo, has trabajado en un proyecto para desarrollar una aplicación que transcriba las discusiones de la reunión y luego proporcione un resumen conciso, destacando los puntos clave. Utilizaste OpenAI Whisper para la conversión de voz a texto y aprovechaste las capacidades de Llama 2 LLM para resumir y extraer los puntos clave. Para crear una interfaz fácil de usar para la aplicación, utilizaste Hugging Face Gradio.




![[Pasted image 20241212172605.png]]


![[Pasted image 20241212172614.png]]


![[Pasted image 20241212172623.png]]

![[Pasted image 20241212172633.png]]






